$ python melo.en.american.py 
tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 251/251 [00:00<?, ?B/s]
vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 231k/231k [00:00<00:00, 552kB/s]
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     C:\Users\xxx\AppData\Roaming\nltk_data...
[nltk_data]   Unzipping taggers\averaged_perceptron_tagger.zip.
[nltk_data] Downloading package cmudict to
[nltk_data]     C:\Users\xxx\AppData\Roaming\nltk_data...
[nltk_data]   Unzipping corpora\cmudict.zip.
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<?, ?B/s]
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 570/570 [00:00<?, ?B/s]
vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 590kB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 2.39MB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<?, ?B/s]
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [00:00<?, ?B/s]
vocab.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 872k/872k [00:00<00:00, 4.82MB/s]
tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 1.72M/1.72M [00:01<00:00, 1.65MB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 80.0/80.0 [00:00<?, ?B/s]
config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 725/725 [00:00<00:00, 725kB/s]
vocab.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 344k/344k [00:00<00:00, 2.22MB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 83.0/83.0 [00:00<?, ?B/s]
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 420/420 [00:00<?, ?B/s]
vocab.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 227k/227k [00:02<00:00, 92.6kB/s]
tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 310/310 [00:00<00:00, 308kB/s]
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 650/650 [00:00<?, ?B/s]
vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 248k/248k [00:00<00:00, 347kB/s]
tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 486k/486k [00:00<00:00, 504kB/s]
special_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:00<?, ?B/s]
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.49k/3.49k [00:00<?, ?B/s]
C:\Users\xxx\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\utils\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
checkpoint.pth: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 208M/208M [00:44<00:00, 4.69MB/s]
 > Text split to sentences.
Did you ever hear a folk tale about a giant turtle?
 > ===========================
model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 440M/440M [01:27<00:00, 5.01MB/s] 
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:07<00:00, 127.26s/it]
 > Text split to sentences.
Did you ever hear a folk tale about a giant turtle?
 > ===========================
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.70s/it] 
 > Text split to sentences.
Did you ever hear a folk tale about a giant turtle?
 > ===========================
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.95s/it] 
 > Text split to sentences.
Did you ever hear a folk tale about a giant turtle?
 > ===========================
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.17s/it] 
 > Text split to sentences.
Did you ever hear a folk tale about a giant turtle?
 > ===========================
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.58s/it]