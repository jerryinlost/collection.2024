From: <Saved by Blink>
Snapshot-Content-Location: https://slickgpt.com/t/20
Subject: =?utf-8?Q?Training=20a=20Custom=20Korean=20Language=20Model=20for=20Korea?=
 =?utf-8?Q?n=20ASR=20-=20General=20=E2=80=A2=20SlickGPT?=
Date: Fri, 24 May 2024 02:30:17 -0400
MIME-Version: 1.0
Content-Type: multipart/related;
	type="text/html";
	boundary="----MultipartBoundary--6sVMFWoZJHSn1FW7TfhRLnwT0nyOM3FYO4qr2cklbx----"


------MultipartBoundary--6sVMFWoZJHSn1FW7TfhRLnwT0nyOM3FYO4qr2cklbx----
Content-Type: text/html
Content-ID: <frame-8A8CBC7723F82B09FB33403DB8B3DDD7@mhtml.blink>
Content-Transfer-Encoding: quoted-printable
Content-Location: https://slickgpt.com/t/20

<!DOCTYPE html><html lang=3D"en" translate=3D"no" style=3D"--vh: 9.53px;"><=
head><meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3DUTF-=
8"><link rel=3D"stylesheet" type=3D"text/css" href=3D"cid:css-ce4cbb40-bf19=
-4ad7-a5fb-e68cc3205d8e@mhtml.blink" />
   =20
    <link rel=3D"preconnect" href=3D"https://magicproxy-dnfo3dcduq-ew.a.run=
.app/">
    <link rel=3D"preconnect" href=3D"https://slickgpt-web-pe7fohytbq-uc.a.r=
un.app/">
    <link rel=3D"preconnect" href=3D"https://slickgpt-web-pe7fohytbq-ew.a.r=
un.app/">
    <link rel=3D"preconnect" href=3D"https://slickgpt-web-pe7fohytbq-zf.a.r=
un.app/">
    <link rel=3D"preconnect" href=3D"https://slickgpt-web-pe7fohytbq-de.a.r=
un.app/">
   =20
    <meta http-equiv=3D"X-UA-Compatible" content=3D"IE=3Dedge">

    <meta name=3D"viewport" content=3D"width=3Ddevice-width,initial-scale=
=3D1.0">
    <meta name=3D"referrer" content=3D"origin">

    <title>Training a Custom Korean Language Model for Korean ASR - General=
 =E2=80=A2 SlickGPT</title>
    <meta name=3D"description" content=3D"Training a Custom Korean Language=
 Model for Korean ASR - General">
    <meta name=3D"keywords" content=3D"SlickGPT, ChatGPT, AI assistant, no-=
code, marketing, creativity, productivity, personal assistant">
    <meta property=3D"og:title" content=3D"Training a Custom Korean Languag=
e Model for Korean ASR - General">
    <meta property=3D"og:description" content=3D"Training a Custom Korean L=
anguage Model for Korean ASR - General">
    <meta property=3D"og:image" content=3D"https://slickgpt.com/img/social.=
jpg">
    <meta property=3D"og:url" content=3D"https://slickgpt.com">
    <meta property=3D"og:type" content=3D"website">
    <meta property=3D"og:site_name" content=3D"SlickGPT">
    <meta name=3D"twitter:card" content=3D"summary_large_image">
    <meta name=3D"twitter:title" content=3D"Training a Custom Korean Langua=
ge Model for Korean ASR - General">
    <meta name=3D"twitter:description" content=3D"Training a Custom Korean =
Language Model for Korean ASR - General">
    <meta name=3D"twitter:image" content=3D"https://slickgpt.com/img/social=
.jpg">
    <meta name=3D"twitter:site" content=3D"@SlickGPT">
    <meta name=3D"twitter:creator" content=3D"@SlickGPT">

    <meta name=3D"apple-mobile-web-app-capable" content=3D"yes">
    <meta name=3D"format-detection" content=3D"telephone=3Dno">
    <meta name=3D"google" content=3D"notranslate">

    <meta name=3D"apple-mobile-web-app-title" content=3D"SlickGPT">
    <meta name=3D"application-name" content=3D"SlickGPT">
    <link rel=3D"apple-touch-icon" sizes=3D"180x180" href=3D"https://slickg=
pt.com/apple-touch-icon.png">
    <link rel=3D"icon" type=3D"image/png" sizes=3D"32x32" href=3D"https://s=
lickgpt.com/favicon-32x32.png">
    <link rel=3D"icon" type=3D"image/png" sizes=3D"16x16" href=3D"https://s=
lickgpt.com/favicon-16x16.png">
    <link rel=3D"manifest" href=3D"https://slickgpt.com/site.webmanifest">
    <link rel=3D"mask-icon" href=3D"https://slickgpt.com/safari-pinned-tab.=
svg" color=3D"#511446">
    <meta name=3D"msapplication-TileColor" content=3D"#511446">
    <meta name=3D"theme-color" content=3D"#080b16">

    <meta property=3D"og:locale" content=3D"en">
    <meta http-equiv=3D"Content-Language" content=3D"en">
   =20
    <link rel=3D"modulepreload" crossorigin=3D"" href=3D"https://slickgpt.c=
om/assets/vendor-2643db22.js">
    <link rel=3D"stylesheet" href=3D"https://slickgpt.com/assets/index-0587=
c55e.css">
  <link rel=3D"modulepreload" as=3D"script" crossorigin=3D"" href=3D"https:=
//slickgpt.com/assets/thread-id-bc4e7372.js"><link rel=3D"modulepreload" as=
=3D"script" crossorigin=3D"" href=3D"https://slickgpt.com/assets/useHead-43=
c28b06.js"><link rel=3D"modulepreload" as=3D"script" crossorigin=3D"" href=
=3D"https://slickgpt.com/assets/ThreadPage.vue_vue_type_script_setup_true_l=
ang-c15e9b98.js"><link rel=3D"modulepreload" as=3D"script" crossorigin=3D""=
 href=3D"https://slickgpt.com/assets/PageHeader-9edcc061.js"><link rel=3D"m=
odulepreload" as=3D"script" crossorigin=3D"" href=3D"https://slickgpt.com/a=
ssets/UserAvatar.vue_vue_type_script_setup_true_lang-42a6e52b.js"><link rel=
=3D"modulepreload" as=3D"script" crossorigin=3D"" href=3D"https://slickgpt.=
com/assets/ChannelInput.vue_vue_type_script_setup_true_lang-5a387590.js"><l=
ink rel=3D"modulepreload" as=3D"script" crossorigin=3D"" href=3D"https://sl=
ickgpt.com/assets/new-55d33ec1.js"><link rel=3D"modulepreload" as=3D"script=
" crossorigin=3D"" href=3D"https://slickgpt.com/assets/models-05589cc3.js">=
</head>
  <body class=3D"dark">
   =20
    <div id=3D"app" data-v-app=3D""><!----><div class=3D"min-h-screen page =
flex flex-col sm:flex-row relative transition"><nav class=3D"fixed sm:stick=
y z-[100] top-0 left-0 h-screen max-h-screen border-r text-white bg-[#090b1=
6] transition-all select-none w-[256px] -translate-x-[256px] sm:translate-x=
-0 overflow-hidden sm:overflow-visible sm:w-[68px] border-slate-800/50"><di=
v class=3D"flex flex-col h-full"><div class=3D"group flex relative items-ce=
nter justify-end px-6 h-16 hover:bg-[#172B51]/50 transition cursor-pointer"=
><span class=3D"text-lg absolute left-0 ml-5 font-black text-center flex it=
ems-center"><svg class=3D"h-5 w-5 ml-1 transition" xmlns=3D"http://www.w3.o=
rg/2000/svg" viewBox=3D"0 0 64 64"><path fill=3D"#50c18e" d=3D"M15 57c-2-10=
 6-22 16-24 5 9 3 33-9 31-3-1-6-3-7-7Z"></path><path fill=3D"#50c18e" d=3D"=
M1 42c0 9 14 9 13 0 1-9-13-9-13 0Z" opacity=3D".8"></path><path fill=3D"#f9=
bb49" d=3D"M7 15c10-2 22 6 24 16-9 5-33 3-31-9 1-3 3-6 7-7Z"></path><path f=
ill=3D"#f9bb49" d=3D"M22 1c-9 0-9 14 0 13 9 1 9-13 0-13Z" opacity=3D".8"></=
path><path fill=3D"#e92e42" d=3D"M49 7c2 10-6 22-16 24-5-9-3-33 9-31 3 1 6 =
3 7 7Z"></path><path fill=3D"#e92e42" d=3D"M63 22c0-9-14-9-13 0-1 9 13 9 13=
 0Z" opacity=3D".8"></path><path fill=3D"#58caf7" d=3D"M57 49c-10 2-22-6-24=
-16 9-5 33-3 31 9-1 3-3 6-7 7Z"></path><path fill=3D"#58caf7" d=3D"M42 63c9=
 0 9-14 0-13-9-1-9 13 0 13Z" opacity=3D".8"></path></svg><!----></span><!--=
--></div><a class=3D"border rounded-full flex items-center ml-4 mr-4 mt-1 c=
ursor-pointer group ring-2 ring-transparent transition duration-300 border-=
slate-800 ring-slate-800 bg-slate-900 p-2 justify-center hover:border-blue-=
500 hover:ring-blue-500" href=3D"https://slickgpt.com/new"><div class=3D"te=
xt-sm font-bold text-slate-100 dark:text-slate-400 truncate"><svg class=3D"=
w-3 h-3 my-1" xmlns=3D"http://www.w3.org/2000/svg" viewBox=3D"0 0 13.41 13.=
41"><path fill=3D"currentColor" d=3D"M0 6.7c0 .65.53 1.18 1.18 1.18h4.34v4.=
34a1.18 1.18 0 0 0 2.37 0V7.88h4.34a1.18 1.18 0 0 0 0-2.37H7.89V1.18a1.18 1=
.18 0 0 0-2.37 0v4.33H1.18A1.2 1.2 0 0 0 0 6.7Z"></path></svg></div><!---->=
</a><div class=3D"my-3"></div><div class=3D"flex-1 overflow-auto microScrol=
lbar w-full flex flex-col"><a class=3D"px-2 group transition text-[15px] bl=
ock" href=3D"https://slickgpt.com/settings"><div class=3D"px-4 py-1 rounded=
-md transition flex items-center justify-between border text-[#c0d0f1] hove=
r:text-white group-hover:bg-[#172B51]/50 border-transparent"><svg class=3D"=
h-4 w-4 min-w-[16px]" xmlns=3D"http://www.w3.org/2000/svg" fill=3D"none" vi=
ewBox=3D"0 0 24 24" stroke-width=3D"1.5" stroke=3D"currentColor"><path stro=
ke-linecap=3D"round" stroke-linejoin=3D"round" d=3D"M9.594 3.94c.09-.542.56=
-.94 1.11-.94h2.593c.55 0 1.02.398 1.11.94l.213 1.281c.063.374.313.686.645.=
87.074.04.147.083.22.127.324.196.72.257 1.075.124l1.217-.456a1.125 1.125 0 =
011.37.49l1.296 2.247a1.125 1.125 0 01-.26 1.431l-1.003.827c-.293.24-.438.6=
13-.431.992a6.759 6.759 0 010 .255c-.007.378.138.75.43.99l1.005.828c.424.35=
.534.954.26 1.43l-1.298 2.247a1.125 1.125 0 01-1.369.491l-1.217-.456c-.355-=
.133-.75-.072-1.076.124a6.57 6.57 0 01-.22.128c-.331.183-.581.495-.644.869l=
-.213 1.28c-.09.543-.56.941-1.11.941h-2.594c-.55 0-1.02-.398-1.11-.94l-.213=
-1.281c-.062-.374-.312-.686-.644-.87a6.52 6.52 0 01-.22-.127c-.325-.196-.72=
-.257-1.076-.124l-1.217.456a1.125 1.125 0 01-1.369-.49l-1.297-2.247a1.125 1=
.125 0 01.26-1.431l1.004-.827c.292-.24.437-.613.43-.992a6.932 6.932 0 010-.=
255c.007-.378-.138-.75-.43-.99l-1.004-.828a1.125 1.125 0 01-.26-1.43l1.297-=
2.247a1.125 1.125 0 011.37-.491l1.216.456c.356.133.751.072 1.076-.124.072-.=
044.146-.087.22-.128.332-.183.582-.495.644-.869l.214-1.281z"></path><path s=
troke-linecap=3D"round" stroke-linejoin=3D"round" d=3D"M15 12a3 3 0 11-6 0 =
3 3 0 016 0z"></path></svg><span class=3D"ml-2 truncate leading-none transi=
tion flex-1">Settings</span><!----><!----><!----></div></a><a class=3D"px-2=
 group transition text-[15px] block" href=3D"https://slickgpt.com/search"><=
div class=3D"px-4 py-1 rounded-md transition flex items-center justify-betw=
een border text-[#c0d0f1] hover:text-white group-hover:bg-[#172B51]/50 bord=
er-transparent"><svg class=3D"h-4 w-4 min-w-[16px]" xmlns=3D"http://www.w3.=
org/2000/svg" fill=3D"none" viewBox=3D"0 0 24 24" stroke-width=3D"1.5" stro=
ke=3D"currentColor"><path stroke-linecap=3D"round" stroke-linejoin=3D"round=
" d=3D"M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 1=
0.607z"></path></svg><span class=3D"ml-2 truncate leading-none transition f=
lex-1">Search</span><!----><!----><!----></div></a><a class=3D"px-2 group t=
ransition text-[15px] block" href=3D"https://slickgpt.com/archive"><div cla=
ss=3D"px-4 py-1 rounded-md transition flex items-center justify-between bor=
der text-[#c0d0f1] hover:text-white group-hover:bg-[#172B51]/50 border-tran=
sparent"><svg class=3D"h-4 w-4 min-w-[16px]" xmlns=3D"http://www.w3.org/200=
0/svg" fill=3D"none" viewBox=3D"0 0 24 24" stroke-width=3D"1.5" stroke=3D"c=
urrentColor"><path stroke-linecap=3D"round" stroke-linejoin=3D"round" d=3D"=
M20.25 7.5l-.625 10.632a2.25 2.25 0 01-2.247 2.118H6.622a2.25 2.25 0 01-2.2=
47-2.118L3.75 7.5M10 11.25h4M3.375 7.5h17.25c.621 0 1.125-.504 1.125-1.125v=
-1.5c0-.621-.504-1.125-1.125-1.125H3.375c-.621 0-1.125.504-1.125 1.125v1.5c=
0 .621.504 1.125 1.125 1.125z"></path></svg><span class=3D"ml-2 truncate le=
ading-none transition flex-1">Archived</span><!----><!----><!----></div></a=
><a href=3D"https://magicspace.agency/contact" target=3D"_blank" class=3D"p=
x-2 group transition text-[15px] block"><div class=3D"px-4 py-1 rounded-md =
transition flex items-center justify-between border text-[#c0d0f1] hover:te=
xt-white group-hover:bg-[#172B51]/50 border-transparent"><svg class=3D"w-4 =
h-4 min-w-[16px] group-hover:text-[#7289da] transition" xmlns=3D"http://www=
.w3.org/2000/svg" viewBox=3D"0 0 640 512"><path fill=3D"currentColor" d=3D"=
M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1=
.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0=
,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A=
483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.=
651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.66=
6,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,43=
0.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1=
.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1=
,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2=
.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.=
407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.01=
4,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.88=
2,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM22=
2.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,21=
9.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.=
491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,41=
7.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337=
.58,417.871,337.58Z"></path></svg><span class=3D"ml-2 truncate leading-none=
 transition flex-1">Contact</span></div></a><a class=3D"px-2 group transiti=
on text-[15px] block" href=3D"https://slickgpt.com/changelog"><div class=3D=
"px-4 py-1 rounded-md transition flex items-center justify-between border t=
ext-[#c0d0f1] hover:text-white group-hover:bg-[#172B51]/50 border-transpare=
nt"><svg class=3D"w-3.5 min-w-[14px] mx-px" xmlns=3D"http://www.w3.org/2000=
/svg" viewBox=3D"0 0 18.65 18.37"><path fill=3D"currentColor" d=3D"M14.3 9.=
3c-.7 0-1.29-.57-1.29-1.27s.59-1.3 1.29-1.29c.71.01 1.28.59 1.28 1.29s-.57 =
1.26-1.28 1.27Zm0 3.53c-.7 0-1.29-.59-1.29-1.29s.59-1.28 1.29-1.28a1.28 1.2=
8 0 0 1 0 2.57ZM7.44 7.78a.65.65 0 0 1-.64-.65.63.63 0 1 1 1.26 0c0 .34-.28=
.65-.62.65Zm1.86.54c-.34 0-.61-.29-.61-.64s.27-.64.61-.64c.37 0 .64.29.64.6=
4s-.28.64-.64.64Zm1.41 1.36a.62.62 0 0 1-.62-.62c0-.37.28-.66.62-.66s.64.29=
.64.66c0 .35-.29.62-.64.62Zm.48 1.81c-.35 0-.63-.28-.63-.63s.28-.63.63-.63.=
62.28.62.63-.28.63-.62.63Zm-.48 1.83c-.34 0-.62-.29-.62-.65s.28-.63.62-.63.=
64.28.64.63-.29.65-.64.65ZM9.3 14.71c-.34 0-.61-.29-.61-.64s.28-.64.61-.64c=
.36 0 .64.3.64.64s-.28.64-.64.64Zm-1.86.56c-.35 0-.64-.29-.64-.63s.29-.65.6=
4-.65.62.29.62.65-.28.63-.62.63Zm-1.88-.56c-.36 0-.63-.29-.63-.64s.28-.64.6=
3-.64.61.3.61.64-.27.64-.61.64Zm-1.41-1.39c-.35 0-.64-.3-.64-.65s.3-.63.64-=
.63.61.28.61.63-.27.65-.61.65Zm-.48-1.83c-.34 0-.62-.28-.62-.63s.28-.63.62-=
.63.63.28.63.63-.29.63-.63.63Zm.48-1.81a.63.63 0 0 1-.64-.62c0-.37.3-.66.64=
-.66s.61.29.61.66a.6.6 0 0 1-.61.62Zm1.41-1.36c-.36 0-.63-.29-.63-.64s.27-.=
64.63-.64.61.29.61.64-.27.64-.61.64Zm1.88 1.39c-.35 0-.64-.28-.64-.63s.29-.=
65.64-.65.62.29.62.65-.28.63-.62.63Zm1.72.77c-.34 0-.64-.3-.64-.64s.29-.64.=
64-.64.62.29.62.64-.28.64-.62.64Zm0 2.11c-.35 0-.64-.29-.64-.64s.29-.64.64-=
.64.62.29.62.64-.27.64-.62.64Zm-1.72.77c-.35 0-.64-.3-.64-.65s.29-.63.64-.6=
3.62.28.62.63-.28.65-.62.65Zm-1.73-.77c-.35 0-.63-.29-.63-.64s.28-.64.63-.6=
4.64.29.64.64-.3.64-.64.64Zm0-2.11c-.34 0-.63-.29-.63-.64s.29-.64.63-.64.64=
.3.64.64-.3.64-.64.64Zm1.73 1.04c-.35 0-.64-.28-.64-.63s.29-.63.64-.63.62.2=
8.62.63-.28.63-.62.63Zm7.59-6.25.53-1.66L4.6.04a.87.87 0 0 0-.55 1.66l10.98=
 3.56ZM2.76 18.37h13.13c1.79 0 2.76-.97 2.76-2.74V6.17c0-1.77-.97-2.74-2.76=
-2.74H2.76C.98 3.43 0 4.4 0 6.17v9.46c0 1.77.98 2.74 2.76 2.74Z"></path></s=
vg><span class=3D"ml-2 truncate leading-none transition flex-1">Changelog</=
span><!----><!----><!----></div></a><div class=3D"my-3"></div><div class=3D=
"space-y-2"><div id=3D"header-General" class=3D"flex flex-col transition-al=
l"><div class=3D"h-7 px-2 group transition text-[15px]"><div class=3D"flex =
items-center"><div class=3D"ml-3 my-0.5 h-6 w-6 flex items-center justify-c=
enter text-[#c0d0f1] hover:text-white hover:bg-[#172B51]/50 rounded-md tran=
sition cursor-pointer"><svg class=3D"w-2" xmlns=3D"http://www.w3.org/2000/s=
vg" viewBox=3D"0 0 21 20"><path fill=3D"currentColor" d=3D"M18 0H3C1 0 0 2 =
0 3v2l8 13 3 2 3-2 7-13V3c0-1-1-3-3-3Z"></path></svg></div><!----><!----></=
div></div><a class=3D"px-2 group transition text-[15px] block" href=3D"http=
s://slickgpt.com/t/0"><div class=3D"px-4 py-1 rounded-md transition flex it=
ems-center justify-between border text-[#c0d0f1] hover:text-white group-hov=
er:bg-[#172B51]/50 border-transparent"><span class=3D"h-4 w-4 min-w-[16px] =
flex items-center justify-center font-bold">#</span><span class=3D"ml-2 tru=
ncate leading-none transition flex-1">New Thread</span><!----><!----><!----=
></div></a><a class=3D"px-2 group transition text-[15px] block" href=3D"htt=
ps://slickgpt.com/t/2"><div class=3D"px-4 py-1 rounded-md transition flex i=
tems-center justify-between border text-[#c0d0f1] hover:text-white group-ho=
ver:bg-[#172B51]/50 border-transparent"><span class=3D"h-4 w-4 min-w-[16px]=
 flex items-center justify-center font-bold">#</span><span class=3D"ml-2 tr=
uncate leading-none transition flex-1">New Thread</span><!----><!----><!---=
-></div></a><a class=3D"px-2 group transition text-[15px] block" href=3D"ht=
tps://slickgpt.com/t/3"><div class=3D"px-4 py-1 rounded-md transition flex =
items-center justify-between border text-[#c0d0f1] hover:text-white group-h=
over:bg-[#172B51]/50 border-transparent"><span class=3D"h-4 w-4 min-w-[16px=
] flex items-center justify-center font-bold">=F0=9F=A4=96</span><span clas=
s=3D"ml-2 truncate leading-none transition flex-1">Best Korean LLM Models</=
span><!----><!----><!----></div></a><a class=3D"px-2 group transition text-=
[15px] block" href=3D"https://slickgpt.com/t/4"><div class=3D"px-4 py-1 rou=
nded-md transition flex items-center justify-between border text-[#c0d0f1] =
hover:text-white group-hover:bg-[#172B51]/50 border-transparent"><span clas=
s=3D"h-4 w-4 min-w-[16px] flex items-center justify-center font-bold">=F0=
=9F=94=A7</span><span class=3D"ml-2 truncate leading-none transition flex-1=
">Obtaining WSL Update x64 MSI Installer URLs</span><!----><!----><!----></=
div></a><a class=3D"px-2 group transition text-[15px] block" href=3D"https:=
//slickgpt.com/t/13"><div class=3D"px-4 py-1 rounded-md transition flex ite=
ms-center justify-between border text-[#c0d0f1] hover:text-white group-hove=
r:bg-[#172B51]/50 border-transparent"><span class=3D"h-4 w-4 min-w-[16px] f=
lex items-center justify-center font-bold">=F0=9F=94=8D</span><span class=
=3D"ml-2 truncate leading-none transition flex-1">Remove Duplicate Files in=
 Directory Using Python</span><!----><!----><!----></div></a><a class=3D"px=
-2 group transition text-[15px] block" href=3D"https://slickgpt.com/t/20"><=
div class=3D"px-4 py-1 rounded-md transition flex items-center justify-betw=
een border text-white bg-gradient-to-br from-[#172B51] to-[#1d3259] border-=
white/5"><span class=3D"h-4 w-4 min-w-[16px] flex items-center justify-cent=
er font-bold">=F0=9F=87=B0=F0=9F=87=B7</span><span class=3D"ml-2 truncate l=
eading-none transition flex-1">Training a Custom Korean Language Model for =
Korean ASR</span><!----><!----><!----></div></a></div><div id=3D"header-Mac=
hine Learning" class=3D"flex flex-col transition-all"><div class=3D"h-7 px-=
2 group transition text-[15px]"><div class=3D"flex items-center"><div class=
=3D"ml-3 my-0.5 h-6 w-6 flex items-center justify-center text-[#c0d0f1] hov=
er:text-white hover:bg-[#172B51]/50 rounded-md transition cursor-pointer"><=
svg class=3D"w-2" xmlns=3D"http://www.w3.org/2000/svg" viewBox=3D"0 0 21 20=
"><path fill=3D"currentColor" d=3D"M18 0H3C1 0 0 2 0 3v2l8 13 3 2 3-2 7-13V=
3c0-1-1-3-3-3Z"></path></svg></div><!----><!----></div></div><a class=3D"px=
-2 group transition text-[15px] block" href=3D"https://slickgpt.com/t/5"><d=
iv class=3D"px-4 py-1 rounded-md transition flex items-center justify-betwe=
en border text-[#c0d0f1] hover:text-white group-hover:bg-[#172B51]/50 borde=
r-transparent"><span class=3D"h-4 w-4 min-w-[16px] flex items-center justif=
y-center font-bold">=F0=9F=A4=96</span><span class=3D"ml-2 truncate leading=
-none transition flex-1">Training a Custom Korean ASR Model with Python</sp=
an><!----><!----><!----></div></a><a class=3D"px-2 group transition text-[1=
5px] block" href=3D"https://slickgpt.com/t/6"><div class=3D"px-4 py-1 round=
ed-md transition flex items-center justify-between border text-[#c0d0f1] ho=
ver:text-white group-hover:bg-[#172B51]/50 border-transparent"><span class=
=3D"h-4 w-4 min-w-[16px] flex items-center justify-center font-bold">=F0=9F=
=94=8A</span><span class=3D"ml-2 truncate leading-none transition flex-1">T=
raining Facebook's Denoiser on Custom Korean Speech Dataset with Various No=
ises</span><!----><!----><!----></div></a><a class=3D"px-2 group transition=
 text-[15px] block" href=3D"https://slickgpt.com/t/7"><div class=3D"px-4 py=
-1 rounded-md transition flex items-center justify-between border text-[#c0=
d0f1] hover:text-white group-hover:bg-[#172B51]/50 border-transparent"><spa=
n class=3D"h-4 w-4 min-w-[16px] flex items-center justify-center font-bold"=
>=F0=9F=A4=96</span><span class=3D"ml-2 truncate leading-none transition fl=
ex-1">Training a Korean Language Model with Python</span><!----><!----><!--=
--></div></a><a class=3D"px-2 group transition text-[15px] block" href=3D"h=
ttps://slickgpt.com/t/8"><div class=3D"px-4 py-1 rounded-md transition flex=
 items-center justify-between border text-[#c0d0f1] hover:text-white group-=
hover:bg-[#172B51]/50 border-transparent"><span class=3D"h-4 w-4 min-w-[16p=
x] flex items-center justify-center font-bold">=F0=9F=94=8A</span><span cla=
ss=3D"ml-2 truncate leading-none transition flex-1">Language-Free Speaker V=
erification Model Training on Mobile</span><!----><!----><!----></div></a><=
a class=3D"px-2 group transition text-[15px] block" href=3D"https://slickgp=
t.com/t/9"><div class=3D"px-4 py-1 rounded-md transition flex items-center =
justify-between border text-[#c0d0f1] hover:text-white group-hover:bg-[#172=
B51]/50 border-transparent"><span class=3D"h-4 w-4 min-w-[16px] flex items-=
center justify-center font-bold">=F0=9F=94=8A</span><span class=3D"ml-2 tru=
ncate leading-none transition flex-1">Language-Free Speaker Diarization Mod=
el for Mobile Devices</span><!----><!----><!----></div></a><a class=3D"px-2=
 group transition text-[15px] block" href=3D"https://slickgpt.com/t/10"><di=
v class=3D"px-4 py-1 rounded-md transition flex items-center justify-betwee=
n border text-[#c0d0f1] hover:text-white group-hover:bg-[#172B51]/50 border=
-transparent"><span class=3D"h-4 w-4 min-w-[16px] flex items-center justify=
-center font-bold">=F0=9F=8E=99=EF=B8=8F</span><span class=3D"ml-2 truncate=
 leading-none transition flex-1">Multilingual ASR Pretrained Model Evaluati=
on for Commercial Use</span><!----><!----><!----></div></a><a class=3D"px-2=
 group transition text-[15px] block" href=3D"https://slickgpt.com/t/11"><di=
v class=3D"px-4 py-1 rounded-md transition flex items-center justify-betwee=
n border text-[#c0d0f1] hover:text-white group-hover:bg-[#172B51]/50 border=
-transparent"><span class=3D"h-4 w-4 min-w-[16px] flex items-center justify=
-center font-bold">#</span><span class=3D"ml-2 truncate leading-none transi=
tion flex-1">Building a Windows Application with Pretrained Models using Py=
Torch</span><!----><!----><!----></div></a><a class=3D"px-2 group transitio=
n text-[15px] block" href=3D"https://slickgpt.com/t/12"><div class=3D"px-4 =
py-1 rounded-md transition flex items-center justify-between border text-[#=
c0d0f1] hover:text-white group-hover:bg-[#172B51]/50 border-transparent"><s=
pan class=3D"h-4 w-4 min-w-[16px] flex items-center justify-center font-bol=
d">=F0=9F=94=8D</span><span class=3D"ml-2 truncate leading-none transition =
flex-1">Developer Guide for Training Object Detection Model with YOLO-v9</s=
pan><!----><!----><!----></div></a><a class=3D"px-2 group transition text-[=
15px] block" href=3D"https://slickgpt.com/t/14"><div class=3D"px-4 py-1 rou=
nded-md transition flex items-center justify-between border text-[#c0d0f1] =
hover:text-white group-hover:bg-[#172B51]/50 border-transparent"><span clas=
s=3D"h-4 w-4 min-w-[16px] flex items-center justify-center font-bold">=F0=
=9F=94=81</span><span class=3D"ml-2 truncate leading-none transition flex-1=
">Converting a MeloTTS Model to ONNX Format</span><!----><!----><!----></di=
v></a><a class=3D"px-2 group transition text-[15px] block" href=3D"https://=
slickgpt.com/t/15"><div class=3D"px-4 py-1 rounded-md transition flex items=
-center justify-between border text-[#c0d0f1] hover:text-white group-hover:=
bg-[#172B51]/50 border-transparent"><span class=3D"h-4 w-4 min-w-[16px] fle=
x items-center justify-center font-bold">=F0=9F=A7=A0</span><span class=3D"=
ml-2 truncate leading-none transition flex-1">Memory Mechanism in ChatGPT</=
span><!----><!----><!----></div></a><a class=3D"px-2 group transition text-=
[15px] block" href=3D"https://slickgpt.com/t/16"><div class=3D"px-4 py-1 ro=
unded-md transition flex items-center justify-between border text-[#c0d0f1]=
 hover:text-white group-hover:bg-[#172B51]/50 border-transparent"><span cla=
ss=3D"h-4 w-4 min-w-[16px] flex items-center justify-center font-bold">=F0=
=9F=93=B1</span><span class=3D"ml-2 truncate leading-none transition flex-1=
">Choosing the Best Framework for On-Device Machine Learning</span><!----><=
!----><!----></div></a><a class=3D"px-2 group transition text-[15px] block"=
 href=3D"https://slickgpt.com/t/17"><div class=3D"px-4 py-1 rounded-md tran=
sition flex items-center justify-between border text-[#c0d0f1] hover:text-w=
hite group-hover:bg-[#172B51]/50 border-transparent"><span class=3D"h-4 w-4=
 min-w-[16px] flex items-center justify-center font-bold">=F0=9F=94=8A</spa=
n><span class=3D"ml-2 truncate leading-none transition flex-1">Speaker Veri=
fication Dataset Overview</span><!----><!----><!----></div></a><a class=3D"=
px-2 group transition text-[15px] block" href=3D"https://slickgpt.com/t/18"=
><div class=3D"px-4 py-1 rounded-md transition flex items-center justify-be=
tween border text-[#c0d0f1] hover:text-white group-hover:bg-[#172B51]/50 bo=
rder-transparent"><span class=3D"h-4 w-4 min-w-[16px] flex items-center jus=
tify-center font-bold">=F0=9F=94=8D</span><span class=3D"ml-2 truncate lead=
ing-none transition flex-1">Training a Model for Image Search Engine</span>=
<!----><!----><!----></div></a><a class=3D"px-2 group transition text-[15px=
] block" href=3D"https://slickgpt.com/t/19"><div class=3D"px-4 py-1 rounded=
-md transition flex items-center justify-between border text-[#c0d0f1] hove=
r:text-white group-hover:bg-[#172B51]/50 border-transparent"><span class=3D=
"h-4 w-4 min-w-[16px] flex items-center justify-center font-bold">=F0=9F=9B=
=8D=EF=B8=8F</span><span class=3D"ml-2 truncate leading-none transition fle=
x-1">Extracting Product Cards from Amazon Website Screenshot using Python</=
span><!----><!----><!----></div></a></div><!----></div></div><div class=3D"=
h-0 w-full border-b border-slate-800/50"></div><div data-headlessui-state=
=3D"" class=3D"p-1"><button id=3D"headlessui-menu-button-1" type=3D"button"=
 aria-haspopup=3D"menu" aria-expanded=3D"false" data-headlessui-state=3D"" =
class=3D"hover:bg-uiBlue-600/80 ui-open:bg-white/10 ui-open:text-white ui-n=
ot-open:hover:bg-white/[0.08] ui-not-open:dark:hover:text-white/80 active:b=
g-slate-800 active:ring-uiBlue-600 select-none py-2 px-4 text-base font-med=
ium text-white group bg-opacity-80 focus:outline-none active:ring-2 shadow-=
sm w-full rounded-lg focus:ring-1 focus:ring-blue-500"><div class=3D"flex i=
tems-center"><div class=3D"font-extrabold font-num text-center flex items-c=
enter justify-center text-white overflow-hidden text-[8px] bg-black rounded=
 h-5 w-5 min-w-[20px] transition" style=3D"background-color: rgb(178, 183, =
187);">CW</div><!----></div></button><!----></div></div></nav><div class=3D=
"flex flex-col page-fade-in flex-1"><div class=3D"sticky text-xs flex items=
-center justify-between z-20 top-0 left-0 w-full h-10 border-b border-slate=
-700/50 text-white bg-darkBlue-500/75 backdrop-blur select-none"><div class=
=3D"sm:hidden px-6"><svg class=3D"w-5 h-5" xmlns=3D"http://www.w3.org/2000/=
svg" fill=3D"none" viewBox=3D"0 0 24 24" stroke-width=3D"1.5" stroke=3D"cur=
rentColor"><path stroke-linecap=3D"round" stroke-linejoin=3D"round" d=3D"M3=
.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div><div class=
=3D"flex-1 h-10 group cursor-pointer py-1 rounded-md flex items-center font=
-black tracking-tight transition text-[#c0d0f1] group-hover:text-white grou=
p-hover:bg-[#172B51]/50 border-transparent px-1 sm:px-4"><span class=3D"fon=
t-bold mr-1">=F0=9F=87=B0=F0=9F=87=B7</span><span class=3D"whitespace-nowra=
p transition">General</span></div><div class=3D"flex-1 px-1 h-10 hidden sm:=
flex items-center group cursor-pointer truncate sm:px-3 py-1 rounded-md fon=
t-semibold transition text-[#c0d0f1]/50 group-hover:text-white group-hover:=
bg-[#172B51]/50 border-transparent">Training a Custom Korean Language Model=
 for Korean ASR</div><div class=3D"h-10 group cursor-pointer px-3 py-1 roun=
ded-md flex items-center font-black tracking-tight transition text-[#c0d0f1=
] group-hover:text-white group-hover:bg-[#172B51]/50 border-transparent"><s=
pan>4</span></div><div data-headlessui-state=3D"" class=3D"p-1 mr-2"><butto=
n id=3D"headlessui-menu-button-23" type=3D"button" aria-haspopup=3D"menu" a=
ria-expanded=3D"false" data-headlessui-state=3D"" class=3D"relative hover:b=
g-uiBlue-600/80 ui-open:bg-white/10 ui-open:text-white ui-not-open:hover:bg=
-white/[0.08] ui-not-open:dark:hover:text-white/80 active:bg-slate-800 acti=
ve:ring-uiBlue-600 select-none p-1 text-base font-medium text-white group b=
g-opacity-80 focus:outline-none active:ring-2 shadow-sm w-full rounded-lg f=
ocus:ring-1 focus:ring-blue-500" aria-label=3D"Account"><div class=3D"overf=
low-hidden bg-black rounded-lg h-6 w-6 transition group-hover:scale-110"><i=
mg referrerpolicy=3D"no-referrer" src=3D"https://lh3.googleusercontent.com/=
a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G9E0cdWXQ=3Ds96-c" alt=3D"" =
loading=3D"lazy" class=3D"transition h-full w-full opacity-100"></div></but=
ton><!----></div></div><div class=3D"flex-1 flex flex-col justify-between i=
tems-end"><div class=3D"flex flex-col justify-end h-full w-full"><div class=
=3D"p-6"><h2 class=3D"flex items-center text-lg lg:text-2xl"><span class=3D=
"font-bold mr-1">=F0=9F=87=B0=F0=9F=87=B7</span><span class=3D"font-black">=
General</span></h2><p class=3D"text-sm lg:text-base">Training a Custom Kore=
an Language Model for Korean ASR</p></div><div class=3D"pb-6 px-3 flex item=
s-center"><div class=3D"px-3"><div class=3D"text-[10px] text-slate-400 font=
-bold tracking-wide"> Model </div><div class=3D"text-sm font-black tracking=
-tight transition hover:text-[#f9bb49]">gpt-4o-2024-05-13</div></div><div c=
lass=3D"px-3"><div class=3D"text-[10px] text-slate-400 font-bold tracking-w=
ide"> Agent </div><div class=3D"text-sm font-black tracking-tight transitio=
n hover:text-[#f9bb49]">Assistant</div></div></div><div class=3D"chat-base =
px-6 group relative hover:bg-slate-800/50"><div class=3D"flex items-start p=
y-3"><div class=3D"h-8 w-8 mr-3 rounded flex items-center justify-center bg=
-slate-500/20 text-slate-100/50 transition cursor-pointer hover:bg-slate-50=
0/50 hover:text-slate-100"><svg xmlns=3D"http://www.w3.org/2000/svg" viewBo=
x=3D"0 0 17.08 17.07" width=3D"16" height=3D"16"><path fill=3D"currentColor=
" d=3D"M7.83 17.07h1.42c.56 0 1-.34 1.12-.89l.37-1.56c.21-.07.41-.15.58-.23=
l1.36.84c.46.29 1.02.23 1.41-.15l1-.99c.39-.39.44-.98.14-1.44l-.83-1.35c.09=
-.18.16-.37.22-.56l1.57-.38c.54-.12.88-.55.88-1.12V7.85c0-.55-.34-.99-.88-1=
.11l-1.55-.37a4.5 4.5 0 0 0-.23-.57l.84-1.38A1.1 1.1 0 0 0 15.1 3l-1.02-1a1=
.13 1.13 0 0 0-1.36-.16l-1.4.87a3.89 3.89 0 0 0-.58-.24L10.36.89A1.1 1.1 0 =
0 0 9.24 0H7.82c-.56 0-1 .35-1.12.89l-.38 1.56c-.21.07-.41.15-.59.25l-1.39-=
.86c-.47-.25-.97-.2-1.37.17l-1.01 1c-.4.38-.45.95-.14 1.42l.83 1.38c-.07.16=
-.16.36-.22.57l-1.55.37a1.1 1.1 0 0 0-.89 1.11v1.39c0 .56.34 1 .89 1.12l1.5=
7.38c.06.19.14.38.22.56l-.82 1.35c-.31.46-.25 1.05.14 1.44l.99.99c.39.38.95=
.44 1.42.15l1.36-.84.58.23.38 1.56c.12.54.55.89 1.12.89Zm.71-5.84a2.69 2.69=
 0 1 1 0-5.38 2.69 2.69 0 0 1 0 5.38Z"></path></svg></div><!----><!----><di=
v class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><div clas=
s=3D"text-sm leading-none font-bold text-white mr-2"><span>System</span><!-=
---><!----></div><div class=3D"text-xs leading-none text-slate-400 hover:un=
derline" title=3D"Thursday, May 23, 2024 at 11:53 PM">11:53 PM</div></div><=
div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>You ar=
e SlickGPT, a large language model on <a href=3D"http://slickgpt.com/">Slic=
kGPT.com</a>, answer as concisely as possible.<br>
Current date: 2024-05-24</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-black=
 rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https://l=
h3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G9E=
0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-full=
 opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Thursday, May 23, 2024 at 11=
:53 PM">11:53 PM</div></div><div class=3D"flex-1 chat-content break-words" =
agent=3D"Assistant"><p>I need to train a custom Korean language model for K=
orean ASR. which framework or tool to choose, how to train, how the dataset=
 look like</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Thursday, May 23, 2024 at 11:53 PM">11:53 PM</di=
v></div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant">=
<p>To train a custom Korean language model for Automatic Speech Recognition=
 (ASR), consider using frameworks like <strong>Kaldi</strong>, <strong>ESPn=
et</strong>, or <strong>Hugging Face=E2=80=99s Transformers</strong>. Here=
=E2=80=99s a concise guide:</p>
<h3 id=3D"frameworks-and-tools" tabindex=3D"-1"><a class=3D"header-anchor" =
href=3D"https://slickgpt.com/t/20#frameworks-and-tools" aria-hidden=3D"true=
">#</a> Frameworks and Tools:</h3>
<ol>
<li><strong>Kaldi</strong>: Well-suited for traditional ASR tasks and offer=
s extensive community support.</li>
<li><strong>ESPnet</strong>: Provides end-to-end ASR solutions and supports=
 multiple languages, including Korean.</li>
<li><strong>Hugging Face Transformers</strong>: Useful for leveraging pre-t=
rained models and fine-tuning for specific tasks.</li>
</ol>
<h3 id=3D"training-steps" tabindex=3D"-1"><a class=3D"header-anchor" href=
=3D"https://slickgpt.com/t/20#training-steps" aria-hidden=3D"true">#</a> Tr=
aining Steps:</h3>
<ol>
<li><strong>Data Collection</strong>: Gather a large, diverse Korean speech=
 dataset. Sources include public datasets like <strong>ClovaCall</strong>, =
<strong>Zeroth-Korean</strong>, or <strong>Common Voice</strong>.</li>
<li><strong>Preprocessing</strong>:
<ul>
<li><strong>Audio Processing</strong>: Convert audio files to a consistent =
format (e.g., 16 kHz, mono).</li>
<li><strong>Transcriptions</strong>: Ensure high-quality, accurate transcri=
ptions in Korean script (Hangul).</li>
</ul>
</li>
<li><strong>Feature Extraction</strong>: Convert audio signals into feature=
s (e.g., MFCCs, spectrograms).</li>
<li><strong>Model Training</strong>:
<ul>
<li><strong>Kaldi</strong>: Follow Kaldi=E2=80=99s ASR recipes, adapting th=
em for Korean.</li>
<li><strong>ESPnet</strong>: Use pre-built recipes or adapt an existing one=
 for Korean.</li>
<li><strong>Hugging Face</strong>: Fine-tune a pre-trained model like Wav2V=
ec2 or Whisper on your Korean dataset.</li>
</ul>
</li>
<li><strong>Evaluation and Tuning</strong>: Test the model on a separate va=
lidation set, adjust hyperparameters, and iterate.</li>
</ol>
<h3 id=3D"dataset-structure" tabindex=3D"-1"><a class=3D"header-anchor" hre=
f=3D"https://slickgpt.com/t/20#dataset-structure" aria-hidden=3D"true">#</a=
> Dataset Structure:</h3>
<ol>
<li><strong>Audio Files</strong>: Organized in folders, named consistently =
(e.g., <code>audio_001.wav</code>).</li>
<li><strong>Transcriptions</strong>: Corresponding text files for each audi=
o file, using the same naming convention (e.g., <code>audio_001.txt</code>)=
.</li>
<li><strong>Metadata File</strong> (optional): CSV or JSON file mapping aud=
io files to their transcriptions, including additional metadata (e.g., spea=
ker ID, duration).</li>
</ol>
<p>Example structure:</p>
<pre><code class=3D"hljs">/dataset
  /audio
    audio_001.wav
    audio_002.wav
    ...
  /transcriptions
    audio_001.txt
    audio_002.txt
    ...
  metadata.csv (optional)
</code></pre>
<p>By choosing the appropriate framework and following these steps, you can=
 effectively train a custom Korean ASR model.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Thursday, May 23, 2024 at 11=
:57 PM">11:57 PM</div></div><div class=3D"flex-1 chat-content break-words" =
agent=3D"Assistant"><p>Is it a good idea for performance - to separate lang=
uage model from ASR system, use CTC/Conformer for acoustic modelling, and u=
se a language model trained from language texts to predict the final result=
?</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Thursday, May 23, 2024 at 11:57 PM">11:57 PM</di=
v></div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant">=
<p>Yes, separating the language model from the ASR system can be a good ide=
a for enhancing performance. This approach leverages the strengths of both =
acoustic and language modeling. Here=E2=80=99s a concise breakdown:</p>
<h3 id=3D"benefits-of-separating-the-language-model" tabindex=3D"-1"><a cla=
ss=3D"header-anchor" href=3D"https://slickgpt.com/t/20#benefits-of-separati=
ng-the-language-model" aria-hidden=3D"true">#</a> Benefits of Separating th=
e Language Model:</h3>
<ol>
<li><strong>Improved Accuracy</strong>: A dedicated language model (LM) tra=
ined on a large corpus of text can better capture linguistic nuances and co=
ntext, improving transcription accuracy.</li>
<li><strong>Flexibility</strong>: You can update or improve the language mo=
del independently of the acoustic model, allowing for easier maintenance an=
d upgrades.</li>
<li><strong>Specialization</strong>: Acoustic models like CTC (Connectionis=
t Temporal Classification) or Conformer (a variant of Transformer tailored =
for speech) excel at modeling the raw audio features, while the LM can focu=
s purely on the text.</li>
</ol>
<h3 id=3D"suggested-approach" tabindex=3D"-1"><a class=3D"header-anchor" hr=
ef=3D"https://slickgpt.com/t/20#suggested-approach" aria-hidden=3D"true">#<=
/a> Suggested Approach:</h3>
<ol>
<li>
<p><strong>Acoustic Modeling</strong>:</p>
<ul>
<li>Use <strong>CTC</strong> or <strong>Conformer</strong> for the acoustic=
 model. These models are well-suited for handling sequential data and can e=
ffectively learn from raw speech.</li>
<li>Train the acoustic model on paired audio-transcription data.</li>
</ul>
</li>
<li>
<p><strong>Language Modeling</strong>:</p>
<ul>
<li>Train a language model (e.g., an RNN, Transformer, or BERT-based model)=
 on a large corpus of Korean text.</li>
<li>This model should learn the probabilities of sequences of words/charact=
ers to enhance the coherency of the output.</li>
</ul>
</li>
<li>
<p><strong>Decoding</strong>:</p>
<ul>
<li>Combine the outputs of the acoustic model and the language model during=
 the decoding phase.</li>
<li>Use techniques like <strong>beam search</strong> to explore multiple hy=
potheses and re-score them using the language model for the best possible t=
ranscription.</li>
</ul>
</li>
</ol>
<h3 id=3D"training-and-integration" tabindex=3D"-1"><a class=3D"header-anch=
or" href=3D"https://slickgpt.com/t/20#training-and-integration" aria-hidden=
=3D"true">#</a> Training and Integration:</h3>
<ol>
<li>
<p><strong>Training Acoustic Model</strong>:</p>
<ul>
<li>Collect a large dataset of Korean speech with accurate transcriptions.<=
/li>
<li>Preprocess the data and extract features (e.g., MFCCs, spectrograms).</=
li>
<li>Train the CTC or Conformer model on this data.</li>
</ul>
</li>
<li>
<p><strong>Training Language Model</strong>:</p>
<ul>
<li>Gather a large corpus of Korean text from diverse sources (books, artic=
les, conversations).</li>
<li>Train the language model to predict the next word/character in a sequen=
ce.</li>
</ul>
</li>
<li>
<p><strong>Integration</strong>:</p>
<ul>
<li>During inference, the acoustic model generates a probability distributi=
on over possible transcriptions.</li>
<li>The language model re-scores these hypotheses to produce the most likel=
y transcription based on linguistic context.</li>
</ul>
</li>
</ol>
<h3 id=3D"example-workflow" tabindex=3D"-1"><a class=3D"header-anchor" href=
=3D"https://slickgpt.com/t/20#example-workflow" aria-hidden=3D"true">#</a> =
Example Workflow:</h3>
<ol>
<li><strong>Acoustic Model</strong>: Generates preliminary transcriptions.<=
/li>
<li><strong>Language Model</strong>: Refines these transcriptions using lin=
guistic probabilities.</li>
<li><strong>Decoder</strong>: Uses beam search to find the best transcripti=
on based on combined scores from both models.</li>
</ol>
<p>By separating the language model from the ASR system and leveraging mode=
ls like CTC/Conformer for acoustic modeling, you can achieve a more robust =
and accurate ASR system.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Thursday, May 23, 2024 at 11=
:57 PM">11:57 PM</div></div><div class=3D"flex-1 chat-content break-words" =
agent=3D"Assistant"><p>I need a developer guide. detailed explanation with =
sample code</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Thursday, May 23, 2024 at 11:57 PM">11:57 PM</di=
v></div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant">=
<p>Certainly! Here=E2=80=99s a detailed developer guide for training a cust=
om Korean ASR system using a separate acoustic model and language model app=
roach, combining CTC/Conformer for acoustic modeling and a Transformer-base=
d language model. The guide will walk you through the setup, training, and =
integration phases.</p>
<h3 id=3D"prerequisites" tabindex=3D"-1"><a class=3D"header-anchor" href=3D=
"https://slickgpt.com/t/20#prerequisites" aria-hidden=3D"true">#</a> Prereq=
uisites</h3>
<ul>
<li>Python 3.8+</li>
<li>PyTorch</li>
<li>Libraries: <code>torchaudio</code>, <code>transformers</code>, <code>da=
tasets</code>, <code>sentencepiece</code></li>
<li>Basic understanding of ASR concepts</li>
</ul>
<h3 id=3D"step-1-set-up-environment" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-1-set-up-environment" aria-hidd=
en=3D"true">#</a> Step 1: Set Up Environment</h3>
<pre><code class=3D"hljs language-bash"><span class=3D"hljs-comment"># Crea=
te and activate a virtual environment (optional)</span>
python -m venv asr_env
<span class=3D"hljs-built_in">source</span> asr_env/bin/activate  <span cla=
ss=3D"hljs-comment"># On Windows use `.\asr_env\Scripts\activate`</span>

<span class=3D"hljs-comment"># Install necessary libraries</span>
pip install torch torchaudio transformers datasets sentencepiece
</code></pre>
<h3 id=3D"step-2-prepare-the-data" tabindex=3D"-1"><a class=3D"header-ancho=
r" href=3D"https://slickgpt.com/t/20#step-2-prepare-the-data" aria-hidden=
=3D"true">#</a> Step 2: Prepare the Data</h3>
<p>Ensure you have a dataset with audio files and corresponding transcripts=
. For this example, we=E2=80=99ll use a hypothetical dataset.</p>
<h3 id=3D"step-3-preprocess-data" tabindex=3D"-1"><a class=3D"header-anchor=
" href=3D"https://slickgpt.com/t/20#step-3-preprocess-data" aria-hidden=3D"=
true">#</a> Step 3: Preprocess Data</h3>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torchaudio
<span class=3D"hljs-keyword">from</span> datasets <span class=3D"hljs-keywo=
rd">import</span> load_dataset

<span class=3D"hljs-comment"># Load your dataset (replace with actual datas=
et loading logic)</span>
dataset =3D load_dataset(<span class=3D"hljs-string">"your_dataset_name"</s=
pan>)

<span class=3D"hljs-comment"># Preprocess audio data</span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">preprocess_audio</span>(<span class=3D"hljs-params">batch</span>):
    audio, sample_rate =3D torchaudio.load(batch[<span class=3D"hljs-string=
">"audio_file"</span>])
    batch[<span class=3D"hljs-string">"input_values"</span>] =3D audio.sque=
eze().numpy()
    <span class=3D"hljs-keyword">return</span> batch

<span class=3D"hljs-comment"># Apply preprocessing</span>
dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_aud=
io)

<span class=3D"hljs-comment"># Tokenize transcripts</span>
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> AutoTokenizer

tokenizer =3D AutoTokenizer.from_pretrained(<span class=3D"hljs-string">"be=
rt-base-multilingual-cased"</span>)
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">preprocess_transcripts</span>(<span class=3D"hljs-params">batch</span>):
    batch[<span class=3D"hljs-string">"labels"</span>] =3D tokenizer(batch[=
<span class=3D"hljs-string">"transcript"</span>], padding=3D<span class=3D"=
hljs-string">"max_length"</span>, truncation=3D<span class=3D"hljs-literal"=
>True</span>, max_length=3D<span class=3D"hljs-number">512</span>).input_id=
s
    <span class=3D"hljs-keyword">return</span> batch

dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_tra=
nscripts)
</code></pre>
<h3 id=3D"step-4-train-acoustic-model-ctc-conformer" tabindex=3D"-1"><a cla=
ss=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-4-train-acousti=
c-model-ctc-conformer" aria-hidden=3D"true">#</a> Step 4: Train Acoustic Mo=
del (CTC/Conformer)</h3>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> Wav2Vec2ForCTC, Wav2Vec2Processor

<span class=3D"hljs-comment"># Load processor and model</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-large-960h"</span>)
model =3D Wav2Vec2ForCTC.from_pretrained(<span class=3D"hljs-string">"faceb=
ook/wav2vec2-large-960h"</span>)

<span class=3D"hljs-comment"># Prepare data for training</span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">prepare_dataset</span>(<span class=3D"hljs-params">batch</span>):
    batch[<span class=3D"hljs-string">"input_values"</span>] =3D processor(=
batch[<span class=3D"hljs-string">"input_values"</span>], sampling_rate=3D<=
span class=3D"hljs-number">16000</span>).input_values[<span class=3D"hljs-n=
umber">0</span>]
    batch[<span class=3D"hljs-string">"labels"</span>] =3D torch.tensor(bat=
ch[<span class=3D"hljs-string">"labels"</span>])
    <span class=3D"hljs-keyword">return</span> batch

dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(prepare_datase=
t)

<span class=3D"hljs-comment"># Create data loaders</span>
<span class=3D"hljs-keyword">from</span> torch.utils.data <span class=3D"hl=
js-keyword">import</span> DataLoader

train_loader =3D DataLoader(dataset[<span class=3D"hljs-string">"train"</sp=
an>], batch_size=3D<span class=3D"hljs-number">8</span>, shuffle=3D<span cl=
ass=3D"hljs-literal">True</span>)
val_loader =3D DataLoader(dataset[<span class=3D"hljs-string">"validation"<=
/span>], batch_size=3D<span class=3D"hljs-number">8</span>)

<span class=3D"hljs-comment"># Training loop</span>
optimizer =3D torch.optim.AdamW(model.parameters(), lr=3D<span class=3D"hlj=
s-number">5e-5</span>)

<span class=3D"hljs-keyword">for</span> epoch <span class=3D"hljs-keyword">=
in</span> <span class=3D"hljs-built_in">range</span>(<span class=3D"hljs-nu=
mber">10</span>):
    model.train()
    <span class=3D"hljs-keyword">for</span> batch <span class=3D"hljs-keywo=
rd">in</span> train_loader:
        input_values =3D batch[<span class=3D"hljs-string">"input_values"</=
span>].to(torch.float32)
        labels =3D batch[<span class=3D"hljs-string">"labels"</span>]
        outputs =3D model(input_values, labels=3Dlabels)
        loss =3D outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
   =20
    model.<span class=3D"hljs-built_in">eval</span>()
    val_loss =3D <span class=3D"hljs-number">0</span>
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        <span class=3D"hljs-keyword">for</span> batch <span class=3D"hljs-k=
eyword">in</span> val_loader:
            input_values =3D batch[<span class=3D"hljs-string">"input_value=
s"</span>].to(torch.float32)
            labels =3D batch[<span class=3D"hljs-string">"labels"</span>]
            outputs =3D model(input_values, labels=3Dlabels)
            val_loss +=3D outputs.loss.item()
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Epoch <span class=3D"hljs-subst">{epoch}</span>, Validation Loss: <span c=
lass=3D"hljs-subst">{val_loss/<span class=3D"hljs-built_in">len</span>(val_=
loader)}</span>"</span>)
</code></pre>
<h3 id=3D"step-5-train-language-model" tabindex=3D"-1"><a class=3D"header-a=
nchor" href=3D"https://slickgpt.com/t/20#step-5-train-language-model" aria-=
hidden=3D"true">#</a> Step 5: Train Language Model</h3>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> BertForMask=
edLM, BertTokenizer, Trainer, TrainingArguments

<span class=3D"hljs-comment"># Load tokenizer and model</span>
tokenizer =3D BertTokenizer.from_pretrained(<span class=3D"hljs-string">"be=
rt-base-multilingual-cased"</span>)
lm_model =3D BertForMaskedLM.from_pretrained(<span class=3D"hljs-string">"b=
ert-base-multilingual-cased"</span>)

<span class=3D"hljs-comment"># Prepare dataset for language model training<=
/span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">tokenize_function</span>(<span class=3D"hljs-params">examples</span>):
    <span class=3D"hljs-keyword">return</span> tokenizer(examples[<span cla=
ss=3D"hljs-string">"text"</span>], padding=3D<span class=3D"hljs-string">"m=
ax_length"</span>, truncation=3D<span class=3D"hljs-literal">True</span>, m=
ax_length=3D<span class=3D"hljs-number">128</span>)

text_dataset =3D load_dataset(<span class=3D"hljs-string">"your_text_datase=
t"</span>)
tokenized_dataset =3D text_dataset.<span class=3D"hljs-built_in">map</span>=
(tokenize_function, batched=3D<span class=3D"hljs-literal">True</span>)
tokenized_dataset =3D tokenized_dataset.remove_columns([<span class=3D"hljs=
-string">"text"</span>])

<span class=3D"hljs-comment"># Training arguments</span>
training_args =3D TrainingArguments(
    output_dir=3D<span class=3D"hljs-string">"./results"</span>,
    evaluation_strategy=3D<span class=3D"hljs-string">"epoch"</span>,
    learning_rate=3D<span class=3D"hljs-number">2e-5</span>,
    per_device_train_batch_size=3D<span class=3D"hljs-number">16</span>,
    per_device_eval_batch_size=3D<span class=3D"hljs-number">16</span>,
    num_train_epochs=3D<span class=3D"hljs-number">3</span>,
    weight_decay=3D<span class=3D"hljs-number">0.01</span>,
)

<span class=3D"hljs-comment"># Trainer</span>
trainer =3D Trainer(
    model=3Dlm_model,
    args=3Dtraining_args,
    train_dataset=3Dtokenized_dataset[<span class=3D"hljs-string">"train"</=
span>],
    eval_dataset=3Dtokenized_dataset[<span class=3D"hljs-string">"validatio=
n"</span>],
)

<span class=3D"hljs-comment"># Train the language model</span>
trainer.train()
</code></pre>
<h3 id=3D"step-6-integration-for-inference" tabindex=3D"-1"><a class=3D"hea=
der-anchor" href=3D"https://slickgpt.com/t/20#step-6-integration-for-infere=
nce" aria-hidden=3D"true">#</a> Step 6: Integration for Inference</h3>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> Wav2Vec2For=
CTC, Wav2Vec2Processor, BertForMaskedLM, BertTokenizer
<span class=3D"hljs-keyword">import</span> torch

<span class=3D"hljs-comment"># Load models and tokenizers</span>
acoustic_model =3D Wav2Vec2ForCTC.from_pretrained(<span class=3D"hljs-strin=
g">"your_trained_acoustic_model"</span>)
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-large-960h"</span>)
lm_model =3D BertForMaskedLM.from_pretrained(<span class=3D"hljs-string">"y=
our_trained_language_model"</span>)
lm_tokenizer =3D BertTokenizer.from_pretrained(<span class=3D"hljs-string">=
"bert-base-multilingual-cased"</span>)

<span class=3D"hljs-comment"># Inference function</span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">transcribe</span>(<span class=3D"hljs-params">audio_file</span>):
    audio_input, _ =3D torchaudio.load(audio_file)
    input_values =3D processor(audio_input.squeeze().numpy(), sampling_rate=
=3D<span class=3D"hljs-number">16000</span>).input_values[<span class=3D"hl=
js-number">0</span>]
    input_values =3D torch.tensor(input_values).unsqueeze(<span class=3D"hl=
js-number">0</span>)

    acoustic_model.<span class=3D"hljs-built_in">eval</span>()
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        logits =3D acoustic_model(input_values).logits

    predicted_ids =3D torch.argmax(logits, dim=3D-<span class=3D"hljs-numbe=
r">1</span>)
    transcription =3D processor.decode(predicted_ids[<span class=3D"hljs-nu=
mber">0</span>])

    <span class=3D"hljs-comment"># Use language model to refine transcripti=
on</span>
    lm_input =3D lm_tokenizer(transcription, return_tensors=3D<span class=
=3D"hljs-string">"pt"</span>)
    lm_output =3D lm_model(**lm_input)
    refined_transcription =3D lm_tokenizer.decode(torch.argmax(lm_output.lo=
gits, dim=3D-<span class=3D"hljs-number">1</span>)[<span class=3D"hljs-numb=
er">0</span>])

    <span class=3D"hljs-keyword">return</span> refined_transcription

<span class=3D"hljs-comment"># Example usage</span>
audio_file =3D <span class=3D"hljs-string">"path/to/korean/audio.wav"</span=
>
transcription =3D transcribe(audio_file)
<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">"Tra=
nscription:"</span>, transcription)
</code></pre>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>This guide covers setting up the environment, preparing the data, traini=
ng both the acoustic and language models, and integrating them for inferenc=
e. By following these steps, you can build a robust Korean ASR system lever=
aging the strengths of both CTC/Conformer and a Transformer-based language =
model.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 1:22=
 AM">1:22 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>I need small models for on-device apps like smart phone =
apps.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 1:22 AM">1:22 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>F=
or on-device ASR applications, such as those on smartphones, lightweight mo=
dels are essential to ensure efficient performance and low resource consump=
tion. Here=E2=80=99s a guide to using smaller models and optimizing them fo=
r on-device applications:</p>
<h3 id=3D"step-1-select-smaller-models" tabindex=3D"-1"><a class=3D"header-=
anchor" href=3D"https://slickgpt.com/t/20#step-1-select-smaller-models" ari=
a-hidden=3D"true">#</a> Step 1: Select Smaller Models</h3>
<ol>
<li>
<p><strong>Acoustic Model</strong>:</p>
<ul>
<li>Use <code>Wav2Vec2</code> or <code>Conformer</code> models with smaller=
 architectures.</li>
<li>Consider models like <code>DistilWav2Vec2</code> or <code>Tiny Conforme=
r</code>.</li>
</ul>
</li>
<li>
<p><strong>Language Model</strong>:</p>
<ul>
<li>Use smaller Transformer models like <code>DistilBERT</code> or <code>Ti=
nyBERT</code>.</li>
</ul>
</li>
</ol>
<h3 id=3D"step-2-set-up-environment" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-2-set-up-environment" aria-hidd=
en=3D"true">#</a> Step 2: Set Up Environment</h3>
<pre><code class=3D"hljs language-bash"><span class=3D"hljs-comment"># Crea=
te and activate a virtual environment (optional)</span>
python -m venv asr_env
<span class=3D"hljs-built_in">source</span> asr_env/bin/activate  <span cla=
ss=3D"hljs-comment"># On Windows use `.\asr_env\Scripts\activate`</span>

<span class=3D"hljs-comment"># Install necessary libraries</span>
pip install torch torchaudio transformers datasets sentencepiece
</code></pre>
<h3 id=3D"step-3-prepare-data" tabindex=3D"-1"><a class=3D"header-anchor" h=
ref=3D"https://slickgpt.com/t/20#step-3-prepare-data" aria-hidden=3D"true">=
#</a> Step 3: Prepare Data</h3>
<p>Ensure you have your dataset ready with audio files and corresponding tr=
anscripts.</p>
<h3 id=3D"step-4-train-a-smaller-acoustic-model" tabindex=3D"-1"><a class=
=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-4-train-a-smaller=
-acoustic-model" aria-hidden=3D"true">#</a> Step 4: Train a Smaller Acousti=
c Model</h3>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> Wav2Vec2Processor, Wav2Vec2ForCTC

<span class=3D"hljs-comment"># Load a smaller Wav2Vec2 model</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-large-960h"</span>)
model =3D Wav2Vec2ForCTC.from_pretrained(<span class=3D"hljs-string">"faceb=
ook/wav2vec2-large-960h"</span>)

<span class=3D"hljs-comment"># Reduce model size</span>
model.config.hidden_size =3D <span class=3D"hljs-number">256</span>
model.config.num_attention_heads =3D <span class=3D"hljs-number">4</span>
model.config.num_hidden_layers =3D <span class=3D"hljs-number">6</span>

<span class=3D"hljs-comment"># Prepare data</span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">preprocess_audio</span>(<span class=3D"hljs-params">batch</span>):
    audio, sample_rate =3D torchaudio.load(batch[<span class=3D"hljs-string=
">"audio_file"</span>])
    batch[<span class=3D"hljs-string">"input_values"</span>] =3D processor(=
audio.squeeze().numpy(), sampling_rate=3D<span class=3D"hljs-number">16000<=
/span>).input_values[<span class=3D"hljs-number">0</span>]
    batch[<span class=3D"hljs-string">"labels"</span>] =3D torch.tensor(pro=
cessor.tokenizer(batch[<span class=3D"hljs-string">"transcript"</span>]).in=
put_ids)
    <span class=3D"hljs-keyword">return</span> batch

<span class=3D"hljs-comment"># Load and preprocess dataset (replace 'your_d=
ataset' with actual dataset)</span>
<span class=3D"hljs-comment"># dataset =3D load_dataset("your_dataset")</sp=
an>
dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_aud=
io)

<span class=3D"hljs-comment"># Create data loaders</span>
<span class=3D"hljs-keyword">from</span> torch.utils.data <span class=3D"hl=
js-keyword">import</span> DataLoader

train_loader =3D DataLoader(dataset[<span class=3D"hljs-string">'train'</sp=
an>], batch_size=3D<span class=3D"hljs-number">8</span>, shuffle=3D<span cl=
ass=3D"hljs-literal">True</span>)
val_loader =3D DataLoader(dataset[<span class=3D"hljs-string">'validation'<=
/span>], batch_size=3D<span class=3D"hljs-number">8</span>)

<span class=3D"hljs-comment"># Training loop</span>
optimizer =3D torch.optim.AdamW(model.parameters(), lr=3D<span class=3D"hlj=
s-number">1e-4</span>)

<span class=3D"hljs-keyword">for</span> epoch <span class=3D"hljs-keyword">=
in</span> <span class=3D"hljs-built_in">range</span>(<span class=3D"hljs-nu=
mber">10</span>):
    model.train()
    <span class=3D"hljs-keyword">for</span> batch <span class=3D"hljs-keywo=
rd">in</span> train_loader:
        input_values =3D batch[<span class=3D"hljs-string">"input_values"</=
span>]
        labels =3D batch[<span class=3D"hljs-string">"labels"</span>]
        outputs =3D model(input_values, labels=3Dlabels)
        loss =3D outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    model.<span class=3D"hljs-built_in">eval</span>()
    val_loss =3D <span class=3D"hljs-number">0</span>
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        <span class=3D"hljs-keyword">for</span> batch <span class=3D"hljs-k=
eyword">in</span> val_loader:
            input_values =3D batch[<span class=3D"hljs-string">"input_value=
s"</span>]
            labels =3D batch[<span class=3D"hljs-string">"labels"</span>]
            outputs =3D model(input_values, labels=3Dlabels)
            val_loss +=3D outputs.loss.item()
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Epoch <span class=3D"hljs-subst">{epoch}</span>, Validation Loss: <span c=
lass=3D"hljs-subst">{val_loss/<span class=3D"hljs-built_in">len</span>(val_=
loader)}</span>"</span>)
</code></pre>
<h3 id=3D"step-5-train-a-smaller-language-model" tabindex=3D"-1"><a class=
=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-5-train-a-smaller=
-language-model" aria-hidden=3D"true">#</a> Step 5: Train a Smaller Languag=
e Model</h3>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> DistilBertF=
orMaskedLM, DistilBertTokenizer, Trainer, TrainingArguments

<span class=3D"hljs-comment"># Load tokenizer and model</span>
tokenizer =3D DistilBertTokenizer.from_pretrained(<span class=3D"hljs-strin=
g">"distilbert-base-multilingual-cased"</span>)
lm_model =3D DistilBertForMaskedLM.from_pretrained(<span class=3D"hljs-stri=
ng">"distilbert-base-multilingual-cased"</span>)

<span class=3D"hljs-comment"># Prepare dataset for language model training<=
/span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">tokenize_function</span>(<span class=3D"hljs-params">examples</span>):
    <span class=3D"hljs-keyword">return</span> tokenizer(examples[<span cla=
ss=3D"hljs-string">"text"</span>], padding=3D<span class=3D"hljs-string">"m=
ax_length"</span>, truncation=3D<span class=3D"hljs-literal">True</span>, m=
ax_length=3D<span class=3D"hljs-number">128</span>)

text_dataset =3D load_dataset(<span class=3D"hljs-string">"your_text_datase=
t"</span>)
tokenized_dataset =3D text_dataset.<span class=3D"hljs-built_in">map</span>=
(tokenize_function, batched=3D<span class=3D"hljs-literal">True</span>)
tokenized_dataset =3D tokenized_dataset.remove_columns([<span class=3D"hljs=
-string">"text"</span>])

<span class=3D"hljs-comment"># Training arguments</span>
training_args =3D TrainingArguments(
    output_dir=3D<span class=3D"hljs-string">"./results"</span>,
    evaluation_strategy=3D<span class=3D"hljs-string">"epoch"</span>,
    learning_rate=3D<span class=3D"hljs-number">1e-4</span>,
    per_device_train_batch_size=3D<span class=3D"hljs-number">16</span>,
    per_device_eval_batch_size=3D<span class=3D"hljs-number">16</span>,
    num_train_epochs=3D<span class=3D"hljs-number">3</span>,
    weight_decay=3D<span class=3D"hljs-number">0.01</span>,
)

<span class=3D"hljs-comment"># Trainer</span>
trainer =3D Trainer(
    model=3Dlm_model,
    args=3Dtraining_args,
    train_dataset=3Dtokenized_dataset[<span class=3D"hljs-string">"train"</=
span>],
    eval_dataset=3Dtokenized_dataset[<span class=3D"hljs-string">"validatio=
n"</span>],
)

<span class=3D"hljs-comment"># Train the language model</span>
trainer.train()
</code></pre>
<h3 id=3D"step-6-model-optimization-for-on-device-deployment" tabindex=3D"-=
1"><a class=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-6-mode=
l-optimization-for-on-device-deployment" aria-hidden=3D"true">#</a> Step 6:=
 Model Optimization for On-Device Deployment</h3>
<ol>
<li><strong>Quantization</strong>: Convert your models to lower precision (=
e.g., 8-bit).</li>
<li><strong>Pruning</strong>: Remove less important neurons or layers.</li>
</ol>
<p>Example: Quantization using PyTorch</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch.quantization

<span class=3D"hljs-comment"># Quantize the acoustic model</span>
model.<span class=3D"hljs-built_in">eval</span>()
quantized_model =3D torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=3Dtorch.qint8
)

<span class=3D"hljs-comment"># Save the quantized model</span>
torch.save(quantized_model.state_dict(), <span class=3D"hljs-string">"quant=
ized_acoustic_model.pth"</span>)
</code></pre>
<h3 id=3D"step-7-integration-and-inference" tabindex=3D"-1"><a class=3D"hea=
der-anchor" href=3D"https://slickgpt.com/t/20#step-7-integration-and-infere=
nce" aria-hidden=3D"true">#</a> Step 7: Integration and Inference</h3>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> Wav2Vec2Pro=
cessor, DistilBertTokenizer, DistilBertForMaskedLM
<span class=3D"hljs-keyword">import</span> torch

<span class=3D"hljs-comment"># Load quantized models</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-large-960h"</span>)
acoustic_model =3D Wav2Vec2ForCTC.from_pretrained(<span class=3D"hljs-strin=
g">"your_trained_acoustic_model"</span>)
acoustic_model.load_state_dict(torch.load(<span class=3D"hljs-string">"quan=
tized_acoustic_model.pth"</span>))
lm_model =3D DistilBertForMaskedLM.from_pretrained(<span class=3D"hljs-stri=
ng">"your_trained_language_model"</span>)
lm_tokenizer =3D DistilBertTokenizer.from_pretrained(<span class=3D"hljs-st=
ring">"distilbert-base-multilingual-cased"</span>)

<span class=3D"hljs-comment"># Inference function</span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">transcribe</span>(<span class=3D"hljs-params">audio_file</span>):
    audio_input, _ =3D torchaudio.load(audio_file)
    input_values =3D processor(audio_input.squeeze().numpy(), sampling_rate=
=3D<span class=3D"hljs-number">16000</span>).input_values[<span class=3D"hl=
js-number">0</span>]
    input_values =3D torch.tensor(input_values).unsqueeze(<span class=3D"hl=
js-number">0</span>)

    acoustic_model.<span class=3D"hljs-built_in">eval</span>()
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        logits =3D acoustic_model(input_values).logits

    predicted_ids =3D torch.argmax(logits, dim=3D-<span class=3D"hljs-numbe=
r">1</span>)
    transcription =3D processor.decode(predicted_ids[<span class=3D"hljs-nu=
mber">0</span>])

    <span class=3D"hljs-comment"># Use language model to refine transcripti=
on</span>
    lm_input =3D lm_tokenizer(transcription, return_tensors=3D<span class=
=3D"hljs-string">"pt"</span>)
    lm_output =3D lm_model(**lm_input)
    refined_transcription =3D lm_tokenizer.decode(torch.argmax(lm_output.lo=
gits, dim=3D-<span class=3D"hljs-number">1</span>)[<span class=3D"hljs-numb=
er">0</span>])

    <span class=3D"hljs-keyword">return</span> refined_transcription

<span class=3D"hljs-comment"># Example usage</span>
audio_file =3D <span class=3D"hljs-string">"path/to/korean/audio.wav"</span=
>
transcription =3D transcribe(audio_file)
<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">"Tra=
nscription:"</span>, transcription)
</code></pre>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>This guide walks you through training smaller models suitable for on-dev=
ice ASR applications, including quantization to further optimize for perfor=
mance. Using smaller architectures and model optimizations ensures that you=
r ASR system runs efficiently on smartphones and other resource-constrained=
 devices.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 1:30=
 AM">1:30 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>The size of the above models are over 500MB. I don=E2=80=
=99t think it=E2=80=99s suitable for mobile phone. What is the 8 bit quanti=
zed model size? If it=E2=80=99s still bigger than 200MB, try another. I als=
o need to use ctc for encoder and conformer for decoder.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 1:30 AM">1:30 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>U=
nderstood. To create an efficient on-device ASR system with a model size un=
der 200MB, let=E2=80=99s consider 8-bit quantization and using a smaller ar=
chitecture. Here, we=E2=80=99ll focus on using a CTC-based encoder and a Co=
nformer-based decoder.</p>
<h3 id=3D"step-1-set-up-environment" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-1-set-up-environment" aria-hidd=
en=3D"true">#</a> Step 1: Set Up Environment</h3>
<pre><code class=3D"hljs language-bash"><span class=3D"hljs-comment"># Crea=
te and activate a virtual environment (optional)</span>
python -m venv asr_env
<span class=3D"hljs-built_in">source</span> asr_env/bin/activate  <span cla=
ss=3D"hljs-comment"># On Windows use `.\asr_env\Scripts\activate`</span>

<span class=3D"hljs-comment"># Install necessary libraries</span>
pip install torch torchaudio transformers datasets sentencepiece
</code></pre>
<h3 id=3D"step-2-prepare-data" tabindex=3D"-1"><a class=3D"header-anchor" h=
ref=3D"https://slickgpt.com/t/20#step-2-prepare-data" aria-hidden=3D"true">=
#</a> Step 2: Prepare Data</h3>
<p>Ensure you have your dataset ready with audio files and corresponding tr=
anscripts.</p>
<h3 id=3D"step-3-define-a-smaller-acoustic-model-ctc" tabindex=3D"-1"><a cl=
ass=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-3-define-a-sma=
ller-acoustic-model-ctc" aria-hidden=3D"true">#</a> Step 3: Define a Smalle=
r Acoustic Model (CTC)</h3>
<p>We will use a smaller Wav2Vec2 model and then quantize it to reduce its =
size.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> Wav2Vec2Processor, Wav2Vec2ForCTC

<span class=3D"hljs-comment"># Load a smaller Wav2Vec2 model</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-base-960h"</span>)
model =3D Wav2Vec2ForCTC.from_pretrained(<span class=3D"hljs-string">"faceb=
ook/wav2vec2-base-960h"</span>)

<span class=3D"hljs-comment"># Prepare data</span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">preprocess_audio</span>(<span class=3D"hljs-params">batch</span>):
    audio, sample_rate =3D torchaudio.load(batch[<span class=3D"hljs-string=
">"audio_file"</span>])
    batch[<span class=3D"hljs-string">"input_values"</span>] =3D processor(=
audio.squeeze().numpy(), sampling_rate=3D<span class=3D"hljs-number">16000<=
/span>).input_values[<span class=3D"hljs-number">0</span>]
    batch[<span class=3D"hljs-string">"labels"</span>] =3D torch.tensor(pro=
cessor.tokenizer(batch[<span class=3D"hljs-string">"transcript"</span>]).in=
put_ids)
    <span class=3D"hljs-keyword">return</span> batch

<span class=3D"hljs-comment"># Load and preprocess dataset (replace 'your_d=
ataset' with actual dataset)</span>
dataset =3D load_dataset(<span class=3D"hljs-string">"your_dataset"</span>)
dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_aud=
io)

<span class=3D"hljs-comment"># Create data loaders</span>
<span class=3D"hljs-keyword">from</span> torch.utils.data <span class=3D"hl=
js-keyword">import</span> DataLoader

train_loader =3D DataLoader(dataset[<span class=3D"hljs-string">'train'</sp=
an>], batch_size=3D<span class=3D"hljs-number">8</span>, shuffle=3D<span cl=
ass=3D"hljs-literal">True</span>)
val_loader =3D DataLoader(dataset[<span class=3D"hljs-string">'validation'<=
/span>], batch_size=3D<span class=3D"hljs-number">8</span>)

<span class=3D"hljs-comment"># Training loop</span>
optimizer =3D torch.optim.AdamW(model.parameters(), lr=3D<span class=3D"hlj=
s-number">1e-4</span>)

<span class=3D"hljs-keyword">for</span> epoch <span class=3D"hljs-keyword">=
in</span> <span class=3D"hljs-built_in">range</span>(<span class=3D"hljs-nu=
mber">10</span>):
    model.train()
    <span class=3D"hljs-keyword">for</span> batch <span class=3D"hljs-keywo=
rd">in</span> train_loader:
        input_values =3D batch[<span class=3D"hljs-string">"input_values"</=
span>]
        labels =3D batch[<span class=3D"hljs-string">"labels"</span>]
        outputs =3D model(input_values, labels=3Dlabels)
        loss =3D outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    model.<span class=3D"hljs-built_in">eval</span>()
    val_loss =3D <span class=3D"hljs-number">0</span>
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        <span class=3D"hljs-keyword">for</span> batch <span class=3D"hljs-k=
eyword">in</span> val_loader:
            input_values =3D batch[<span class=3D"hljs-string">"input_value=
s"</span>]
            labels =3D batch[<span class=3D"hljs-string">"labels"</span>]
            outputs =3D model(input_values, labels=3Dlabels)
            val_loss +=3D outputs.loss.item()
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Epoch <span class=3D"hljs-subst">{epoch}</span>, Validation Loss: <span c=
lass=3D"hljs-subst">{val_loss/<span class=3D"hljs-built_in">len</span>(val_=
loader)}</span>"</span>)

<span class=3D"hljs-comment"># Save the trained model</span>
model.save_pretrained(<span class=3D"hljs-string">"small_wav2vec2_model"</s=
pan>)
</code></pre>
<h3 id=3D"step-4-quantize-the-acoustic-model" tabindex=3D"-1"><a class=3D"h=
eader-anchor" href=3D"https://slickgpt.com/t/20#step-4-quantize-the-acousti=
c-model" aria-hidden=3D"true">#</a> Step 4: Quantize the Acoustic Model</h3=
>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch.quantization

<span class=3D"hljs-comment"># Load the trained model</span>
model =3D Wav2Vec2ForCTC.from_pretrained(<span class=3D"hljs-string">"small=
_wav2vec2_model"</span>)

<span class=3D"hljs-comment"># Quantize the model to 8-bit</span>
model.<span class=3D"hljs-built_in">eval</span>()
quantized_model =3D torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=3Dtorch.qint8
)

<span class=3D"hljs-comment"># Save the quantized model</span>
quantized_model.save_pretrained(<span class=3D"hljs-string">"quantized_wav2=
vec2_model"</span>)

<span class=3D"hljs-comment"># Check the size</span>
<span class=3D"hljs-keyword">import</span> os
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">get_model_size</span>(<span class=3D"hljs-params">model_path</span>):
    total_size =3D <span class=3D"hljs-number">0</span>
    <span class=3D"hljs-keyword">for</span> dirpath, dirnames, filenames <s=
pan class=3D"hljs-keyword">in</span> os.walk(model_path):
        <span class=3D"hljs-keyword">for</span> f <span class=3D"hljs-keywo=
rd">in</span> filenames:
            fp =3D os.path.join(dirpath, f)
            total_size +=3D os.path.getsize(fp)
    <span class=3D"hljs-keyword">return</span> total_size / (<span class=3D=
"hljs-number">1024</span> * <span class=3D"hljs-number">1024</span>)  <span=
 class=3D"hljs-comment"># Convert to MB</span>

<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">f"Qu=
antized Model Size: <span class=3D"hljs-subst">{get_model_size(<span class=
=3D"hljs-string">'quantized_wav2vec2_model'</span>)}</span> MB"</span>)
</code></pre>
<h3 id=3D"step-5-define-a-smaller-language-model-distilbert" tabindex=3D"-1=
"><a class=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-5-defin=
e-a-smaller-language-model-distilbert" aria-hidden=3D"true">#</a> Step 5: D=
efine a Smaller Language Model (DistilBERT)</h3>
<p>Train a smaller language model and quantize it.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> DistilBertF=
orMaskedLM, DistilBertTokenizer, Trainer, TrainingArguments

<span class=3D"hljs-comment"># Load tokenizer and model</span>
tokenizer =3D DistilBertTokenizer.from_pretrained(<span class=3D"hljs-strin=
g">"distilbert-base-multilingual-cased"</span>)
lm_model =3D DistilBertForMaskedLM.from_pretrained(<span class=3D"hljs-stri=
ng">"distilbert-base-multilingual-cased"</span>)

<span class=3D"hljs-comment"># Prepare dataset for language model training<=
/span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">tokenize_function</span>(<span class=3D"hljs-params">examples</span>):
    <span class=3D"hljs-keyword">return</span> tokenizer(examples[<span cla=
ss=3D"hljs-string">"text"</span>], padding=3D<span class=3D"hljs-string">"m=
ax_length"</span>, truncation=3D<span class=3D"hljs-literal">True</span>, m=
ax_length=3D<span class=3D"hljs-number">128</span>)

text_dataset =3D load_dataset(<span class=3D"hljs-string">"your_text_datase=
t"</span>)
tokenized_dataset =3D text_dataset.<span class=3D"hljs-built_in">map</span>=
(tokenize_function, batched=3D<span class=3D"hljs-literal">True</span>)
tokenized_dataset =3D tokenized_dataset.remove_columns([<span class=3D"hljs=
-string">"text"</span>])

<span class=3D"hljs-comment"># Training arguments</span>
training_args =3D TrainingArguments(
    output_dir=3D<span class=3D"hljs-string">"./results"</span>,
    evaluation_strategy=3D<span class=3D"hljs-string">"epoch"</span>,
    learning_rate=3D<span class=3D"hljs-number">1e-4</span>,
    per_device_train_batch_size=3D<span class=3D"hljs-number">16</span>,
    per_device_eval_batch_size=3D<span class=3D"hljs-number">16</span>,
    num_train_epochs=3D<span class=3D"hljs-number">3</span>,
    weight_decay=3D<span class=3D"hljs-number">0.01</span>,
)

<span class=3D"hljs-comment"># Trainer</span>
trainer =3D Trainer(
    model=3Dlm_model,
    args=3Dtraining_args,
    train_dataset=3Dtokenized_dataset[<span class=3D"hljs-string">"train"</=
span>],
    eval_dataset=3Dtokenized_dataset[<span class=3D"hljs-string">"validatio=
n"</span>],
)

<span class=3D"hljs-comment"># Train the language model</span>
trainer.train()
</code></pre>
<h3 id=3D"step-6-quantize-the-language-model" tabindex=3D"-1"><a class=3D"h=
eader-anchor" href=3D"https://slickgpt.com/t/20#step-6-quantize-the-languag=
e-model" aria-hidden=3D"true">#</a> Step 6: Quantize the Language Model</h3=
>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-comment"># Qu=
antize the language model to 8-bit</span>
lm_model.<span class=3D"hljs-built_in">eval</span>()
quantized_lm_model =3D torch.quantization.quantize_dynamic(
    lm_model, {torch.nn.Linear}, dtype=3Dtorch.qint8
)

<span class=3D"hljs-comment"># Save the quantized language model</span>
quantized_lm_model.save_pretrained(<span class=3D"hljs-string">"quantized_d=
istilbert_model"</span>)

<span class=3D"hljs-comment"># Check the size</span>
<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">f"Qu=
antized LM Model Size: <span class=3D"hljs-subst">{get_model_size(<span cla=
ss=3D"hljs-string">'quantized_distilbert_model'</span>)}</span> MB"</span>)
</code></pre>
<h3 id=3D"step-7-integration-and-inference" tabindex=3D"-1"><a class=3D"hea=
der-anchor" href=3D"https://slickgpt.com/t/20#step-7-integration-and-infere=
nce" aria-hidden=3D"true">#</a> Step 7: Integration and Inference</h3>
<p>Combine the quantized acoustic and language models for inference.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> Wav2Vec2Pro=
cessor, DistilBertTokenizer, DistilBertForMaskedLM
<span class=3D"hljs-keyword">import</span> torch

<span class=3D"hljs-comment"># Load quantized models</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-base-960h"</span>)
acoustic_model =3D Wav2Vec2ForCTC.from_pretrained(<span class=3D"hljs-strin=
g">"quantized_wav2vec2_model"</span>)
lm_model =3D DistilBertForMaskedLM.from_pretrained(<span class=3D"hljs-stri=
ng">"quantized_distilbert_model"</span>)
lm_tokenizer =3D DistilBertTokenizer.from_pretrained(<span class=3D"hljs-st=
ring">"distilbert-base-multilingual-cased"</span>)

<span class=3D"hljs-comment"># Inference function</span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">transcribe</span>(<span class=3D"hljs-params">audio_file</span>):
    audio_input, _ =3D torchaudio.load(audio_file)
    input_values =3D processor(audio_input.squeeze().numpy(), sampling_rate=
=3D<span class=3D"hljs-number">16000</span>).input_values[<span class=3D"hl=
js-number">0</span>]
    input_values =3D torch.tensor(input_values).unsqueeze(<span class=3D"hl=
js-number">0</span>)

    acoustic_model.<span class=3D"hljs-built_in">eval</span>()
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        logits =3D acoustic_model(input_values).logits

    predicted_ids =3D torch.argmax(logits, dim=3D-<span class=3D"hljs-numbe=
r">1</span>)
    transcription =3D processor.decode(predicted_ids[<span class=3D"hljs-nu=
mber">0</span>])

    <span class=3D"hljs-comment"># Use language model to refine transcripti=
on</span>
    lm_input =3D lm_tokenizer(transcription, return_tensors=3D<span class=
=3D"hljs-string">"pt"</span>)
    lm_output =3D lm_model(**lm_input)
    refined_transcription =3D lm_tokenizer.decode(torch.argmax(lm_output.lo=
gits, dim=3D-<span class=3D"hljs-number">1</span>)[<span class=3D"hljs-numb=
er">0</span>])

    <span class=3D"hljs-keyword">return</span> refined_transcription

<span class=3D"hljs-comment"># Example usage</span>
audio_file =3D <span class=3D"hljs-string">"path/to/korean/audio.wav"</span=
>
transcription =3D transcribe(audio_file)
<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">"Tra=
nscription:"</span>, transcription)
</code></pre>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>By using smaller models and applying 8-bit quantization, you can signifi=
cantly reduce the model size, making it suitable for on-device applications=
. The goal is to ensure both the acoustic and language models fit within th=
e 200MB constraint while maintaining reasonable performance.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 1:42=
 AM">1:42 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>The size of the wav2vec2-base model is 378M and the one =
of distilbert-base is 542MB. What is the size of the quantized one respecti=
vely?</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 1:42 AM">1:42 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>T=
he size of the quantized models can vary based on the specific implementati=
on and the extent of quantization. However, let=E2=80=99s estimate the size=
 reduction you can expect from dynamic quantization, which typically reduce=
s the model size by approximately 3-4 times.</p>
<h3 id=3D"estimating-quantized-model-sizes" tabindex=3D"-1"><a class=3D"hea=
der-anchor" href=3D"https://slickgpt.com/t/20#estimating-quantized-model-si=
zes" aria-hidden=3D"true">#</a> Estimating Quantized Model Sizes</h3>
<ol>
<li>
<p><strong>Wav2Vec2-Base Model</strong>:</p>
<ul>
<li>Original size: 378MB</li>
<li>Estimated size after 8-bit quantization: 378MB / 4 =E2=89=88 95MB</li>
</ul>
</li>
<li>
<p><strong>DistilBERT-Base Model</strong>:</p>
<ul>
<li>Original size: 542MB</li>
<li>Estimated size after 8-bit quantization: 542MB / 4 =E2=89=88 135MB</li>
</ul>
</li>
</ol>
<p>Given these estimates, both models should fit within the 200MB constrain=
t after quantization.</p>
<h3 id=3D"quantization-code-example" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#quantization-code-example" aria-hidd=
en=3D"true">#</a> Quantization Code Example</h3>
<p>Here=E2=80=99s a code example to quantize the models and check their siz=
es:</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> Wav2Vec2ForCTC, Wav2Vec2Processor, DistilBertForMaske=
dLM, DistilBertTokenizer
<span class=3D"hljs-keyword">import</span> os

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">get_model_size</span>(<span class=3D"hljs-params">model_path</span>):
    total_size =3D <span class=3D"hljs-number">0</span>
    <span class=3D"hljs-keyword">for</span> dirpath, dirnames, filenames <s=
pan class=3D"hljs-keyword">in</span> os.walk(model_path):
        <span class=3D"hljs-keyword">for</span> f <span class=3D"hljs-keywo=
rd">in</span> filenames:
            fp =3D os.path.join(dirpath, f)
            total_size +=3D os.path.getsize(fp)
    <span class=3D"hljs-keyword">return</span> total_size / (<span class=3D=
"hljs-number">1024</span> * <span class=3D"hljs-number">1024</span>)  <span=
 class=3D"hljs-comment"># Convert to MB</span>

<span class=3D"hljs-comment"># Quantize Wav2Vec2-Base Model</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-base-960h"</span>)
acoustic_model =3D Wav2Vec2ForCTC.from_pretrained(<span class=3D"hljs-strin=
g">"facebook/wav2vec2-base-960h"</span>)

quantized_acoustic_model =3D torch.quantization.quantize_dynamic(
    acoustic_model, {torch.nn.Linear}, dtype=3Dtorch.qint8
)
quantized_acoustic_model.save_pretrained(<span class=3D"hljs-string">"quant=
ized_wav2vec2_model"</span>)

<span class=3D"hljs-comment"># Quantize DistilBERT-Base Model</span>
lm_model =3D DistilBertForMaskedLM.from_pretrained(<span class=3D"hljs-stri=
ng">"distilbert-base-multilingual-cased"</span>)
quantized_lm_model =3D torch.quantization.quantize_dynamic(
    lm_model, {torch.nn.Linear}, dtype=3Dtorch.qint8
)
quantized_lm_model.save_pretrained(<span class=3D"hljs-string">"quantized_d=
istilbert_model"</span>)

<span class=3D"hljs-comment"># Check sizes</span>
acoustic_model_size =3D get_model_size(<span class=3D"hljs-string">"quantiz=
ed_wav2vec2_model"</span>)
lm_model_size =3D get_model_size(<span class=3D"hljs-string">"quantized_dis=
tilbert_model"</span>)

<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">f"Qu=
antized Wav2Vec2 Model Size: <span class=3D"hljs-subst">{acoustic_model_siz=
e}</span> MB"</span>)
<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">f"Qu=
antized DistilBERT Model Size: <span class=3D"hljs-subst">{lm_model_size}</=
span> MB"</span>)
</code></pre>
<h3 id=3D"running-the-quantization" tabindex=3D"-1"><a class=3D"header-anch=
or" href=3D"https://slickgpt.com/t/20#running-the-quantization" aria-hidden=
=3D"true">#</a> Running the Quantization</h3>
<p>Run the above code to quantize the models and determine their sizes. Typ=
ically, the quantized models should be significantly smaller and more suita=
ble for deployment on mobile devices.</p>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>Quantizing both the Wav2Vec2-Base and DistilBERT-Base models should brin=
g their sizes within the required limits for mobile applications. The estim=
ated sizes after quantization are:</p>
<ul>
<li>Quantized Wav2Vec2-Base: ~95MB</li>
<li>Quantized DistilBERT-Base: ~135MB</li>
</ul>
<p>Combined, these models should fit comfortably within a 200MB constraint.=
 Feel free to run the provided code to get the exact sizes and verify the r=
eductions.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 1:44=
 AM">1:44 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>What is the inference time when run this model on mobile=
 phone of MT6789 cpu and 4GB RAM? input audio size is 3~5s</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 1:44 AM">1:44 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>T=
he inference time for running an ASR model on a mobile device, such as one =
with an MT6789 CPU and 4GB of RAM, depends on several factors including the=
 efficiency of the model, the optimization techniques used, and the specifi=
c implementation details.</p>
<h3 id=3D"key-factors-affecting-inference-time" tabindex=3D"-1"><a class=3D=
"header-anchor" href=3D"https://slickgpt.com/t/20#key-factors-affecting-inf=
erence-time" aria-hidden=3D"true">#</a> Key Factors Affecting Inference Tim=
e</h3>
<ol>
<li><strong>Model Size and Complexity</strong>: Smaller and simpler models =
typically infer faster.</li>
<li><strong>Quantization</strong>: Reducing model precision (e.g., to 8-bit=
) can significantly improve speed.</li>
<li><strong>Framework and Backend</strong>: The library and backend used fo=
r inference (e.g., PyTorch Mobile, TensorFlow Lite) can impact performance.=
</li>
<li><strong>Hardware</strong>: The performance of the MT6789 CPU and availa=
ble RAM play crucial roles.</li>
</ol>
<h3 id=3D"example-estimation" tabindex=3D"-1"><a class=3D"header-anchor" hr=
ef=3D"https://slickgpt.com/t/20#example-estimation" aria-hidden=3D"true">#<=
/a> Example Estimation</h3>
<p>Given that we are quantizing both the Wav2Vec2 (CTC-based encoder) and D=
istilBERT (language model) to 8-bit, let=E2=80=99s consider some rough esti=
mates.</p>
<h3 id=3D"steps-to-measure-inference-time" tabindex=3D"-1"><a class=3D"head=
er-anchor" href=3D"https://slickgpt.com/t/20#steps-to-measure-inference-tim=
e" aria-hidden=3D"true">#</a> Steps to Measure Inference Time</h3>
<ol>
<li>
<p><strong>Convert Models to Mobile-Friendly Format</strong>:</p>
<ul>
<li>Use PyTorch Mobile or TensorFlow Lite for deployment.</li>
<li>Quantize the models as previously shown.</li>
</ul>
</li>
<li>
<p><strong>Measure Inference Time</strong>:</p>
<ul>
<li>Load the models on the mobile device.</li>
<li>Measure the time taken to process 3-5 seconds of audio.</li>
</ul>
</li>
</ol>
<h3 id=3D"example-code-for-measuring-inference-time-on-mobile" tabindex=3D"=
-1"><a class=3D"header-anchor" href=3D"https://slickgpt.com/t/20#example-co=
de-for-measuring-inference-time-on-mobile" aria-hidden=3D"true">#</a> Examp=
le Code for Measuring Inference Time on Mobile</h3>
<p>Here=E2=80=99s a Python example using PyTorch Mobile to measure the infe=
rence time. You would need to run this on the mobile device to get accurate=
 measurements.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch
<span class=3D"hljs-keyword">import</span> torchaudio
<span class=3D"hljs-keyword">import</span> time
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> Wav2Vec2Processor, DistilBertTokenizer

<span class=3D"hljs-comment"># Load quantized models</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-base-960h"</span>)
acoustic_model =3D torch.jit.load(<span class=3D"hljs-string">"quantized_wa=
v2vec2_model/pytorch_model.bin"</span>)
lm_model =3D torch.jit.load(<span class=3D"hljs-string">"quantized_distilbe=
rt_model/pytorch_model.bin"</span>)
lm_tokenizer =3D DistilBertTokenizer.from_pretrained(<span class=3D"hljs-st=
ring">"distilbert-base-multilingual-cased"</span>)

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">measure_inference_time</span>(<span class=3D"hljs-params">audio_file</spa=
n>):
    audio_input, _ =3D torchaudio.load(audio_file)
    input_values =3D processor(audio_input.squeeze().numpy(), sampling_rate=
=3D<span class=3D"hljs-number">16000</span>).input_values[<span class=3D"hl=
js-number">0</span>]
    input_values =3D torch.tensor(input_values).unsqueeze(<span class=3D"hl=
js-number">0</span>)

    <span class=3D"hljs-comment"># Measure acoustic model inference time</s=
pan>
    start_time =3D time.time()
    acoustic_model.<span class=3D"hljs-built_in">eval</span>()
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        logits =3D acoustic_model(input_values).logits
    acoustic_end_time =3D time.time()

    predicted_ids =3D torch.argmax(logits, dim=3D-<span class=3D"hljs-numbe=
r">1</span>)
    transcription =3D processor.decode(predicted_ids[<span class=3D"hljs-nu=
mber">0</span>])

    <span class=3D"hljs-comment"># Measure language model inference time</s=
pan>
    lm_input =3D lm_tokenizer(transcription, return_tensors=3D<span class=
=3D"hljs-string">"pt"</span>)
    lm_model.<span class=3D"hljs-built_in">eval</span>()
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        lm_output =3D lm_model(**lm_input)
    lm_end_time =3D time.time()

    refined_transcription =3D lm_tokenizer.decode(torch.argmax(lm_output.lo=
gits, dim=3D-<span class=3D"hljs-number">1</span>)[<span class=3D"hljs-numb=
er">0</span>])

    acoustic_inference_time =3D acoustic_end_time - start_time
    lm_inference_time =3D lm_end_time - acoustic_end_time
    total_inference_time =3D lm_end_time - start_time

    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Acoustic Model Inference Time: <span class=3D"hljs-subst">{acoustic_infer=
ence_time:<span class=3D"hljs-number">.4</span>f}</span> seconds"</span>)
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Language Model Inference Time: <span class=3D"hljs-subst">{lm_inference_t=
ime:<span class=3D"hljs-number">.4</span>f}</span> seconds"</span>)
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Total Inference Time: <span class=3D"hljs-subst">{total_inference_time:<s=
pan class=3D"hljs-number">.4</span>f}</span> seconds"</span>)

    <span class=3D"hljs-keyword">return</span> refined_transcription, total=
_inference_time

<span class=3D"hljs-comment"># Example usage</span>
audio_file =3D <span class=3D"hljs-string">"path/to/3-5s/korean/audio.wav"<=
/span>
transcription, inference_time =3D measure_inference_time(audio_file)
<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">"Tra=
nscription:"</span>, transcription)
</code></pre>
<h3 id=3D"expected-inference-time" tabindex=3D"-1"><a class=3D"header-ancho=
r" href=3D"https://slickgpt.com/t/20#expected-inference-time" aria-hidden=
=3D"true">#</a> Expected Inference Time</h3>
<p>Based on similar setups and optimizations, the total inference time for =
a 3-5 second audio clip on a mid-range mobile CPU like the MT6789 can be ex=
pected to be within the range of 1-2 seconds, but this can vary.</p>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>To get the exact inference time, it is essential to run the model on the=
 actual device. The provided code will help you measure the inference time =
accurately. With proper quantization and optimization, achieving an inferen=
ce time of around 1-2 seconds for a 3-5 second audio clip is reasonable on =
a device with an MT6789 CPU and 4GB RAM.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 1:54=
 AM">1:54 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>do you really think fine-tuning is better than training =
from the scratch for this case? Korean ASR you know</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 1:54 AM">1:54 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>I=
n the context of developing a Korean ASR system, the choice between fine-tu=
ning pre-trained models and training models from scratch depends on several=
 factors, including available resources, data, and desired performance. Her=
e=E2=80=99s a detailed comparison to help you make an informed decision:</p=
>
<h3 id=3D"fine-tuning-pre-trained-models" tabindex=3D"-1"><a class=3D"heade=
r-anchor" href=3D"https://slickgpt.com/t/20#fine-tuning-pre-trained-models"=
 aria-hidden=3D"true">#</a> Fine-Tuning Pre-Trained Models</h3>
<p><strong>Advantages:</strong></p>
<ol>
<li><strong>Data Efficiency</strong>: Requires less data compared to traini=
ng from scratch. Pre-trained models already have learned useful representat=
ions from large datasets, typically in multiple languages.</li>
<li><strong>Speed</strong>: Significantly faster to fine-tune a pre-trained=
 model than to train from scratch.</li>
<li><strong>Performance</strong>: Pre-trained models often achieve better p=
erformance with less data and training time due to their pre-learned knowle=
dge.</li>
</ol>
<p><strong>Disadvantages:</strong></p>
<ol>
<li><strong>Pre-trained Models Availability</strong>: The effectiveness of =
fine-tuning depends on the quality and relevance of the available pre-train=
ed models.</li>
<li><strong>Specialization</strong>: If the pre-trained model isn=E2=80=99t=
 well-suited to Korean, its performance might be suboptimal.</li>
</ol>
<p><strong>Recommended Approach for Fine-Tuning:</strong></p>
<ol>
<li><strong>Select a Pre-Trained Model</strong>: Choose models like Wav2Vec=
2 or HuBERT that support multiple languages.</li>
<li><strong>Fine-Tuning</strong>: Fine-tune the pre-trained model on a Kore=
an-specific dataset.</li>
</ol>
<h3 id=3D"training-from-scratch" tabindex=3D"-1"><a class=3D"header-anchor"=
 href=3D"https://slickgpt.com/t/20#training-from-scratch" aria-hidden=3D"tr=
ue">#</a> Training from Scratch</h3>
<p><strong>Advantages:</strong></p>
<ol>
<li><strong>Customization</strong>: Allows for full control over the archit=
ecture and training process, tailored specifically for Korean.</li>
<li><strong>Potentially Better Results</strong>: For languages or dialects =
with unique phonetic and linguistic characteristics, training from scratch =
might yield better results.</li>
</ol>
<p><strong>Disadvantages:</strong></p>
<ol>
<li><strong>Resource Intensive</strong>: Requires significant computational=
 resources and time.</li>
<li><strong>Data Requirement</strong>: Needs a large amount of high-quality=
 labeled data to achieve good performance.</li>
<li><strong>Complexity</strong>: More complex in terms of implementation an=
d optimization.</li>
</ol>
<p><strong>Recommended Approach for Training from Scratch:</strong></p>
<ol>
<li><strong>Data Collection</strong>: Gather a large and diverse Korean spe=
ech dataset.</li>
<li><strong>Model Architecture</strong>: Design or choose an ASR architectu=
re suitable for the task (e.g., Conformer for encoder, Transformer for deco=
der).</li>
<li><strong>Training</strong>: Train the model using extensive computationa=
l resources.</li>
</ol>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>Given the context of developing a Korean ASR system:</p>
<ul>
<li><strong>Fine-Tuning</strong> is generally recommended if you have acces=
s to high-quality pre-trained models and a limited amount of Korean-specifi=
c data. This approach is faster, more resource-efficient, and can achieve g=
ood performance with less effort.</li>
<li><strong>Training from Scratch</strong> might be necessary if you have a=
mple resources and a large, high-quality Korean dataset, or if the pre-trai=
ned models do not perform well on Korean.</li>
</ul>
<h3 id=3D"example-fine-tuning-a-wav2vec2-model-for-korean-asr" tabindex=3D"=
-1"><a class=3D"header-anchor" href=3D"https://slickgpt.com/t/20#example-fi=
ne-tuning-a-wav2vec2-model-for-korean-asr" aria-hidden=3D"true">#</a> Examp=
le: Fine-Tuning a Wav2Vec2 Model for Korean ASR</h3>
<p>Here=E2=80=99s a concise guide to fine-tuning a pre-trained Wav2Vec2 mod=
el for Korean ASR:</p>
<ol>
<li>
<p><strong>Set Up Environment</strong></p>
<pre><code class=3D"hljs language-bash">python -m venv asr_env
<span class=3D"hljs-built_in">source</span> asr_env/bin/activate  <span cla=
ss=3D"hljs-comment"># On Windows use `.\asr_env\Scripts\activate`</span>
pip install torch torchaudio transformers datasets
</code></pre>
</li>
<li>
<p><strong>Prepare Data</strong><br>
Ensure you have a labeled Korean speech dataset.</p>
</li>
<li>
<p><strong>Load Pre-Trained Model and Tokenizer</strong></p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> Wav2Vec2For=
CTC, Wav2Vec2Processor
<span class=3D"hljs-keyword">from</span> datasets <span class=3D"hljs-keywo=
rd">import</span> load_dataset

<span class=3D"hljs-comment"># Load pre-trained model and processor</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-large-xlsr-53"</span>)
model =3D Wav2Vec2ForCTC.from_pretrained(<span class=3D"hljs-string">"faceb=
ook/wav2vec2-large-xlsr-53"</span>)

<span class=3D"hljs-comment"># Load and preprocess dataset</span>
dataset =3D load_dataset(<span class=3D"hljs-string">"your_dataset_name"</s=
pan>)

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">preprocess_data</span>(<span class=3D"hljs-params">batch</span>):
    audio =3D batch[<span class=3D"hljs-string">"audio"</span>][<span class=
=3D"hljs-string">"array"</span>]
    batch[<span class=3D"hljs-string">"input_values"</span>] =3D processor(=
audio, sampling_rate=3D<span class=3D"hljs-number">16000</span>).input_valu=
es[<span class=3D"hljs-number">0</span>]
    batch[<span class=3D"hljs-string">"labels"</span>] =3D processor.tokeni=
zer(batch[<span class=3D"hljs-string">"transcript"</span>]).input_ids
    <span class=3D"hljs-keyword">return</span> batch

dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_dat=
a, remove_columns=3D[<span class=3D"hljs-string">"audio"</span>, <span clas=
s=3D"hljs-string">"transcript"</span>])
</code></pre>
</li>
<li>
<p><strong>Fine-Tuning</strong></p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> TrainingArg=
uments, Trainer

<span class=3D"hljs-comment"># Define training arguments</span>
training_args =3D TrainingArguments(
    output_dir=3D<span class=3D"hljs-string">"./results"</span>,
    evaluation_strategy=3D<span class=3D"hljs-string">"epoch"</span>,
    learning_rate=3D<span class=3D"hljs-number">1e-4</span>,
    per_device_train_batch_size=3D<span class=3D"hljs-number">16</span>,
    per_device_eval_batch_size=3D<span class=3D"hljs-number">16</span>,
    num_train_epochs=3D<span class=3D"hljs-number">3</span>,
    weight_decay=3D<span class=3D"hljs-number">0.01</span>,
)

<span class=3D"hljs-comment"># Define Trainer</span>
trainer =3D Trainer(
    model=3Dmodel,
    args=3Dtraining_args,
    train_dataset=3Ddataset[<span class=3D"hljs-string">"train"</span>],
    eval_dataset=3Ddataset[<span class=3D"hljs-string">"validation"</span>]=
,
)

<span class=3D"hljs-comment"># Fine-tune the model</span>
trainer.train()
</code></pre>
</li>
</ol>
<p>By fine-tuning a pre-trained model like Wav2Vec2, you leverage existing =
knowledge and adapt it to the Korean language, making it a practical and ef=
ficient approach for most use cases.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 1:56=
 AM">1:56 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>Give me training from scratch scenario.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 1:56 AM">1:56 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>T=
raining an ASR model from scratch for Korean requires substantial computati=
onal resources, a large dataset, and careful tuning. Below, I=E2=80=99ll ou=
tline the steps to train a CTC-based encoder (such as a Conformer) and a Co=
nformer-based decoder from scratch, using PyTorch.</p>
<h3 id=3D"step-1-set-up-environment" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-1-set-up-environment" aria-hidd=
en=3D"true">#</a> Step 1: Set Up Environment</h3>
<pre><code class=3D"hljs language-bash"><span class=3D"hljs-comment"># Crea=
te and activate a virtual environment (optional)</span>
python -m venv asr_env
<span class=3D"hljs-built_in">source</span> asr_env/bin/activate  <span cla=
ss=3D"hljs-comment"># On Windows use `.\asr_env\Scripts\activate`</span>

<span class=3D"hljs-comment"># Install necessary libraries</span>
pip install torch torchaudio transformers datasets
</code></pre>
<h3 id=3D"step-2-prepare-the-data" tabindex=3D"-1"><a class=3D"header-ancho=
r" href=3D"https://slickgpt.com/t/20#step-2-prepare-the-data" aria-hidden=
=3D"true">#</a> Step 2: Prepare the Data</h3>
<p>Ensure you have a large, high-quality Korean speech dataset with corresp=
onding transcripts. This guide assumes the dataset is in a format compatibl=
e with the Hugging Face <code>datasets</code> library.</p>
<h3 id=3D"step-3-define-the-model-architecture" tabindex=3D"-1"><a class=3D=
"header-anchor" href=3D"https://slickgpt.com/t/20#step-3-define-the-model-a=
rchitecture" aria-hidden=3D"true">#</a> Step 3: Define the Model Architectu=
re</h3>
<p>We=E2=80=99ll define a simple Conformer-based ASR model.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch
<span class=3D"hljs-keyword">import</span> torch.nn <span class=3D"hljs-key=
word">as</span> nn
<span class=3D"hljs-keyword">import</span> torchaudio
<span class=3D"hljs-keyword">from</span> torchaudio.models <span class=3D"h=
ljs-keyword">import</span> Conformer
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> Wav2Vec2Processor
<span class=3D"hljs-keyword">from</span> datasets <span class=3D"hljs-keywo=
rd">import</span> load_dataset

<span class=3D"hljs-keyword">class</span> <span class=3D"hljs-title class_"=
>ASRModel</span>(nn.Module):
    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__init__</span>(<span class=3D"hljs-params">self</span>):
        <span class=3D"hljs-built_in">super</span>(ASRModel, self).__init__=
()
        self.processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D=
"hljs-string">"facebook/wav2vec2-base"</span>)
        self.encoder =3D Conformer(
            input_dim=3D<span class=3D"hljs-number">80</span>,  <span class=
=3D"hljs-comment"># Number of mel frequency bins</span>
            num_heads=3D<span class=3D"hljs-number">4</span>,
            ffn_dim=3D<span class=3D"hljs-number">256</span>,
            num_layers=3D<span class=3D"hljs-number">6</span>,
            dropout=3D<span class=3D"hljs-number">0.1</span>,
        )
        self.decoder =3D nn.Linear(<span class=3D"hljs-number">256</span>, =
self.processor.tokenizer.vocab_size)

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">forward</span>(<span class=3D"hljs-params">self, input_values, attent=
ion_mask=3D<span class=3D"hljs-literal">None</span>, labels=3D<span class=
=3D"hljs-literal">None</span></span>):
        x, _ =3D torchaudio.compliance.kaldi.fbank(input_values, num_mel_bi=
ns=3D<span class=3D"hljs-number">80</span>)
        x =3D self.encoder(x)
        logits =3D self.decoder(x)
        <span class=3D"hljs-keyword">return</span> logits

<span class=3D"hljs-comment"># Instantiate the model</span>
model =3D ASRModel()
</code></pre>
<h3 id=3D"step-4-preprocess-data" tabindex=3D"-1"><a class=3D"header-anchor=
" href=3D"https://slickgpt.com/t/20#step-4-preprocess-data" aria-hidden=3D"=
true">#</a> Step 4: Preprocess Data</h3>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-comment"># Lo=
ad the dataset</span>
dataset =3D load_dataset(<span class=3D"hljs-string">"your_dataset_name"</s=
pan>)

<span class=3D"hljs-comment"># Preprocess data</span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">preprocess_data</span>(<span class=3D"hljs-params">batch</span>):
    audio =3D batch[<span class=3D"hljs-string">"audio"</span>][<span class=
=3D"hljs-string">"array"</span>]
    batch[<span class=3D"hljs-string">"input_values"</span>] =3D processor(=
audio, sampling_rate=3D<span class=3D"hljs-number">16000</span>).input_valu=
es[<span class=3D"hljs-number">0</span>]
    batch[<span class=3D"hljs-string">"labels"</span>] =3D processor.tokeni=
zer(batch[<span class=3D"hljs-string">"transcript"</span>]).input_ids
    <span class=3D"hljs-keyword">return</span> batch

dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_dat=
a, remove_columns=3D[<span class=3D"hljs-string">"audio"</span>, <span clas=
s=3D"hljs-string">"transcript"</span>])
</code></pre>
<h3 id=3D"step-5-create-data-loaders" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-5-create-data-loaders" aria-hi=
dden=3D"true">#</a> Step 5: Create Data Loaders</h3>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> torch.utils.data <span class=3D"hljs-keyword">import</span> DataLoa=
der, Dataset

<span class=3D"hljs-keyword">class</span> <span class=3D"hljs-title class_"=
>ASRDataset</span>(<span class=3D"hljs-title class_ inherited__">Dataset</s=
pan>):
    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__init__</span>(<span class=3D"hljs-params">self, dataset</span>):
        self.dataset =3D dataset

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__len__</span>(<span class=3D"hljs-params">self</span>):
        <span class=3D"hljs-keyword">return</span> <span class=3D"hljs-buil=
t_in">len</span>(self.dataset)

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__getitem__</span>(<span class=3D"hljs-params">self, idx</span>):
        item =3D self.dataset[idx]
        <span class=3D"hljs-keyword">return</span> {
            <span class=3D"hljs-string">"input_values"</span>: torch.tensor=
(item[<span class=3D"hljs-string">"input_values"</span>], dtype=3Dtorch.flo=
at32),
            <span class=3D"hljs-string">"labels"</span>: torch.tensor(item[=
<span class=3D"hljs-string">"labels"</span>], dtype=3Dtorch.int64),
        }

train_dataset =3D ASRDataset(dataset[<span class=3D"hljs-string">"train"</s=
pan>])
val_dataset =3D ASRDataset(dataset[<span class=3D"hljs-string">"validation"=
</span>])

train_loader =3D DataLoader(train_dataset, batch_size=3D<span class=3D"hljs=
-number">16</span>, shuffle=3D<span class=3D"hljs-literal">True</span>)
val_loader =3D DataLoader(val_dataset, batch_size=3D<span class=3D"hljs-num=
ber">16</span>)
</code></pre>
<h3 id=3D"step-6-define-training-loop" tabindex=3D"-1"><a class=3D"header-a=
nchor" href=3D"https://slickgpt.com/t/20#step-6-define-training-loop" aria-=
hidden=3D"true">#</a> Step 6: Define Training Loop</h3>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch.optim <span class=3D"hljs-keyword">as</span> optim
<span class=3D"hljs-keyword">from</span> torch.nn <span class=3D"hljs-keywo=
rd">import</span> CTCLoss

<span class=3D"hljs-comment"># Define loss function and optimizer</span>
criterion =3D CTCLoss(blank=3Dprocessor.tokenizer.pad_token_id)
optimizer =3D optim.AdamW(model.parameters(), lr=3D<span class=3D"hljs-numb=
er">1e-4</span>)

<span class=3D"hljs-comment"># Training loop</span>
num_epochs =3D <span class=3D"hljs-number">10</span>

<span class=3D"hljs-keyword">for</span> epoch <span class=3D"hljs-keyword">=
in</span> <span class=3D"hljs-built_in">range</span>(num_epochs):
    model.train()
    running_loss =3D <span class=3D"hljs-number">0.0</span>
    <span class=3D"hljs-keyword">for</span> batch <span class=3D"hljs-keywo=
rd">in</span> train_loader:
        input_values =3D batch[<span class=3D"hljs-string">"input_values"</=
span>]
        labels =3D batch[<span class=3D"hljs-string">"labels"</span>]

        optimizer.zero_grad()
        logits =3D model(input_values)
       =20
        <span class=3D"hljs-comment"># Compute loss</span>
        input_lengths =3D torch.full((logits.size(<span class=3D"hljs-numbe=
r">0</span>),), logits.size(<span class=3D"hljs-number">1</span>), dtype=3D=
torch.long)
        target_lengths =3D torch.tensor([<span class=3D"hljs-built_in">len<=
/span>(label) <span class=3D"hljs-keyword">for</span> label <span class=3D"=
hljs-keyword">in</span> labels], dtype=3Dtorch.long)
        loss =3D criterion(logits, labels, input_lengths, target_lengths)
       =20
        loss.backward()
        optimizer.step()
       =20
        running_loss +=3D loss.item()
   =20
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Epoch <span class=3D"hljs-subst">{epoch+<span class=3D"hljs-number">1</sp=
an>}</span>/<span class=3D"hljs-subst">{num_epochs}</span>, Loss: <span cla=
ss=3D"hljs-subst">{running_loss/<span class=3D"hljs-built_in">len</span>(tr=
ain_loader)}</span>"</span>)
   =20
    <span class=3D"hljs-comment"># Evaluation</span>
    model.<span class=3D"hljs-built_in">eval</span>()
    val_loss =3D <span class=3D"hljs-number">0.0</span>
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        <span class=3D"hljs-keyword">for</span> batch <span class=3D"hljs-k=
eyword">in</span> val_loader:
            input_values =3D batch[<span class=3D"hljs-string">"input_value=
s"</span>]
            labels =3D batch[<span class=3D"hljs-string">"labels"</span>]
           =20
            logits =3D model(input_values)
           =20
            <span class=3D"hljs-comment"># Compute loss</span>
            input_lengths =3D torch.full((logits.size(<span class=3D"hljs-n=
umber">0</span>),), logits.size(<span class=3D"hljs-number">1</span>), dtyp=
e=3Dtorch.long)
            target_lengths =3D torch.tensor([<span class=3D"hljs-built_in">=
len</span>(label) <span class=3D"hljs-keyword">for</span> label <span class=
=3D"hljs-keyword">in</span> labels], dtype=3Dtorch.long)
            loss =3D criterion(logits, labels, input_lengths, target_length=
s)
           =20
            val_loss +=3D loss.item()
   =20
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Validation Loss: <span class=3D"hljs-subst">{val_loss/<span class=3D"hljs=
-built_in">len</span>(val_loader)}</span>"</span>)
</code></pre>
<h3 id=3D"step-7-save-the-model" tabindex=3D"-1"><a class=3D"header-anchor"=
 href=3D"https://slickgpt.com/t/20#step-7-save-the-model" aria-hidden=3D"tr=
ue">#</a> Step 7: Save the Model</h3>
<pre><code class=3D"hljs language-python">torch.save(model.state_dict(), <s=
pan class=3D"hljs-string">"asr_model.pth"</span>)
</code></pre>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>Training an ASR model from scratch for Korean is resource-intensive and =
time-consuming but allows for full customization. The above steps outline t=
he process, from setting up the environment and preparing data to defining =
the model architecture, training, and evaluation.</p>
<p>Ensure that you have access to a large and diverse Korean speech dataset=
 and sufficient computational resources. The Conformer architecture used he=
re is a state-of-the-art choice for ASR tasks, providing a good balance bet=
ween performance and complexity.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 2:01=
 AM">2:01 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>Give me multilingual solution - one single ASR model wor=
ks for korean, english and chinese. fine-tuning is acceptable. but try trai=
ning from scratch if fine-tuning wouldn=E2=80=99t be a good choice for perf=
ormance aspect.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 2:01 AM">2:01 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>C=
reating a multilingual ASR model capable of recognizing Korean, English, an=
d Chinese can be approached by either fine-tuning a pre-trained multilingua=
l model or training a model from scratch. Given the complexity and resource=
 demands of training from scratch, fine-tuning a pre-trained multilingual m=
odel is generally more practical and efficient. Here=E2=80=99s a detailed g=
uide for both approaches:</p>
<h3 id=3D"fine-tuning-a-pre-trained-multilingual-model" tabindex=3D"-1"><a =
class=3D"header-anchor" href=3D"https://slickgpt.com/t/20#fine-tuning-a-pre=
-trained-multilingual-model" aria-hidden=3D"true">#</a> Fine-Tuning a Pre-T=
rained Multilingual Model</h3>
<h4 id=3D"step-1-set-up-environment" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-1-set-up-environment" aria-hidd=
en=3D"true">#</a> Step 1: Set Up Environment</h4>
<pre><code class=3D"hljs language-bash"><span class=3D"hljs-comment"># Crea=
te and activate a virtual environment (optional)</span>
python -m venv asr_env
<span class=3D"hljs-built_in">source</span> asr_env/bin/activate  <span cla=
ss=3D"hljs-comment"># On Windows use `.\asr_env\Scripts\activate`</span>

<span class=3D"hljs-comment"># Install necessary libraries</span>
pip install torch torchaudio transformers datasets
</code></pre>
<h4 id=3D"step-2-prepare-the-data" tabindex=3D"-1"><a class=3D"header-ancho=
r" href=3D"https://slickgpt.com/t/20#step-2-prepare-the-data" aria-hidden=
=3D"true">#</a> Step 2: Prepare the Data</h4>
<p>Ensure you have datasets for Korean, English, and Chinese speech with tr=
anscripts. You can use datasets like Common Voice, AISHELL-1 for Chinese, L=
ibrispeech for English, and Zeroth-Korean. Combine these datasets into a si=
ngle multilingual dataset.</p>
<h4 id=3D"step-3-load-and-preprocess-data" tabindex=3D"-1"><a class=3D"head=
er-anchor" href=3D"https://slickgpt.com/t/20#step-3-load-and-preprocess-dat=
a" aria-hidden=3D"true">#</a> Step 3: Load and Preprocess Data</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> datasets <span class=3D"hljs-keyword">import</span> load_dataset, c=
oncatenate_datasets

<span class=3D"hljs-comment"># Load datasets</span>
korean_dataset =3D load_dataset(<span class=3D"hljs-string">'zeroth_korean'=
</span>, split=3D<span class=3D"hljs-string">'train'</span>)
english_dataset =3D load_dataset(<span class=3D"hljs-string">'librispeech_a=
sr'</span>, <span class=3D"hljs-string">'train.960'</span>)
chinese_dataset =3D load_dataset(<span class=3D"hljs-string">'aishell'</spa=
n>, split=3D<span class=3D"hljs-string">'train'</span>)

<span class=3D"hljs-comment"># Combine datasets</span>
dataset =3D concatenate_datasets([korean_dataset, english_dataset, chinese_=
dataset])

<span class=3D"hljs-comment"># Preprocess data</span>
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> Wav2Vec2Processor

processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-large-xlsr-53"</span>)

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">preprocess_data</span>(<span class=3D"hljs-params">batch</span>):
    audio =3D batch[<span class=3D"hljs-string">"audio"</span>][<span class=
=3D"hljs-string">"array"</span>]
    batch[<span class=3D"hljs-string">"input_values"</span>] =3D processor(=
audio, sampling_rate=3D<span class=3D"hljs-number">16000</span>).input_valu=
es[<span class=3D"hljs-number">0</span>]
    batch[<span class=3D"hljs-string">"labels"</span>] =3D processor.tokeni=
zer(batch[<span class=3D"hljs-string">"text"</span>]).input_ids
    <span class=3D"hljs-keyword">return</span> batch

dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_dat=
a, remove_columns=3D[<span class=3D"hljs-string">"audio"</span>, <span clas=
s=3D"hljs-string">"text"</span>])
</code></pre>
<h4 id=3D"step-4-create-data-loaders" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-4-create-data-loaders" aria-hi=
dden=3D"true">#</a> Step 4: Create Data Loaders</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> torch.utils.data <span class=3D"hljs-keyword">import</span> DataLoa=
der, Dataset

<span class=3D"hljs-keyword">class</span> <span class=3D"hljs-title class_"=
>ASRDataset</span>(<span class=3D"hljs-title class_ inherited__">Dataset</s=
pan>):
    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__init__</span>(<span class=3D"hljs-params">self, dataset</span>):
        self.dataset =3D dataset

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__len__</span>(<span class=3D"hljs-params">self</span>):
        <span class=3D"hljs-keyword">return</span> <span class=3D"hljs-buil=
t_in">len</span>(self.dataset)

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__getitem__</span>(<span class=3D"hljs-params">self, idx</span>):
        item =3D self.dataset[idx]
        <span class=3D"hljs-keyword">return</span> {
            <span class=3D"hljs-string">"input_values"</span>: torch.tensor=
(item[<span class=3D"hljs-string">"input_values"</span>], dtype=3Dtorch.flo=
at32),
            <span class=3D"hljs-string">"labels"</span>: torch.tensor(item[=
<span class=3D"hljs-string">"labels"</span>], dtype=3Dtorch.int64),
        }

train_dataset =3D ASRDataset(dataset[<span class=3D"hljs-string">'train'</s=
pan>])
val_dataset =3D ASRDataset(dataset[<span class=3D"hljs-string">'validation'=
</span>])

train_loader =3D DataLoader(train_dataset, batch_size=3D<span class=3D"hljs=
-number">16</span>, shuffle=3D<span class=3D"hljs-literal">True</span>)
val_loader =3D DataLoader(val_dataset, batch_size=3D<span class=3D"hljs-num=
ber">16</span>)
</code></pre>
<h4 id=3D"step-5-fine-tune-the-model" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-5-fine-tune-the-model" aria-hi=
dden=3D"true">#</a> Step 5: Fine-Tune the Model</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> Wav2Vec2For=
CTC, TrainingArguments, Trainer

<span class=3D"hljs-comment"># Load pre-trained model</span>
model =3D Wav2Vec2ForCTC.from_pretrained(<span class=3D"hljs-string">"faceb=
ook/wav2vec2-large-xlsr-53"</span>)

<span class=3D"hljs-comment"># Define training arguments</span>
training_args =3D TrainingArguments(
    output_dir=3D<span class=3D"hljs-string">"./results"</span>,
    evaluation_strategy=3D<span class=3D"hljs-string">"epoch"</span>,
    learning_rate=3D<span class=3D"hljs-number">1e-4</span>,
    per_device_train_batch_size=3D<span class=3D"hljs-number">16</span>,
    per_device_eval_batch_size=3D<span class=3D"hljs-number">16</span>,
    num_train_epochs=3D<span class=3D"hljs-number">10</span>,
    weight_decay=3D<span class=3D"hljs-number">0.01</span>,
    save_steps=3D<span class=3D"hljs-number">500</span>,
    eval_steps=3D<span class=3D"hljs-number">500</span>,
)

<span class=3D"hljs-comment"># Define Trainer</span>
trainer =3D Trainer(
    model=3Dmodel,
    args=3Dtraining_args,
    train_dataset=3Dtrain_dataset,
    eval_dataset=3Dval_dataset,
)

<span class=3D"hljs-comment"># Fine-tune the model</span>
trainer.train()
</code></pre>
<h3 id=3D"training-from-scratch-if-fine-tuning-is-not-optimal" tabindex=3D"=
-1"><a class=3D"header-anchor" href=3D"https://slickgpt.com/t/20#training-f=
rom-scratch-if-fine-tuning-is-not-optimal" aria-hidden=3D"true">#</a> Train=
ing from Scratch (if Fine-Tuning is not Optimal)</h3>
<p>If fine-tuning doesn=E2=80=99t yield satisfactory performance, you may c=
onsider training from scratch. This approach requires significant computati=
onal resources and a large, diverse dataset.</p>
<h4 id=3D"step-1-define-the-model-architecture" tabindex=3D"-1"><a class=3D=
"header-anchor" href=3D"https://slickgpt.com/t/20#step-1-define-the-model-a=
rchitecture" aria-hidden=3D"true">#</a> Step 1: Define the Model Architectu=
re</h4>
<p>We=E2=80=99ll use a Conformer-based model for both encoding and decoding=
.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch
<span class=3D"hljs-keyword">import</span> torch.nn <span class=3D"hljs-key=
word">as</span> nn
<span class=3D"hljs-keyword">import</span> torchaudio
<span class=3D"hljs-keyword">from</span> torchaudio.models <span class=3D"h=
ljs-keyword">import</span> Conformer

<span class=3D"hljs-keyword">class</span> <span class=3D"hljs-title class_"=
>MultilingualASRModel</span>(nn.Module):
    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__init__</span>(<span class=3D"hljs-params">self</span>):
        <span class=3D"hljs-built_in">super</span>(MultilingualASRModel, se=
lf).__init__()
        self.processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D=
"hljs-string">"facebook/wav2vec2-base"</span>)
        self.encoder =3D Conformer(
            input_dim=3D<span class=3D"hljs-number">80</span>,  <span class=
=3D"hljs-comment"># Number of mel frequency bins</span>
            num_heads=3D<span class=3D"hljs-number">4</span>,
            ffn_dim=3D<span class=3D"hljs-number">256</span>,
            num_layers=3D<span class=3D"hljs-number">6</span>,
            dropout=3D<span class=3D"hljs-number">0.1</span>,
        )
        self.decoder =3D nn.Linear(<span class=3D"hljs-number">256</span>, =
self.processor.tokenizer.vocab_size)

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">forward</span>(<span class=3D"hljs-params">self, input_values, attent=
ion_mask=3D<span class=3D"hljs-literal">None</span>, labels=3D<span class=
=3D"hljs-literal">None</span></span>):
        x, _ =3D torchaudio.compliance.kaldi.fbank(input_values, num_mel_bi=
ns=3D<span class=3D"hljs-number">80</span>)
        x =3D self.encoder(x)
        logits =3D self.decoder(x)
        <span class=3D"hljs-keyword">return</span> logits

<span class=3D"hljs-comment"># Instantiate the model</span>
model =3D MultilingualASRModel()
</code></pre>
<h4 id=3D"step-2-preprocess-data" tabindex=3D"-1"><a class=3D"header-anchor=
" href=3D"https://slickgpt.com/t/20#step-2-preprocess-data" aria-hidden=3D"=
true">#</a> Step 2: Preprocess Data</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-comment"># Lo=
ad and preprocess dataset as shown in the fine-tuning section</span>
dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_dat=
a, remove_columns=3D[<span class=3D"hljs-string">"audio"</span>, <span clas=
s=3D"hljs-string">"text"</span>])
</code></pre>
<h4 id=3D"step-3-create-data-loaders" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-3-create-data-loaders" aria-hi=
dden=3D"true">#</a> Step 3: Create Data Loaders</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-comment"># Cr=
eate data loaders as shown in the fine-tuning section</span>
train_loader =3D DataLoader(train_dataset, batch_size=3D<span class=3D"hljs=
-number">16</span>, shuffle=3D<span class=3D"hljs-literal">True</span>)
val_loader =3D DataLoader(val_dataset, batch_size=3D<span class=3D"hljs-num=
ber">16</span>)
</code></pre>
<h4 id=3D"step-4-define-training-loop" tabindex=3D"-1"><a class=3D"header-a=
nchor" href=3D"https://slickgpt.com/t/20#step-4-define-training-loop" aria-=
hidden=3D"true">#</a> Step 4: Define Training Loop</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch.optim <span class=3D"hljs-keyword">as</span> optim
<span class=3D"hljs-keyword">from</span> torch.nn <span class=3D"hljs-keywo=
rd">import</span> CTCLoss

<span class=3D"hljs-comment"># Define loss function and optimizer</span>
criterion =3D CTCLoss(blank=3Dprocessor.tokenizer.pad_token_id)
optimizer =3D optim.AdamW(model.parameters(), lr=3D<span class=3D"hljs-numb=
er">1e-4</span>)

<span class=3D"hljs-comment"># Training loop</span>
num_epochs =3D <span class=3D"hljs-number">20</span>

<span class=3D"hljs-keyword">for</span> epoch <span class=3D"hljs-keyword">=
in</span> <span class=3D"hljs-built_in">range</span>(num_epochs):
    model.train()
    running_loss =3D <span class=3D"hljs-number">0.0</span>
    <span class=3D"hljs-keyword">for</span> batch <span class=3D"hljs-keywo=
rd">in</span> train_loader:
        input_values =3D batch[<span class=3D"hljs-string">"input_values"</=
span>]
        labels =3D batch[<span class=3D"hljs-string">"labels"</span>]

        optimizer.zero_grad()
        logits =3D model(input_values)
       =20
        <span class=3D"hljs-comment"># Compute loss</span>
        input_lengths =3D torch.full((logits.size(<span class=3D"hljs-numbe=
r">0</span>),), logits.size(<span class=3D"hljs-number">1</span>), dtype=3D=
torch.long)
        target_lengths =3D torch.tensor([<span class=3D"hljs-built_in">len<=
/span>(label) <span class=3D"hljs-keyword">for</span> label <span class=3D"=
hljs-keyword">in</span> labels], dtype=3Dtorch.long)
        loss =3D criterion(logits, labels, input_lengths, target_lengths)
       =20
        loss.backward()
        optimizer.step()
       =20
        running_loss +=3D loss.item()
   =20
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Epoch <span class=3D"hljs-subst">{epoch+<span class=3D"hljs-number">1</sp=
an>}</span>/<span class=3D"hljs-subst">{num_epochs}</span>, Loss: <span cla=
ss=3D"hljs-subst">{running_loss/<span class=3D"hljs-built_in">len</span>(tr=
ain_loader)}</span>"</span>)
   =20
    <span class=3D"hljs-comment"># Evaluation</span>
    model.<span class=3D"hljs-built_in">eval</span>()
    val_loss =3D <span class=3D"hljs-number">0.0</span>
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        <span class=3D"hljs-keyword">for</span> batch <span class=3D"hljs-k=
eyword">in</span> val_loader:
            input_values =3D batch[<span class=3D"hljs-string">"input_value=
s"</span>]
            labels =3D batch[<span class=3D"hljs-string">"labels"</span>]
           =20
            logits =3D model(input_values)
           =20
            <span class=3D"hljs-comment"># Compute loss</span>
            input_lengths =3D torch.full((logits.size(<span class=3D"hljs-n=
umber">0</span>),), logits.size(<span class=3D"hljs-number">1</span>), dtyp=
e=3Dtorch.long)
            target_lengths =3D torch.tensor([<span class=3D"hljs-built_in">=
len</span>(label) <span class=3D"hljs-keyword">for</span> label <span class=
=3D"hljs-keyword">in</span> labels], dtype=3Dtorch.long)
            loss =3D criterion(logits, labels, input_lengths, target_length=
s)
           =20
            val_loss +=3D loss.item()
   =20
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Validation Loss: <span class=3D"hljs-subst">{val_loss/<span class=3D"hljs=
-built_in">len</span>(val_loader)}</span>"</span>)
</code></pre>
<h4 id=3D"step-5-save-the-model" tabindex=3D"-1"><a class=3D"header-anchor"=
 href=3D"https://slickgpt.com/t/20#step-5-save-the-model" aria-hidden=3D"tr=
ue">#</a> Step 5: Save the Model</h4>
<pre><code class=3D"hljs language-python">torch.save(model.state_dict(), <s=
pan class=3D"hljs-string">"multilingual_asr_model.pth"</span>)
</code></pre>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>Fine-tuning a pre-trained multilingual model like <code>facebook/wav2vec=
2-large-xlsr-53</code> is the recommended approach due to its efficiency an=
d effectiveness. If fine-tuning does not meet your performance requirements=
, training from scratch is an option, although it requires more resources. =
This guide provides steps for both approaches to achieve a multilingual ASR=
 model capable of recognizing Korean, English, and Chinese.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 2:06=
 AM">2:06 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>still need to satisfy the requirement on the model size.=
 on-device use you know.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 2:06 AM">2:06 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>G=
iven the requirement for on-device use with a constrained model size, we ca=
n focus on leveraging lightweight architectures and quantization techniques=
. We aim to develop a multilingual ASR model for Korean, English, and Chine=
se that is efficient enough for mobile deployment.</p>
<h3 id=3D"strategy" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"http=
s://slickgpt.com/t/20#strategy" aria-hidden=3D"true">#</a> Strategy</h3>
<ol>
<li><strong>Utilize Smaller Pre-trained Models</strong>: Start with smaller=
 pre-trained models like <code>distilWav2Vec2</code> for the encoder and <c=
ode>DistilBERT</code> for the language model.</li>
<li><strong>Quantization</strong>: Apply 8-bit dynamic quantization to redu=
ce model size.</li>
<li><strong>Optimize the Architecture</strong>: Use efficient architectures=
 such as a combination of CTC-based encoder and a simplified Conformer-base=
d decoder.</li>
</ol>
<h3 id=3D"step-by-step-guide" tabindex=3D"-1"><a class=3D"header-anchor" hr=
ef=3D"https://slickgpt.com/t/20#step-by-step-guide" aria-hidden=3D"true">#<=
/a> Step-by-Step Guide</h3>
<h4 id=3D"step-1-set-up-environment" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-1-set-up-environment" aria-hidd=
en=3D"true">#</a> Step 1: Set Up Environment</h4>
<pre><code class=3D"hljs language-bash"><span class=3D"hljs-comment"># Crea=
te and activate a virtual environment (optional)</span>
python -m venv asr_env
<span class=3D"hljs-built_in">source</span> asr_env/bin/activate  <span cla=
ss=3D"hljs-comment"># On Windows use `.\asr_env\Scripts\activate`</span>

<span class=3D"hljs-comment"># Install necessary libraries</span>
pip install torch torchaudio transformers datasets
</code></pre>
<h4 id=3D"step-2-prepare-the-data" tabindex=3D"-1"><a class=3D"header-ancho=
r" href=3D"https://slickgpt.com/t/20#step-2-prepare-the-data" aria-hidden=
=3D"true">#</a> Step 2: Prepare the Data</h4>
<p>Ensure you have datasets for Korean, English, and Chinese speech with tr=
anscripts. Combine these datasets into a single multilingual dataset.</p>
<h4 id=3D"step-3-load-and-preprocess-data" tabindex=3D"-1"><a class=3D"head=
er-anchor" href=3D"https://slickgpt.com/t/20#step-3-load-and-preprocess-dat=
a" aria-hidden=3D"true">#</a> Step 3: Load and Preprocess Data</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> datasets <span class=3D"hljs-keyword">import</span> load_dataset, c=
oncatenate_datasets

<span class=3D"hljs-comment"># Load datasets</span>
korean_dataset =3D load_dataset(<span class=3D"hljs-string">'zeroth_korean'=
</span>, split=3D<span class=3D"hljs-string">'train'</span>)
english_dataset =3D load_dataset(<span class=3D"hljs-string">'librispeech_a=
sr'</span>, <span class=3D"hljs-string">'train.960'</span>)
chinese_dataset =3D load_dataset(<span class=3D"hljs-string">'aishell'</spa=
n>, split=3D<span class=3D"hljs-string">'train'</span>)

<span class=3D"hljs-comment"># Combine datasets</span>
dataset =3D concatenate_datasets([korean_dataset, english_dataset, chinese_=
dataset])

<span class=3D"hljs-comment"># Preprocess data</span>
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> Wav2Vec2Processor

processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-base"</span>)

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">preprocess_data</span>(<span class=3D"hljs-params">batch</span>):
    audio =3D batch[<span class=3D"hljs-string">"audio"</span>][<span class=
=3D"hljs-string">"array"</span>]
    batch[<span class=3D"hljs-string">"input_values"</span>] =3D processor(=
audio, sampling_rate=3D<span class=3D"hljs-number">16000</span>).input_valu=
es[<span class=3D"hljs-number">0</span>]
    batch[<span class=3D"hljs-string">"labels"</span>] =3D processor.tokeni=
zer(batch[<span class=3D"hljs-string">"text"</span>]).input_ids
    <span class=3D"hljs-keyword">return</span> batch

dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_dat=
a, remove_columns=3D[<span class=3D"hljs-string">"audio"</span>, <span clas=
s=3D"hljs-string">"text"</span>])
</code></pre>
<h4 id=3D"step-4-create-data-loaders" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-4-create-data-loaders" aria-hi=
dden=3D"true">#</a> Step 4: Create Data Loaders</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> torch.utils.data <span class=3D"hljs-keyword">import</span> DataLoa=
der, Dataset

<span class=3D"hljs-keyword">class</span> <span class=3D"hljs-title class_"=
>ASRDataset</span>(<span class=3D"hljs-title class_ inherited__">Dataset</s=
pan>):
    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__init__</span>(<span class=3D"hljs-params">self, dataset</span>):
        self.dataset =3D dataset

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__len__</span>(<span class=3D"hljs-params">self</span>):
        <span class=3D"hljs-keyword">return</span> <span class=3D"hljs-buil=
t_in">len</span>(self.dataset)

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__getitem__</span>(<span class=3D"hljs-params">self, idx</span>):
        item =3D self.dataset[idx]
        <span class=3D"hljs-keyword">return</span> {
            <span class=3D"hljs-string">"input_values"</span>: torch.tensor=
(item[<span class=3D"hljs-string">"input_values"</span>], dtype=3Dtorch.flo=
at32),
            <span class=3D"hljs-string">"labels"</span>: torch.tensor(item[=
<span class=3D"hljs-string">"labels"</span>], dtype=3Dtorch.int64),
        }

train_dataset =3D ASRDataset(dataset[<span class=3D"hljs-string">'train'</s=
pan>])
val_dataset =3D ASRDataset(dataset[<span class=3D"hljs-string">'validation'=
</span>])

train_loader =3D DataLoader(train_dataset, batch_size=3D<span class=3D"hljs=
-number">16</span>, shuffle=3D<span class=3D"hljs-literal">True</span>)
val_loader =3D DataLoader(val_dataset, batch_size=3D<span class=3D"hljs-num=
ber">16</span>)
</code></pre>
<h4 id=3D"step-5-define-the-model-architecture" tabindex=3D"-1"><a class=3D=
"header-anchor" href=3D"https://slickgpt.com/t/20#step-5-define-the-model-a=
rchitecture" aria-hidden=3D"true">#</a> Step 5: Define the Model Architectu=
re</h4>
<p>Here, we define a compact model with a CTC encoder and a Conformer decod=
er.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch
<span class=3D"hljs-keyword">import</span> torch.nn <span class=3D"hljs-key=
word">as</span> nn
<span class=3D"hljs-keyword">import</span> torchaudio
<span class=3D"hljs-keyword">from</span> torchaudio.models <span class=3D"h=
ljs-keyword">import</span> Conformer

<span class=3D"hljs-keyword">class</span> <span class=3D"hljs-title class_"=
>CompactASRModel</span>(nn.Module):
    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__init__</span>(<span class=3D"hljs-params">self</span>):
        <span class=3D"hljs-built_in">super</span>(CompactASRModel, self)._=
_init__()
        self.processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D=
"hljs-string">"facebook/wav2vec2-base"</span>)
        self.encoder =3D Conformer(
            input_dim=3D<span class=3D"hljs-number">80</span>,  <span class=
=3D"hljs-comment"># Number of mel frequency bins</span>
            num_heads=3D<span class=3D"hljs-number">4</span>,
            ffn_dim=3D<span class=3D"hljs-number">256</span>,
            num_layers=3D<span class=3D"hljs-number">4</span>,  <span class=
=3D"hljs-comment"># Reduced number of layers</span>
            dropout=3D<span class=3D"hljs-number">0.1</span>,
        )
        self.decoder =3D nn.Linear(<span class=3D"hljs-number">256</span>, =
self.processor.tokenizer.vocab_size)

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">forward</span>(<span class=3D"hljs-params">self, input_values, attent=
ion_mask=3D<span class=3D"hljs-literal">None</span>, labels=3D<span class=
=3D"hljs-literal">None</span></span>):
        x, _ =3D torchaudio.compliance.kaldi.fbank(input_values, num_mel_bi=
ns=3D<span class=3D"hljs-number">80</span>)
        x =3D self.encoder(x)
        logits =3D self.decoder(x)
        <span class=3D"hljs-keyword">return</span> logits

<span class=3D"hljs-comment"># Instantiate the model</span>
model =3D CompactASRModel()
</code></pre>
<h4 id=3D"step-6-fine-tune-the-model" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-6-fine-tune-the-model" aria-hi=
dden=3D"true">#</a> Step 6: Fine-Tune the Model</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> TrainingArg=
uments, Trainer

<span class=3D"hljs-comment"># Define training arguments</span>
training_args =3D TrainingArguments(
    output_dir=3D<span class=3D"hljs-string">"./results"</span>,
    evaluation_strategy=3D<span class=3D"hljs-string">"epoch"</span>,
    learning_rate=3D<span class=3D"hljs-number">1e-4</span>,
    per_device_train_batch_size=3D<span class=3D"hljs-number">16</span>,
    per_device_eval_batch_size=3D<span class=3D"hljs-number">16</span>,
    num_train_epochs=3D<span class=3D"hljs-number">10</span>,
    weight_decay=3D<span class=3D"hljs-number">0.01</span>,
    save_steps=3D<span class=3D"hljs-number">500</span>,
    eval_steps=3D<span class=3D"hljs-number">500</span>,
)

<span class=3D"hljs-comment"># Define Trainer</span>
trainer =3D Trainer(
    model=3Dmodel,
    args=3Dtraining_args,
    train_dataset=3Dtrain_dataset,
    eval_dataset=3Dval_dataset,
)

<span class=3D"hljs-comment"># Fine-tune the model</span>
trainer.train()
</code></pre>
<h4 id=3D"step-7-quantize-the-model" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-7-quantize-the-model" aria-hidd=
en=3D"true">#</a> Step 7: Quantize the Model</h4>
<p>Quantize the model to 8-bit to reduce its size.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch.quantization

<span class=3D"hljs-comment"># Quantize the model</span>
model.<span class=3D"hljs-built_in">eval</span>()
quantized_model =3D torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=3Dtorch.qint8
)

<span class=3D"hljs-comment"># Save the quantized model</span>
quantized_model.save_pretrained(<span class=3D"hljs-string">"quantized_asr_=
model"</span>)

<span class=3D"hljs-comment"># Check the model size</span>
<span class=3D"hljs-keyword">import</span> os

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">get_model_size</span>(<span class=3D"hljs-params">model_path</span>):
    total_size =3D <span class=3D"hljs-number">0</span>
    <span class=3D"hljs-keyword">for</span> dirpath, dirnames, filenames <s=
pan class=3D"hljs-keyword">in</span> os.walk(model_path):
        <span class=3D"hljs-keyword">for</span> f <span class=3D"hljs-keywo=
rd">in</span> filenames:
            fp =3D os.path.join(dirpath, f)
            total_size +=3D os.path.getsize(fp)
    <span class=3D"hljs-keyword">return</span> total_size / (<span class=3D=
"hljs-number">1024</span> * <span class=3D"hljs-number">1024</span>)  <span=
 class=3D"hljs-comment"># Convert to MB</span>

<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">f"Qu=
antized Model Size: <span class=3D"hljs-subst">{get_model_size(<span class=
=3D"hljs-string">'quantized_asr_model'</span>)}</span> MB"</span>)
</code></pre>
<h4 id=3D"step-8-inference-on-mobile" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-8-inference-on-mobile" aria-hi=
dden=3D"true">#</a> Step 8: Inference on Mobile</h4>
<p>Deploy the quantized model on a mobile device and measure the inference =
time.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> Wav2Vec2Pro=
cessor, DistilBertTokenizer
<span class=3D"hljs-keyword">import</span> torch
<span class=3D"hljs-keyword">import</span> torchaudio
<span class=3D"hljs-keyword">import</span> time

<span class=3D"hljs-comment"># Load quantized models</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-base"</span>)
acoustic_model =3D torch.jit.load(<span class=3D"hljs-string">"quantized_as=
r_model/pytorch_model.bin"</span>)

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">measure_inference_time</span>(<span class=3D"hljs-params">audio_file</spa=
n>):
    audio_input, _ =3D torchaudio.load(audio_file)
    input_values =3D processor(audio_input.squeeze().numpy(), sampling_rate=
=3D<span class=3D"hljs-number">16000</span>).input_values[<span class=3D"hl=
js-number">0</span>]
    input_values =3D torch.tensor(input_values).unsqueeze(<span class=3D"hl=
js-number">0</span>)

    <span class=3D"hljs-comment"># Measure acoustic model inference time</s=
pan>
    start_time =3D time.time()
    acoustic_model.<span class=3D"hljs-built_in">eval</span>()
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        logits =3D acoustic_model(input_values).logits
    end_time =3D time.time()

    inference_time =3D end_time - start_time
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Inference Time: <span class=3D"hljs-subst">{inference_time:<span class=3D=
"hljs-number">.4</span>f}</span> seconds"</span>)

    <span class=3D"hljs-keyword">return</span> logits, inference_time

<span class=3D"hljs-comment"># Example usage</span>
audio_file =3D <span class=3D"hljs-string">"path/to/3-5s/korean/audio.wav"<=
/span>
logits, inference_time =3D measure_inference_time(audio_file)
<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">"Log=
its:"</span>, logits)
</code></pre>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>By leveraging smaller model architectures and applying 8-bit quantizatio=
n, you can create a multilingual ASR model suitable for on-device use. The =
fine-tuning approach with <code>Wav2Vec2</code> and <code>DistilBERT</code>=
, combined with aggressive quantization, ensures the model size is within t=
he constraints for mobile deployment while maintaining acceptable performan=
ce.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 2:13=
 AM">2:13 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>distilbert not quite used in the above for language mode=
l</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 2:13 AM">2:13 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>T=
o create a multilingual ASR model suitable for on-device use, we need to co=
mbine a compact acoustic model with a lightweight language model. Given you=
r requirements, I=E2=80=99ll make sure to include <code>DistilBERT</code> f=
or the language model as well. Here=E2=80=99s the revised guide, focusing o=
n optimizing the model size and ensuring efficient performance on mobile de=
vices.</p>
<h3 id=3D"step-by-step-guide" tabindex=3D"-1"><a class=3D"header-anchor" hr=
ef=3D"https://slickgpt.com/t/20#step-by-step-guide" aria-hidden=3D"true">#<=
/a> Step-by-Step Guide</h3>
<h4 id=3D"step-1-set-up-environment" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-1-set-up-environment" aria-hidd=
en=3D"true">#</a> Step 1: Set Up Environment</h4>
<pre><code class=3D"hljs language-bash"><span class=3D"hljs-comment"># Crea=
te and activate a virtual environment (optional)</span>
python -m venv asr_env
<span class=3D"hljs-built_in">source</span> asr_env/bin/activate  <span cla=
ss=3D"hljs-comment"># On Windows use `.\asr_env\Scripts\activate`</span>

<span class=3D"hljs-comment"># Install necessary libraries</span>
pip install torch torchaudio transformers datasets
</code></pre>
<h4 id=3D"step-2-prepare-the-data" tabindex=3D"-1"><a class=3D"header-ancho=
r" href=3D"https://slickgpt.com/t/20#step-2-prepare-the-data" aria-hidden=
=3D"true">#</a> Step 2: Prepare the Data</h4>
<p>Ensure you have datasets for Korean, English, and Chinese speech with tr=
anscripts. Combine these datasets into a single multilingual dataset.</p>
<h4 id=3D"step-3-load-and-preprocess-data" tabindex=3D"-1"><a class=3D"head=
er-anchor" href=3D"https://slickgpt.com/t/20#step-3-load-and-preprocess-dat=
a" aria-hidden=3D"true">#</a> Step 3: Load and Preprocess Data</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> datasets <span class=3D"hljs-keyword">import</span> load_dataset, c=
oncatenate_datasets

<span class=3D"hljs-comment"># Load datasets</span>
korean_dataset =3D load_dataset(<span class=3D"hljs-string">'zeroth_korean'=
</span>, split=3D<span class=3D"hljs-string">'train'</span>)
english_dataset =3D load_dataset(<span class=3D"hljs-string">'librispeech_a=
sr'</span>, <span class=3D"hljs-string">'train.960'</span>)
chinese_dataset =3D load_dataset(<span class=3D"hljs-string">'aishell'</spa=
n>, split=3D<span class=3D"hljs-string">'train'</span>)

<span class=3D"hljs-comment"># Combine datasets</span>
dataset =3D concatenate_datasets([korean_dataset, english_dataset, chinese_=
dataset])

<span class=3D"hljs-comment"># Preprocess data</span>
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> Wav2Vec2Processor

processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-base"</span>)

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">preprocess_data</span>(<span class=3D"hljs-params">batch</span>):
    audio =3D batch[<span class=3D"hljs-string">"audio"</span>][<span class=
=3D"hljs-string">"array"</span>]
    batch[<span class=3D"hljs-string">"input_values"</span>] =3D processor(=
audio, sampling_rate=3D<span class=3D"hljs-number">16000</span>).input_valu=
es[<span class=3D"hljs-number">0</span>]
    batch[<span class=3D"hljs-string">"labels"</span>] =3D processor.tokeni=
zer(batch[<span class=3D"hljs-string">"text"</span>]).input_ids
    <span class=3D"hljs-keyword">return</span> batch

dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_dat=
a, remove_columns=3D[<span class=3D"hljs-string">"audio"</span>, <span clas=
s=3D"hljs-string">"text"</span>])
</code></pre>
<h4 id=3D"step-4-create-data-loaders" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-4-create-data-loaders" aria-hi=
dden=3D"true">#</a> Step 4: Create Data Loaders</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> torch.utils.data <span class=3D"hljs-keyword">import</span> DataLoa=
der, Dataset

<span class=3D"hljs-keyword">class</span> <span class=3D"hljs-title class_"=
>ASRDataset</span>(<span class=3D"hljs-title class_ inherited__">Dataset</s=
pan>):
    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__init__</span>(<span class=3D"hljs-params">self, dataset</span>):
        self.dataset =3D dataset

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__len__</span>(<span class=3D"hljs-params">self</span>):
        <span class=3D"hljs-keyword">return</span> <span class=3D"hljs-buil=
t_in">len</span>(self.dataset)

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__getitem__</span>(<span class=3D"hljs-params">self, idx</span>):
        item =3D self.dataset[idx]
        <span class=3D"hljs-keyword">return</span> {
            <span class=3D"hljs-string">"input_values"</span>: torch.tensor=
(item[<span class=3D"hljs-string">"input_values"</span>], dtype=3Dtorch.flo=
at32),
            <span class=3D"hljs-string">"labels"</span>: torch.tensor(item[=
<span class=3D"hljs-string">"labels"</span>], dtype=3Dtorch.int64),
        }

train_dataset =3D ASRDataset(dataset[<span class=3D"hljs-string">'train'</s=
pan>])
val_dataset =3D ASRDataset(dataset[<span class=3D"hljs-string">'validation'=
</span>])

train_loader =3D DataLoader(train_dataset, batch_size=3D<span class=3D"hljs=
-number">16</span>, shuffle=3D<span class=3D"hljs-literal">True</span>)
val_loader =3D DataLoader(val_dataset, batch_size=3D<span class=3D"hljs-num=
ber">16</span>)
</code></pre>
<h4 id=3D"step-5-define-the-compact-model-architecture" tabindex=3D"-1"><a =
class=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-5-define-the=
-compact-model-architecture" aria-hidden=3D"true">#</a> Step 5: Define the =
Compact Model Architecture</h4>
<p>Here, we define a compact ASR model with a CTC encoder and a DistilBERT-=
based language model.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch
<span class=3D"hljs-keyword">import</span> torch.nn <span class=3D"hljs-key=
word">as</span> nn
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> DistilBertForSequenceClassification, DistilBertTokeni=
zer

<span class=3D"hljs-keyword">class</span> <span class=3D"hljs-title class_"=
>MultilingualASRModel</span>(nn.Module):
    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__init__</span>(<span class=3D"hljs-params">self</span>):
        <span class=3D"hljs-built_in">super</span>(MultilingualASRModel, se=
lf).__init__()
        self.processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D=
"hljs-string">"facebook/wav2vec2-base"</span>)
        self.encoder =3D nn.Sequential(
            nn.Conv1d(<span class=3D"hljs-number">1</span>, <span class=3D"=
hljs-number">32</span>, kernel_size=3D<span class=3D"hljs-number">5</span>,=
 stride=3D<span class=3D"hljs-number">1</span>, padding=3D<span class=3D"hl=
js-number">2</span>),
            nn.ReLU(),
            nn.Conv1d(<span class=3D"hljs-number">32</span>, <span class=3D=
"hljs-number">64</span>, kernel_size=3D<span class=3D"hljs-number">5</span>=
, stride=3D<span class=3D"hljs-number">1</span>, padding=3D<span class=3D"h=
ljs-number">2</span>),
            nn.ReLU()
        )
        self.decoder =3D nn.Linear(<span class=3D"hljs-number">64</span>, s=
elf.processor.tokenizer.vocab_size)
        self.lm_model =3D DistilBertForSequenceClassification.from_pretrain=
ed(<span class=3D"hljs-string">"distilbert-base-multilingual-cased"</span>)
        self.lm_tokenizer =3D DistilBertTokenizer.from_pretrained(<span cla=
ss=3D"hljs-string">"distilbert-base-multilingual-cased"</span>)

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">forward</span>(<span class=3D"hljs-params">self, input_values, labels=
=3D<span class=3D"hljs-literal">None</span></span>):
        x =3D self.encoder(input_values.unsqueeze(<span class=3D"hljs-numbe=
r">1</span>))
        x =3D x.transpose(<span class=3D"hljs-number">1</span>, <span class=
=3D"hljs-number">2</span>)
        logits =3D self.decoder(x)
        <span class=3D"hljs-keyword">return</span> logits

<span class=3D"hljs-comment"># Instantiate the model</span>
model =3D MultilingualASRModel()
</code></pre>
<h4 id=3D"step-6-fine-tuning-the-model" tabindex=3D"-1"><a class=3D"header-=
anchor" href=3D"https://slickgpt.com/t/20#step-6-fine-tuning-the-model" ari=
a-hidden=3D"true">#</a> Step 6: Fine-Tuning the Model</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> TrainingArg=
uments, Trainer

<span class=3D"hljs-comment"># Define training arguments</span>
training_args =3D TrainingArguments(
    output_dir=3D<span class=3D"hljs-string">"./results"</span>,
    evaluation_strategy=3D<span class=3D"hljs-string">"epoch"</span>,
    learning_rate=3D<span class=3D"hljs-number">1e-4</span>,
    per_device_train_batch_size=3D<span class=3D"hljs-number">16</span>,
    per_device_eval_batch_size=3D<span class=3D"hljs-number">16</span>,
    num_train_epochs=3D<span class=3D"hljs-number">10</span>,
    weight_decay=3D<span class=3D"hljs-number">0.01</span>,
    save_steps=3D<span class=3D"hljs-number">500</span>,
    eval_steps=3D<span class=3D"hljs-number">500</span>,
)

<span class=3D"hljs-comment"># Define Trainer</span>
trainer =3D Trainer(
    model=3Dmodel,
    args=3Dtraining_args,
    train_dataset=3Dtrain_dataset,
    eval_dataset=3Dval_dataset,
)

<span class=3D"hljs-comment"># Fine-tune the model</span>
trainer.train()
</code></pre>
<h4 id=3D"step-7-quantize-the-model" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-7-quantize-the-model" aria-hidd=
en=3D"true">#</a> Step 7: Quantize the Model</h4>
<p>Quantize the model to 8-bit to reduce its size.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch.quantization

<span class=3D"hljs-comment"># Quantize the model</span>
model.<span class=3D"hljs-built_in">eval</span>()
quantized_model =3D torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=3Dtorch.qint8
)

<span class=3D"hljs-comment"># Save the quantized model</span>
torch.save(quantized_model.state_dict(), <span class=3D"hljs-string">"quant=
ized_asr_model.pth"</span>)

<span class=3D"hljs-comment"># Check the model size</span>
<span class=3D"hljs-keyword">import</span> os

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">get_model_size</span>(<span class=3D"hljs-params">model_path</span>):
    total_size =3D <span class=3D"hljs-number">0</span>
    <span class=3D"hljs-keyword">for</span> dirpath, dirnames, filenames <s=
pan class=3D"hljs-keyword">in</span> os.walk(model_path):
        <span class=3D"hljs-keyword">for</span> f <span class=3D"hljs-keywo=
rd">in</span> filenames:
            fp =3D os.path.join(dirpath, f)
            total_size +=3D os.path.getsize(fp)
    <span class=3D"hljs-keyword">return</span> total_size / (<span class=3D=
"hljs-number">1024</span> * <span class=3D"hljs-number">1024</span>)  <span=
 class=3D"hljs-comment"># Convert to MB</span>

<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">f"Qu=
antized Model Size: <span class=3D"hljs-subst">{get_model_size(<span class=
=3D"hljs-string">'quantized_asr_model.pth'</span>)}</span> MB"</span>)
</code></pre>
<h4 id=3D"step-8-inference-on-mobile" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-8-inference-on-mobile" aria-hi=
dden=3D"true">#</a> Step 8: Inference on Mobile</h4>
<p>Deploy the quantized model on a mobile device and measure the inference =
time.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torchaudio
<span class=3D"hljs-keyword">import</span> time

<span class=3D"hljs-comment"># Load quantized models</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-base"</span>)
acoustic_model =3D MultilingualASRModel()
acoustic_model.load_state_dict(torch.load(<span class=3D"hljs-string">"quan=
tized_asr_model.pth"</span>))

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">measure_inference_time</span>(<span class=3D"hljs-params">audio_file</spa=
n>):
    audio_input, _ =3D torchaudio.load(audio_file)
    input_values =3D processor(audio_input.squeeze().numpy(), sampling_rate=
=3D<span class=3D"hljs-number">16000</span>).input_values[<span class=3D"hl=
js-number">0</span>]
    input_values =3D torch.tensor(input_values).unsqueeze(<span class=3D"hl=
js-number">0</span>)

    <span class=3D"hljs-comment"># Measure acoustic model inference time</s=
pan>
    start_time =3D time.time()
    acoustic_model.<span class=3D"hljs-built_in">eval</span>()
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        logits =3D acoustic_model(input_values)
    end_time =3D time.time()

    inference_time =3D end_time - start_time
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Inference Time: <span class=3D"hljs-subst">{inference_time:<span class=3D=
"hljs-number">.4</span>f}</span> seconds"</span>)

    <span class=3D"hljs-keyword">return</span> logits, inference_time

<span class=3D"hljs-comment"># Example usage</span>
audio_file =3D <span class=3D"hljs-string">"path/to/3-5s/korean/audio.wav"<=
/span>
logits, inference_time =3D measure_inference_time(audio_file)
<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">"Log=
its:"</span>, logits)
</code></pre>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>By leveraging smaller model architectures and applying 8-bit quantizatio=
n, you can create a multilingual ASR model suitable for on-device use. The =
combination of a compact CTC encoder and a lightweight language model like =
<code>DistilBERT</code> ensures that the model size is within the constrain=
ts for mobile deployment while maintaining acceptable performance. This gui=
de provides a practical approach to developing and deploying a multilingual=
 ASR system for Korean, English, and Chinese on mobile devices.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 2:20=
 AM">2:20 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>do you think distilbert is best for this case? any alter=
natives with competitive performance in a smaller size and faster inference=
?</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 2:20 AM">2:20 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>W=
hile DistilBERT is a popular choice for lightweight language models, there =
are other alternatives that can provide competitive performance with potent=
ially smaller size and faster inference. Here are some alternatives that yo=
u can consider:</p>
<h3 id=3D"alternatives-to-distilbert" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#alternatives-to-distilbert" aria-hi=
dden=3D"true">#</a> Alternatives to DistilBERT</h3>
<ol>
<li>
<p><strong>TinyBERT</strong></p>
<ul>
<li><strong>Description</strong>: A smaller version of BERT that achieves c=
ompetitive performance with fewer parameters and faster inference time.</li=
>
<li><strong>Advantages</strong>: Smaller size and faster inference compared=
 to DistilBERT.</li>
<li><strong>Disadvantages</strong>: May require more complex training and f=
ine-tuning steps.</li>
</ul>
</li>
<li>
<p><strong>ALBERT (A Lite BERT)</strong></p>
<ul>
<li><strong>Description</strong>: A smaller, faster version of BERT with pa=
rameter-sharing techniques to reduce model size.</li>
<li><strong>Advantages</strong>: Efficient parameter sharing leads to small=
er model size and faster inference.</li>
<li><strong>Disadvantages</strong>: Might require additional fine-tuning to=
 achieve optimal performance.</li>
</ul>
</li>
<li>
<p><strong>MobileBERT</strong></p>
<ul>
<li><strong>Description</strong>: Optimized for mobile and edge devices, Mo=
bileBERT is designed to be lightweight and efficient.</li>
<li><strong>Advantages</strong>: Specifically designed for mobile use, with=
 a balance of size, speed, and performance.</li>
<li><strong>Disadvantages</strong>: Requires careful tuning to maintain hig=
h accuracy.</li>
</ul>
</li>
</ol>
<h3 id=3D"updated-step-by-step-guide-using-tinybert" tabindex=3D"-1"><a cla=
ss=3D"header-anchor" href=3D"https://slickgpt.com/t/20#updated-step-by-step=
-guide-using-tinybert" aria-hidden=3D"true">#</a> Updated Step-by-Step Guid=
e Using TinyBERT</h3>
<p>Let=E2=80=99s update the guide to use TinyBERT as the language model.</p=
>
<h4 id=3D"step-1-set-up-environment" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-1-set-up-environment" aria-hidd=
en=3D"true">#</a> Step 1: Set Up Environment</h4>
<pre><code class=3D"hljs language-bash"><span class=3D"hljs-comment"># Crea=
te and activate a virtual environment (optional)</span>
python -m venv asr_env
<span class=3D"hljs-built_in">source</span> asr_env/bin/activate  <span cla=
ss=3D"hljs-comment"># On Windows use `.\asr_env\Scripts\activate`</span>

<span class=3D"hljs-comment"># Install necessary libraries</span>
pip install torch torchaudio transformers datasets
</code></pre>
<h4 id=3D"step-2-prepare-the-data" tabindex=3D"-1"><a class=3D"header-ancho=
r" href=3D"https://slickgpt.com/t/20#step-2-prepare-the-data" aria-hidden=
=3D"true">#</a> Step 2: Prepare the Data</h4>
<p>Ensure you have datasets for Korean, English, and Chinese speech with tr=
anscripts. Combine these datasets into a single multilingual dataset.</p>
<h4 id=3D"step-3-load-and-preprocess-data" tabindex=3D"-1"><a class=3D"head=
er-anchor" href=3D"https://slickgpt.com/t/20#step-3-load-and-preprocess-dat=
a" aria-hidden=3D"true">#</a> Step 3: Load and Preprocess Data</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> datasets <span class=3D"hljs-keyword">import</span> load_dataset, c=
oncatenate_datasets

<span class=3D"hljs-comment"># Load datasets</span>
korean_dataset =3D load_dataset(<span class=3D"hljs-string">'zeroth_korean'=
</span>, split=3D<span class=3D"hljs-string">'train'</span>)
english_dataset =3D load_dataset(<span class=3D"hljs-string">'librispeech_a=
sr'</span>, <span class=3D"hljs-string">'train.960'</span>)
chinese_dataset =3D load_dataset(<span class=3D"hljs-string">'aishell'</spa=
n>, split=3D<span class=3D"hljs-string">'train'</span>)

<span class=3D"hljs-comment"># Combine datasets</span>
dataset =3D concatenate_datasets([korean_dataset, english_dataset, chinese_=
dataset])

<span class=3D"hljs-comment"># Preprocess data</span>
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> Wav2Vec2Processor

processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-base"</span>)

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">preprocess_data</span>(<span class=3D"hljs-params">batch</span>):
    audio =3D batch[<span class=3D"hljs-string">"audio"</span>][<span class=
=3D"hljs-string">"array"</span>]
    batch[<span class=3D"hljs-string">"input_values"</span>] =3D processor(=
audio, sampling_rate=3D<span class=3D"hljs-number">16000</span>).input_valu=
es[<span class=3D"hljs-number">0</span>]
    batch[<span class=3D"hljs-string">"labels"</span>] =3D processor.tokeni=
zer(batch[<span class=3D"hljs-string">"text"</span>]).input_ids
    <span class=3D"hljs-keyword">return</span> batch

dataset =3D dataset.<span class=3D"hljs-built_in">map</span>(preprocess_dat=
a, remove_columns=3D[<span class=3D"hljs-string">"audio"</span>, <span clas=
s=3D"hljs-string">"text"</span>])
</code></pre>
<h4 id=3D"step-4-create-data-loaders" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-4-create-data-loaders" aria-hi=
dden=3D"true">#</a> Step 4: Create Data Loaders</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> torch.utils.data <span class=3D"hljs-keyword">import</span> DataLoa=
der, Dataset

<span class=3D"hljs-keyword">class</span> <span class=3D"hljs-title class_"=
>ASRDataset</span>(<span class=3D"hljs-title class_ inherited__">Dataset</s=
pan>):
    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__init__</span>(<span class=3D"hljs-params">self, dataset</span>):
        self.dataset =3D dataset

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__len__</span>(<span class=3D"hljs-params">self</span>):
        <span class=3D"hljs-keyword">return</span> <span class=3D"hljs-buil=
t_in">len</span>(self.dataset)

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__getitem__</span>(<span class=3D"hljs-params">self, idx</span>):
        item =3D self.dataset[idx]
        <span class=3D"hljs-keyword">return</span> {
            <span class=3D"hljs-string">"input_values"</span>: torch.tensor=
(item[<span class=3D"hljs-string">"input_values"</span>], dtype=3Dtorch.flo=
at32),
            <span class=3D"hljs-string">"labels"</span>: torch.tensor(item[=
<span class=3D"hljs-string">"labels"</span>], dtype=3Dtorch.int64),
        }

train_dataset =3D ASRDataset(dataset[<span class=3D"hljs-string">'train'</s=
pan>])
val_dataset =3D ASRDataset(dataset[<span class=3D"hljs-string">'validation'=
</span>])

train_loader =3D DataLoader(train_dataset, batch_size=3D<span class=3D"hljs=
-number">16</span>, shuffle=3D<span class=3D"hljs-literal">True</span>)
val_loader =3D DataLoader(val_dataset, batch_size=3D<span class=3D"hljs-num=
ber">16</span>)
</code></pre>
<h4 id=3D"step-5-define-the-compact-model-architecture" tabindex=3D"-1"><a =
class=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-5-define-the=
-compact-model-architecture" aria-hidden=3D"true">#</a> Step 5: Define the =
Compact Model Architecture</h4>
<p>Here, we define a compact ASR model with a CTC encoder and a TinyBERT-ba=
sed language model.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch
<span class=3D"hljs-keyword">import</span> torch.nn <span class=3D"hljs-key=
word">as</span> nn
<span class=3D"hljs-keyword">from</span> transformers <span class=3D"hljs-k=
eyword">import</span> BertTokenizer, BertForSequenceClassification

<span class=3D"hljs-keyword">class</span> <span class=3D"hljs-title class_"=
>MultilingualASRModel</span>(nn.Module):
    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">__init__</span>(<span class=3D"hljs-params">self</span>):
        <span class=3D"hljs-built_in">super</span>(MultilingualASRModel, se=
lf).__init__()
        self.processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D=
"hljs-string">"facebook/wav2vec2-base"</span>)
        self.encoder =3D nn.Sequential(
            nn.Conv1d(<span class=3D"hljs-number">1</span>, <span class=3D"=
hljs-number">32</span>, kernel_size=3D<span class=3D"hljs-number">5</span>,=
 stride=3D<span class=3D"hljs-number">1</span>, padding=3D<span class=3D"hl=
js-number">2</span>),
            nn.ReLU(),
            nn.Conv1d(<span class=3D"hljs-number">32</span>, <span class=3D=
"hljs-number">64</span>, kernel_size=3D<span class=3D"hljs-number">5</span>=
, stride=3D<span class=3D"hljs-number">1</span>, padding=3D<span class=3D"h=
ljs-number">2</span>),
            nn.ReLU()
        )
        self.decoder =3D nn.Linear(<span class=3D"hljs-number">64</span>, s=
elf.processor.tokenizer.vocab_size)
        self.lm_model =3D BertForSequenceClassification.from_pretrained(<sp=
an class=3D"hljs-string">"huawei-noah/TinyBERT_General_6L_768D"</span>)
        self.lm_tokenizer =3D BertTokenizer.from_pretrained(<span class=3D"=
hljs-string">"huawei-noah/TinyBERT_General_6L_768D"</span>)

    <span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title funct=
ion_">forward</span>(<span class=3D"hljs-params">self, input_values, labels=
=3D<span class=3D"hljs-literal">None</span></span>):
        x =3D self.encoder(input_values.unsqueeze(<span class=3D"hljs-numbe=
r">1</span>))
        x =3D x.transpose(<span class=3D"hljs-number">1</span>, <span class=
=3D"hljs-number">2</span>)
        logits =3D self.decoder(x)
        <span class=3D"hljs-keyword">return</span> logits

<span class=3D"hljs-comment"># Instantiate the model</span>
model =3D MultilingualASRModel()
</code></pre>
<h4 id=3D"step-6-fine-tuning-the-model" tabindex=3D"-1"><a class=3D"header-=
anchor" href=3D"https://slickgpt.com/t/20#step-6-fine-tuning-the-model" ari=
a-hidden=3D"true">#</a> Step 6: Fine-Tuning the Model</h4>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">from=
</span> transformers <span class=3D"hljs-keyword">import</span> TrainingArg=
uments, Trainer

<span class=3D"hljs-comment"># Define training arguments</span>
training_args =3D TrainingArguments(
    output_dir=3D<span class=3D"hljs-string">"./results"</span>,
    evaluation_strategy=3D<span class=3D"hljs-string">"epoch"</span>,
    learning_rate=3D<span class=3D"hljs-number">1e-4</span>,
    per_device_train_batch_size=3D<span class=3D"hljs-number">16</span>,
    per_device_eval_batch_size=3D<span class=3D"hljs-number">16</span>,
    num_train_epochs=3D<span class=3D"hljs-number">10</span>,
    weight_decay=3D<span class=3D"hljs-number">0.01</span>,
    save_steps=3D<span class=3D"hljs-number">500</span>,
    eval_steps=3D<span class=3D"hljs-number">500</span>,
)

<span class=3D"hljs-comment"># Define Trainer</span>
trainer =3D Trainer(
    model=3Dmodel,
    args=3Dtraining_args,
    train_dataset=3Dtrain_dataset,
    eval_dataset=3Dval_dataset,
)

<span class=3D"hljs-comment"># Fine-tune the model</span>
trainer.train()
</code></pre>
<h4 id=3D"step-7-quantize-the-model" tabindex=3D"-1"><a class=3D"header-anc=
hor" href=3D"https://slickgpt.com/t/20#step-7-quantize-the-model" aria-hidd=
en=3D"true">#</a> Step 7: Quantize the Model</h4>
<p>Quantize the model to 8-bit to reduce its size.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch.quantization

<span class=3D"hljs-comment"># Quantize the model</span>
model.<span class=3D"hljs-built_in">eval</span>()
quantized_model =3D torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=3Dtorch.qint8
)

<span class=3D"hljs-comment"># Save the quantized model</span>
torch.save(quantized_model.state_dict(), <span class=3D"hljs-string">"quant=
ized_asr_model.pth"</span>)

<span class=3D"hljs-comment"># Check the model size</span>
<span class=3D"hljs-keyword">import</span> os

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">get_model_size</span>(<span class=3D"hljs-params">model_path</span>):
    total_size =3D <span class=3D"hljs-number">0</span>
    <span class=3D"hljs-keyword">for</span> dirpath, dirnames, filenames <s=
pan class=3D"hljs-keyword">in</span> os.walk(model_path):
        <span class=3D"hljs-keyword">for</span> f <span class=3D"hljs-keywo=
rd">in</span> filenames:
            fp =3D os.path.join(dirpath, f)
            total_size +=3D os.path.getsize(fp)
    <span class=3D"hljs-keyword">return</span> total_size / (<span class=3D=
"hljs-number">1024</span> * <span class=3D"hljs-number">1024</span>)  <span=
 class=3D"hljs-comment"># Convert to MB</span>

<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">f"Qu=
antized Model Size: <span class=3D"hljs-subst">{get_model_size(<span class=
=3D"hljs-string">'quantized_asr_model.pth'</span>)}</span> MB"</span>)
</code></pre>
<h4 id=3D"step-8-inference-on-mobile" tabindex=3D"-1"><a class=3D"header-an=
chor" href=3D"https://slickgpt.com/t/20#step-8-inference-on-mobile" aria-hi=
dden=3D"true">#</a> Step 8: Inference on Mobile</h4>
<p>Deploy the quantized model on a mobile device and measure the inference =
time.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torchaudio
<span class=3D"hljs-keyword">import</span> time

<span class=3D"hljs-comment"># Load quantized models</span>
processor =3D Wav2Vec2Processor.from_pretrained(<span class=3D"hljs-string"=
>"facebook/wav2vec2-base"</span>)
acoustic_model =3D MultilingualASRModel()
acoustic_model.load_state_dict(torch.load(<span class=3D"hljs-string">"quan=
tized_asr_model.pth"</span>))

<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">measure_inference_time</span>(<span class=3D"hljs-params">audio_file</spa=
n>):
    audio_input, _ =3D torchaudio.load(audio_file)
    input_values =3D processor(audio_input.squeeze().numpy(), sampling_rate=
=3D<span class=3D"hljs-number">16000</span>).input_values[<span class=3D"hl=
js-number">0</span>]
    input_values =3D torch.tensor(input_values).unsqueeze(<span class=3D"hl=
js-number">0</span>)

    <span class=3D"hljs-comment"># Measure acoustic model inference time</s=
pan>
    start_time =3D time.time()
    acoustic_model.<span class=3D"hljs-built_in">eval</span>()
    <span class=3D"hljs-keyword">with</span> torch.no_grad():
        logits =3D acoustic_model(input_values)
    end_time =3D time.time()

    inference_time =3D end_time - start_time
    <span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">=
f"Inference Time: <span class=3D"hljs-subst">{inference_time:<span class=3D=
"hljs-number">.4</span>f}</span> seconds"</span>)

    <span class=3D"hljs-keyword">return</span> logits, inference_time

<span class=3D"hljs-comment"># Example usage</span>
audio_file =3D <span class=3D"hljs-string">"path/to/3-5s/korean/audio.wav"<=
/span>
logits, inference_time =3D measure_inference_time(audio_file)
<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">"Log=
its:"</span>, logits)
</code></pre>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>By leveraging a smaller language model like TinyBERT and applying 8-bit =
quantization, you can create a multilingual ASR model suitable for on-devic=
e use. TinyBERT offers competitive performance with a smaller size and fast=
er inference compared to DistilBERT. This guide provides a practical approa=
ch to developing and deploying a multilingual ASR system for Korean, Englis=
h, and Chinese on mobile devices.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div =
class=3D"flex items-start py-3"><!----><div class=3D"overflow-hidden bg-bla=
ck rounded h-8 w-8 mr-3"><img referrerpolicy=3D"no-referrer" src=3D"https:/=
/lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G=
9E0cdWXQ=3Ds96-c" alt=3D"" loading=3D"lazy" class=3D"transition h-full w-fu=
ll opacity-100"></div><!----><div class=3D"flex flex-col flex-1"><div class=
=3D"flex items-center"><div class=3D"text-sm leading-none font-bold text-wh=
ite mr-2"><!----><span>You</span><!----></div><div class=3D"text-xs leading=
-none text-slate-400 hover:underline" title=3D"Friday, May 24, 2024 at 2:25=
 AM">2:25 AM</div></div><div class=3D"flex-1 chat-content break-words" agen=
t=3D"Assistant"><p>convert this model to onnx and tflite</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><!----><button cla=
ss=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:bor=
der-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://s=
lickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></div=
><div class=3D"chat-base px-6 group relative hover:bg-slate-800/50"><div cl=
ass=3D"flex items-start py-3"><!----><!----><div class=3D"h-8 w-8 mr-3 roun=
ded flex items-center justify-center bg-slate-500/20"><img src=3D"https://s=
lickgpt.com/img/slickgpt.svg" alt=3D"system" height=3D"24" width=3D"24"></d=
iv><div class=3D"flex flex-col flex-1"><div class=3D"flex items-center"><di=
v class=3D"text-sm leading-none font-bold text-white mr-2"><!----><!----><s=
pan>Assistant</span></div><div class=3D"text-xs leading-none text-slate-400=
 hover:underline" title=3D"Friday, May 24, 2024 at 2:25 AM">2:25 AM</div></=
div><div class=3D"flex-1 chat-content break-words" agent=3D"Assistant"><p>T=
o deploy the multilingual ASR model on mobile devices, converting the model=
 to ONNX and TensorFlow Lite (TFLite) formats is a suitable approach. Here=
=E2=80=99s a step-by-step guide for converting the model to ONNX and then t=
o TFLite:</p>
<h3 id=3D"step-by-step-guide-for-model-conversion" tabindex=3D"-1"><a class=
=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-by-step-guide-for=
-model-conversion" aria-hidden=3D"true">#</a> Step-by-Step Guide for Model =
Conversion</h3>
<h4 id=3D"step-1-set-up-the-environment" tabindex=3D"-1"><a class=3D"header=
-anchor" href=3D"https://slickgpt.com/t/20#step-1-set-up-the-environment" a=
ria-hidden=3D"true">#</a> Step 1: Set Up the Environment</h4>
<p>Ensure you have the necessary tools installed.</p>
<pre><code class=3D"hljs language-bash"><span class=3D"hljs-comment"># Crea=
te and activate a virtual environment (optional)</span>
python -m venv asr_env
<span class=3D"hljs-built_in">source</span> asr_env/bin/activate  <span cla=
ss=3D"hljs-comment"># On Windows use `.\asr_env\Scripts\activate`</span>

<span class=3D"hljs-comment"># Install necessary libraries</span>
pip install torch torchaudio transformers datasets onnx onnxruntime
pip install tf2onnx tensorflow
</code></pre>
<h4 id=3D"step-2-define-the-model-architecture" tabindex=3D"-1"><a class=3D=
"header-anchor" href=3D"https://slickgpt.com/t/20#step-2-define-the-model-a=
rchitecture" aria-hidden=3D"true">#</a> Step 2: Define the Model Architectu=
re</h4>
<p>Ensure your model is defined as shown in the previous steps. We will use=
 the <code>MultilingualASRModel</code> class.</p>
<h4 id=3D"step-3-save-and-load-the-model-for-conversion" tabindex=3D"-1"><a=
 class=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-3-save-and-=
load-the-model-for-conversion" aria-hidden=3D"true">#</a> Step 3: Save and =
Load the Model for Conversion</h4>
<p>Save the trained and quantized model.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch

<span class=3D"hljs-comment"># Assuming `model` is your trained and quantiz=
ed model</span>
torch.save(model.state_dict(), <span class=3D"hljs-string">"quantized_asr_m=
odel.pth"</span>)

<span class=3D"hljs-comment"># Load the model</span>
model.load_state_dict(torch.load(<span class=3D"hljs-string">"quantized_asr=
_model.pth"</span>))
model.<span class=3D"hljs-built_in">eval</span>()
</code></pre>
<h4 id=3D"step-4-convert-to-onnx" tabindex=3D"-1"><a class=3D"header-anchor=
" href=3D"https://slickgpt.com/t/20#step-4-convert-to-onnx" aria-hidden=3D"=
true">#</a> Step 4: Convert to ONNX</h4>
<p>Convert the PyTorch model to ONNX format.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> torch.onnx

<span class=3D"hljs-comment"># Dummy input for the model (batch_size=3D1, s=
equence_length=3D16000 for 1 second of audio at 16kHz)</span>
dummy_input =3D torch.randn(<span class=3D"hljs-number">1</span>, <span cla=
ss=3D"hljs-number">16000</span>)

<span class=3D"hljs-comment"># Export the model to ONNX</span>
torch.onnx.export(
    model,=20
    dummy_input,=20
    <span class=3D"hljs-string">"asr_model.onnx"</span>,=20
    input_names=3D[<span class=3D"hljs-string">"input"</span>],=20
    output_names=3D[<span class=3D"hljs-string">"output"</span>],=20
    dynamic_axes=3D{<span class=3D"hljs-string">"input"</span>: {<span clas=
s=3D"hljs-number">0</span>: <span class=3D"hljs-string">"batch_size"</span>=
, <span class=3D"hljs-number">1</span>: <span class=3D"hljs-string">"sequen=
ce_length"</span>}, <span class=3D"hljs-string">"output"</span>: {<span cla=
ss=3D"hljs-number">0</span>: <span class=3D"hljs-string">"batch_size"</span=
>, <span class=3D"hljs-number">1</span>: <span class=3D"hljs-string">"seque=
nce_length"</span>}},
    opset_version=3D<span class=3D"hljs-number">11</span>
)
</code></pre>
<h4 id=3D"step-5-verify-the-onnx-model" tabindex=3D"-1"><a class=3D"header-=
anchor" href=3D"https://slickgpt.com/t/20#step-5-verify-the-onnx-model" ari=
a-hidden=3D"true">#</a> Step 5: Verify the ONNX Model</h4>
<p>Verify the ONNX model using ONNX Runtime.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> onnx
<span class=3D"hljs-keyword">import</span> onnxruntime <span class=3D"hljs-=
keyword">as</span> ort

<span class=3D"hljs-comment"># Load the ONNX model</span>
onnx_model =3D onnx.load(<span class=3D"hljs-string">"asr_model.onnx"</span=
>)
onnx.checker.check_model(onnx_model)

<span class=3D"hljs-comment"># Create an ONNX Runtime session</span>
ort_session =3D ort.InferenceSession(<span class=3D"hljs-string">"asr_model=
.onnx"</span>)

<span class=3D"hljs-comment"># Run inference</span>
<span class=3D"hljs-keyword">def</span> <span class=3D"hljs-title function_=
">to_numpy</span>(<span class=3D"hljs-params">tensor</span>):
    <span class=3D"hljs-keyword">return</span> tensor.detach().cpu().numpy(=
) <span class=3D"hljs-keyword">if</span> tensor.requires_grad <span class=
=3D"hljs-keyword">else</span> tensor.cpu().numpy()

dummy_input =3D dummy_input.to(<span class=3D"hljs-string">"cpu"</span>)
inputs =3D {ort_session.get_inputs()[<span class=3D"hljs-number">0</span>].=
name: to_numpy(dummy_input)}
outputs =3D ort_session.run(<span class=3D"hljs-literal">None</span>, input=
s)

<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">"ONN=
X Output:"</span>, outputs)
</code></pre>
<h4 id=3D"step-6-convert-onnx-model-to-tensorflow" tabindex=3D"-1"><a class=
=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-6-convert-onnx-mo=
del-to-tensorflow" aria-hidden=3D"true">#</a> Step 6: Convert ONNX Model to=
 TensorFlow</h4>
<p>Convert the ONNX model to TensorFlow format using <code>tf2onnx</code>.<=
/p>
<pre><code class=3D"hljs language-bash"><span class=3D"hljs-comment"># Conv=
ert the ONNX model to TensorFlow</span>
python -m tf2onnx.convert --opset 11 --input asr_model.onnx --output asr_mo=
del.pb --inputs input:0 --outputs output:0
</code></pre>
<h4 id=3D"step-7-convert-tensorflow-model-to-tflite" tabindex=3D"-1"><a cla=
ss=3D"header-anchor" href=3D"https://slickgpt.com/t/20#step-7-convert-tenso=
rflow-model-to-tflite" aria-hidden=3D"true">#</a> Step 7: Convert TensorFlo=
w Model to TFLite</h4>
<p>Convert the TensorFlow model to TFLite format.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> tensorflow <span class=3D"hljs-keyword">as</span> tf

<span class=3D"hljs-comment"># Load the TensorFlow model</span>
converter =3D tf.lite.TFLiteConverter.from_saved_model(<span class=3D"hljs-=
string">"asr_model.pb"</span>)
tflite_model =3D converter.convert()

<span class=3D"hljs-comment"># Save the TFLite model</span>
<span class=3D"hljs-keyword">with</span> <span class=3D"hljs-built_in">open=
</span>(<span class=3D"hljs-string">"asr_model.tflite"</span>, <span class=
=3D"hljs-string">"wb"</span>) <span class=3D"hljs-keyword">as</span> f:
    f.write(tflite_model)
</code></pre>
<h4 id=3D"step-8-verify-the-tflite-model" tabindex=3D"-1"><a class=3D"heade=
r-anchor" href=3D"https://slickgpt.com/t/20#step-8-verify-the-tflite-model"=
 aria-hidden=3D"true">#</a> Step 8: Verify the TFLite Model</h4>
<p>Verify the TFLite model by running inference.</p>
<pre><code class=3D"hljs language-python"><span class=3D"hljs-keyword">impo=
rt</span> numpy <span class=3D"hljs-keyword">as</span> np

<span class=3D"hljs-comment"># Load the TFLite model</span>
interpreter =3D tf.lite.Interpreter(model_path=3D<span class=3D"hljs-string=
">"asr_model.tflite"</span>)
interpreter.allocate_tensors()

<span class=3D"hljs-comment"># Get input and output tensors.</span>
input_details =3D interpreter.get_input_details()
output_details =3D interpreter.get_output_details()

<span class=3D"hljs-comment"># Prepare the input tensor</span>
input_shape =3D input_details[<span class=3D"hljs-number">0</span>][<span c=
lass=3D"hljs-string">'shape'</span>]
input_data =3D np.array(dummy_input, dtype=3Dnp.float32)
interpreter.set_tensor(input_details[<span class=3D"hljs-number">0</span>][=
<span class=3D"hljs-string">'index'</span>], input_data)

<span class=3D"hljs-comment"># Run the inference</span>
interpreter.invoke()

<span class=3D"hljs-comment"># Get the output tensor</span>
output_data =3D interpreter.get_tensor(output_details[<span class=3D"hljs-n=
umber">0</span>][<span class=3D"hljs-string">'index'</span>])
<span class=3D"hljs-built_in">print</span>(<span class=3D"hljs-string">"TFL=
ite Output:"</span>, output_data)
</code></pre>
<h3 id=3D"conclusion" tabindex=3D"-1"><a class=3D"header-anchor" href=3D"ht=
tps://slickgpt.com/t/20#conclusion" aria-hidden=3D"true">#</a> Conclusion</=
h3>
<p>This guide walks you through converting a multilingual ASR model from Py=
Torch to ONNX and then to TensorFlow Lite. This process ensures that your m=
odel is optimized for on-device deployment, making it suitable for mobile a=
pplications. By following these steps, you can leverage the efficiency and =
portability of ONNX and TFLite formats to deploy your ASR model on various =
devices.</p>
</div></div></div><div class=3D"absolute right-0 top-0 -mt-4 mr-4 h-8 px-2 =
flex items-center justify-center text-xs font-bold text-white bg-slate-800 =
rounded-md transition opacity-0 group-hover:opacity-100"><button class=3D"p=
-1 rounded hover:bg-slate-100/10 border border-transparent hover:border-sla=
te-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https://slickgpt=
.com/img/emoji/symbols/repeat-button.svg" alt=3D"repeat"></button><button c=
lass=3D"p-1 rounded hover:bg-slate-100/10 border border-transparent hover:b=
order-slate-600 group cursor-pointer"><img class=3D"h-4 w-4" src=3D"https:/=
/slickgpt.com/img/emoji/objects/pencil.svg" alt=3D"edit"></button></div></d=
iv></div><div class=3D"sticky bottom-0 border-r border-slate-800/50 text-wh=
ite bg-[#090b16] px-2 pb-2 sm:px-6 sm:pb-6 flex gap-2 mt-6 w-full h-auto z-=
10 max-sm:max-h-11"><!----><div class=3D"group relative -mt-1.5 flex w-full=
 rounded-md border border-slate-500/30 focus-within:border-blue-500/50 bg-s=
late-900 p-0.5 text-base font-normal text-white outline-slate-200/40 active=
:outline-2 max-sm:text-sm md:p-1 shrink"><div class=3D"relative flex w-full=
 items-center gap-2"><textarea autocomplete=3D"off" placeholder=3D"Write a =
message..." name=3D"message" title=3D"message" class=3D"font-default pl-4 w=
-full placeholder:text-slate-500 focus:outline-none bg-transparent border-n=
one appearance-none focus:ring-0 resize-none max-h-[60vh]" style=3D"height:=
 40px;"></textarea></div><div class=3D"group-focus-within:opacity-0 transit=
ion flex items-center space-x-0.5 text-xs font-black text-slate-100 dark:te=
xt-slate-500"><kbd class=3D"w-5 h-5 flex items-center justify-center border=
 uppercase border-slate-800 rounded-md">=E2=8C=A5</kbd><kbd class=3D"w-5 h-=
5 flex items-center justify-center border uppercase border-slate-800 rounde=
d-md">L</kbd></div><button class=3D"flex w-auto justify-center items-center=
 p-2 aspect-square text-base font-medium shadow-sm transition-colors rounde=
d-full focus:outline-none bg-slate-800/10 text-slate-500/50" aria-label=3D"=
Send message"><svg class=3D"w-5 h-5" xmlns=3D"http://www.w3.org/2000/svg" v=
iewBox=3D"0 0 24 24" fill=3D"currentColor"><path d=3D"M3.478 2.405a.75.75 0=
 00-.926.94l2.432 7.905H13.5a.75.75 0 010 1.5H4.984l-2.432 7.905a.75.75 0 0=
0.926.94 60.519 60.519 0 0018.445-8.986.75.75 0 000-1.218A60.517 60.517 0 0=
03.478 2.405z"></path></svg></button></div></div></div></div></div><div cla=
ss=3D"fixed z-50 inset-0 flex items-end justify-center px-4 py-6 pointer-ev=
ents-none sm:px-6 sm:py-12 sm:items-start sm:justify-end"><div class=3D"max=
-w-sm w-full"></div></div><!----></div>
   =20

   =20
 =20

<!----></body></html>
------MultipartBoundary--6sVMFWoZJHSn1FW7TfhRLnwT0nyOM3FYO4qr2cklbx----
Content-Type: text/css
Content-Transfer-Encoding: quoted-printable
Content-Location: cid:css-ce4cbb40-bf19-4ad7-a5fb-e68cc3205d8e@mhtml.blink

@charset "utf-8";

body { background-color: rgb(249, 249, 250); color: rgb(17, 24, 39); margin=
: 0px; overflow-y: overlay; }

body.dark { background-color: rgb(8, 11, 22); color: rgb(249, 249, 253); }

.loading-app { min-height: 100vh; display: flex; -webkit-box-orient: vertic=
al; -webkit-box-direction: normal; flex-direction: column; -webkit-box-pack=
: center; justify-content: center; -webkit-box-align: center; align-items: =
center; overflow: hidden; }

.loading-app img, .animate-ring { animation: 2s linear 0s infinite normal n=
one running ring; }

@-webkit-keyframes ring {=20
  0% { transform: rotate(0deg); animation-timing-function: cubic-bezier(0.5=
5, 0.055, 0.675, 0.19); }
  50% { transform: rotate(900deg); animation-timing-function: cubic-bezier(=
0.215, 0.61, 0.355, 1); }
  100% { transform: rotate(1800deg); }
}

@keyframes ring {=20
  0% { transform: rotate(0deg); animation-timing-function: cubic-bezier(0.5=
5, 0.055, 0.675, 0.19); }
  50% { transform: rotate(900deg); animation-timing-function: cubic-bezier(=
0.215, 0.61, 0.355, 1); }
  100% { transform: rotate(1800deg); }
}
------MultipartBoundary--6sVMFWoZJHSn1FW7TfhRLnwT0nyOM3FYO4qr2cklbx----
Content-Type: text/css
Content-Transfer-Encoding: quoted-printable
Content-Location: https://slickgpt.com/assets/index-0587c55e.css

@charset "utf-8";

#bar1[data-v-30b9e74e] { animation: 3s linear 0s infinite normal none runni=
ng move1-30b9e74e, 3s linear 0s infinite normal none running grow1-30b9e74e=
; }

#bar2[data-v-30b9e74e] { animation: 3s linear 0s infinite normal none runni=
ng move2-30b9e74e, 3s linear 0s infinite normal none running grow2-30b9e74e=
; }

@-webkit-keyframes move1-30b9e74e {=20
  0% { left: -100%; }
  10% { left: 0%; }
  65% { left: 80%; }
  100% { left: 100%; }
}

@keyframes move1-30b9e74e {=20
  0% { left: -100%; }
  10% { left: 0%; }
  65% { left: 80%; }
  100% { left: 100%; }
}

@-webkit-keyframes grow1-30b9e74e {=20
  0% { transform: scaleX(0.8); }
  50% { transform: scaleX(0.9); }
  75% { transform: scaleX(0.1); }
  100% { transform: scaleX(0.1); }
}

@keyframes grow1-30b9e74e {=20
  0% { transform: scaleX(0.8); }
  50% { transform: scaleX(0.9); }
  75% { transform: scaleX(0.1); }
  100% { transform: scaleX(0.1); }
}

@-webkit-keyframes move2-30b9e74e {=20
  0% { left: -100%; }
  50% { left: -100%; }
  100% { left: 100%; }
}

@keyframes move2-30b9e74e {=20
  0% { left: -100%; }
  50% { left: -100%; }
  100% { left: 100%; }
}

@-webkit-keyframes grow2-30b9e74e {=20
  0% { transform: scaleX(0.8); }
  50% { transform: scaleX(0.9); }
  75% { transform: scaleX(0.8); }
  100% { transform: scaleX(0.7); }
}

@keyframes grow2-30b9e74e {=20
  0% { transform: scaleX(0.8); }
  50% { transform: scaleX(0.9); }
  75% { transform: scaleX(0.8); }
  100% { transform: scaleX(0.7); }
}

*, ::before, ::after { box-sizing: border-box; border-width: 0px; border-st=
yle: solid; border-color: rgb(226, 232, 240); }

::before, ::after { --tw-content: ""; }

html { line-height: 1.5; text-size-adjust: 100%; tab-size: 4; font-family: =
Satoshi-Variable, Satoshi, ui-sans-serif, system-ui, -apple-system, BlinkMa=
cSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans=
-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Col=
or Emoji"; font-feature-settings: normal; font-variation-settings: normal; =
}

body { margin: 0px; line-height: inherit; }

hr { height: 0px; color: inherit; border-top-width: 1px; }

abbr:where([title]) { text-decoration: underline dotted; }

h1, h2, h3, h4, h5, h6 { font-size: inherit; font-weight: inherit; }

a { color: inherit; text-decoration: inherit; }

b, strong { font-weight: bolder; }

code, kbd, samp, pre { font-family: MonoLisa, ui-monospace, SFMono-Regular,=
 Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; font=
-size: 1em; }

small { font-size: 80%; }

sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-ali=
gn: baseline; }

sub { bottom: -0.25em; }

sup { top: -0.5em; }

table { text-indent: 0px; border-color: inherit; border-collapse: collapse;=
 }

button, input, optgroup, select, textarea { font-family: inherit; font-feat=
ure-settings: inherit; font-variation-settings: inherit; font-size: 100%; f=
ont-weight: inherit; line-height: inherit; color: inherit; margin: 0px; pad=
ding: 0px; }

button, select { text-transform: none; }

button, [type=3D"button"], [type=3D"reset"], [type=3D"submit"] { appearance=
: button; background-color: transparent; background-image: none; }

progress { vertical-align: baseline; }

::-webkit-inner-spin-button, ::-webkit-outer-spin-button { height: auto; }

[type=3D"search"] { appearance: textfield; outline-offset: -2px; }

::-webkit-search-decoration { appearance: none; }

::-webkit-file-upload-button { appearance: button; font: inherit; }

summary { display: list-item; }

blockquote, dl, dd, h1, h2, h3, h4, h5, h6, hr, figure, p, pre { margin: 0p=
x; }

fieldset { margin: 0px; padding: 0px; }

legend { padding: 0px; }

ol, ul, menu { list-style: none; margin: 0px; padding: 0px; }

dialog { padding: 0px; }

textarea { resize: vertical; }

input::-webkit-input-placeholder, textarea::-webkit-input-placeholder { opa=
city: 1; color: rgb(148, 163, 184); }

input::placeholder, textarea::placeholder { opacity: 1; color: rgb(148, 163=
, 184); }

button, [role=3D"button"] { cursor: pointer; }

:disabled { cursor: default; }

img, svg, video, canvas, audio, iframe, embed, object { display: block; ver=
tical-align: middle; }

img, video { max-width: 100%; height: auto; }

[hidden] { display: none; }

[type=3D"text"], input:where(:not([type])), [type=3D"email"], [type=3D"url"=
], [type=3D"password"], [type=3D"number"], [type=3D"date"], [type=3D"dateti=
me-local"], [type=3D"month"], [type=3D"search"], [type=3D"tel"], [type=3D"t=
ime"], [type=3D"week"], [multiple], textarea, select { appearance: none; ba=
ckground-color: rgb(255, 255, 255); border-color: rgb(100, 116, 139); borde=
r-width: 1px; border-radius: 0px; padding: 0.5rem 0.75rem; font-size: 1rem;=
 line-height: 1.5rem; --tw-shadow: 0 0 #0000; }

[type=3D"text"]:focus, input:where(:not([type])):focus, [type=3D"email"]:fo=
cus, [type=3D"url"]:focus, [type=3D"password"]:focus, [type=3D"number"]:foc=
us, [type=3D"date"]:focus, [type=3D"datetime-local"]:focus, [type=3D"month"=
]:focus, [type=3D"search"]:focus, [type=3D"tel"]:focus, [type=3D"time"]:foc=
us, [type=3D"week"]:focus, [multiple]:focus, textarea:focus, select:focus {=
 outline: transparent solid 2px; outline-offset: 2px; --tw-ring-inset: var(=
--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; -=
-tw-ring-color: #3182CE; --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 =
0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow=
: var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--=
tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shad=
ow),var(--tw-shadow); border-color: rgb(49, 130, 206); }

input::-webkit-input-placeholder, textarea::-webkit-input-placeholder { col=
or: rgb(100, 116, 139); opacity: 1; }

input::placeholder, textarea::placeholder { color: rgb(100, 116, 139); opac=
ity: 1; }

::-webkit-datetime-edit-fields-wrapper { padding: 0px; }

::-webkit-date-and-time-value { min-height: 1.5em; text-align: inherit; }

::-webkit-datetime-edit { display: inline-flex; }

::-webkit-datetime-edit, ::-webkit-datetime-edit-year-field, ::-webkit-date=
time-edit-month-field, ::-webkit-datetime-edit-day-field, ::-webkit-datetim=
e-edit-hour-field, ::-webkit-datetime-edit-minute-field, ::-webkit-datetime=
-edit-second-field, ::-webkit-datetime-edit-millisecond-field, ::-webkit-da=
tetime-edit-meridiem-field { padding-top: 0px; padding-bottom: 0px; }

select { background-image: url("data:image/svg+xml,%3csvg xmlns=3D'http://w=
ww.w3.org/2000/svg' fill=3D'none' viewBox=3D'0 0 20 20'%3e%3cpath stroke=3D=
'%2364748b' stroke-linecap=3D'round' stroke-linejoin=3D'round' stroke-width=
=3D'1.5' d=3D'M6 8l4 4 4-4'/%3e%3c/svg%3e"); background-position: right 0.5=
rem center; background-repeat: no-repeat; background-size: 1.5em 1.5em; pad=
ding-right: 2.5rem; -webkit-print-color-adjust: exact; }

[multiple], [size]:where(select:not([size=3D"1"])) { background-image: init=
ial; background-position: initial; background-repeat: unset; background-siz=
e: initial; padding-right: 0.75rem; -webkit-print-color-adjust: unset; }

[type=3D"checkbox"], [type=3D"radio"] { appearance: none; padding: 0px; -we=
bkit-print-color-adjust: exact; display: inline-block; vertical-align: midd=
le; background-origin: border-box; user-select: none; flex-shrink: 0; heigh=
t: 1rem; width: 1rem; color: rgb(49, 130, 206); background-color: rgb(255, =
255, 255); border-color: rgb(100, 116, 139); border-width: 1px; --tw-shadow=
: 0 0 #0000; }

[type=3D"checkbox"] { border-radius: 0px; }

[type=3D"radio"] { border-radius: 100%; }

[type=3D"checkbox"]:focus, [type=3D"radio"]:focus { outline: transparent so=
lid 2px; outline-offset: 2px; --tw-ring-inset: var(--tw-empty, ); --tw-ring=
-offset-width: 2px; --tw-ring-offset-color: #fff; --tw-ring-color: #3182CE;=
 --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-w=
idth) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0=
 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shad=
ow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow); }

[type=3D"checkbox"]:checked, [type=3D"radio"]:checked { border-color: trans=
parent; background-color: currentcolor; background-size: 100% 100%; backgro=
und-position: center center; background-repeat: no-repeat; }

[type=3D"checkbox"]:checked { background-image: url("data:image/svg+xml,%3c=
svg viewBox=3D'0 0 16 16' fill=3D'white' xmlns=3D'http://www.w3.org/2000/sv=
g'%3e%3cpath d=3D'M12.207 4.793a1 1 0 010 1.414l-5 5a1 1 0 01-1.414 0l-2-2a=
1 1 0 011.414-1.414L6.5 9.086l4.293-4.293a1 1 0 011.414 0z'/%3e%3c/svg%3e")=
; }

[type=3D"radio"]:checked { background-image: url("data:image/svg+xml,%3csvg=
 viewBox=3D'0 0 16 16' fill=3D'white' xmlns=3D'http://www.w3.org/2000/svg'%=
3e%3ccircle cx=3D'8' cy=3D'8' r=3D'3'/%3e%3c/svg%3e"); }

[type=3D"checkbox"]:checked:hover, [type=3D"checkbox"]:checked:focus, [type=
=3D"radio"]:checked:hover, [type=3D"radio"]:checked:focus { border-color: t=
ransparent; background-color: currentcolor; }

[type=3D"checkbox"]:indeterminate { background-image: url("data:image/svg+x=
ml,%3csvg xmlns=3D'http://www.w3.org/2000/svg' fill=3D'none' viewBox=3D'0 0=
 16 16'%3e%3cpath stroke=3D'white' stroke-linecap=3D'round' stroke-linejoin=
=3D'round' stroke-width=3D'2' d=3D'M4 8h8'/%3e%3c/svg%3e"); border-color: t=
ransparent; background-color: currentcolor; background-size: 100% 100%; bac=
kground-position: center center; background-repeat: no-repeat; }

[type=3D"checkbox"]:indeterminate:hover, [type=3D"checkbox"]:indeterminate:=
focus { border-color: transparent; background-color: currentcolor; }

[type=3D"file"] { background: unset; border-color: inherit; border-width: 0=
px; border-radius: 0px; padding: 0px; font-size: unset; line-height: inheri=
t; }

[type=3D"file"]:focus { outline: -webkit-focus-ring-color auto 1px; }

*, ::before, ::after { --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; =
--tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; -=
-tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: =
; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-=
from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ;=
 --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-s=
pacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-widt=
h: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(65 153 225 / .5)=
; --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-sha=
dow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightnes=
s: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; -=
-tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --t=
w-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale:=
 ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacit=
y: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

::-webkit-backdrop { --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --=
tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --t=
w-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; =
--tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-fr=
om-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; -=
-tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spa=
cing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width:=
 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(65 153 225 / .5); =
--tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shado=
w: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness:=
 ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --t=
w-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-=
backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ;=
 --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity:=
 ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

::backdrop { --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-trans=
late-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y=
: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pin=
ch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-posit=
ion: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordi=
nal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; =
--tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --=
tw-ring-offset-color: #fff; --tw-ring-color: rgb(65 153 225 / .5); --tw-rin=
g-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #=
0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-=
contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-satura=
te: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop=
-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-ba=
ckdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-=
backdrop-saturate: ; --tw-backdrop-sepia: ; }

.\!container { width: 100% !important; }

.container { width: 100%; }

@media (min-width: 480px) {
  .\!container { max-width: 480px !important; }
  .container { max-width: 480px; }
}

@media (min-width: 640px) {
  .\!container { max-width: 640px !important; }
  .container { max-width: 640px; }
}

@media (min-width: 768px) {
  .\!container { max-width: 768px !important; }
  .container { max-width: 768px; }
}

@media (min-width: 1024px) {
  .\!container { max-width: 1024px !important; }
  .container { max-width: 1024px; }
}

@media (min-width: 1280px) {
  .\!container { max-width: 1280px !important; }
  .container { max-width: 1280px; }
}

@media (min-width: 1536px) {
  .\!container { max-width: 1536px !important; }
  .container { max-width: 1536px; }
}

.form-input, .form-textarea, .form-select, .form-multiselect { appearance: =
none; background-color: rgb(255, 255, 255); border-color: rgb(100, 116, 139=
); border-width: 1px; border-radius: 0px; padding: 0.5rem 0.75rem; font-siz=
e: 1rem; line-height: 1.5rem; --tw-shadow: 0 0 #0000; }

.form-input:focus, .form-textarea:focus, .form-select:focus, .form-multisel=
ect:focus { outline: transparent solid 2px; outline-offset: 2px; --tw-ring-=
inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-col=
or: #fff; --tw-ring-color: #3182CE; --tw-ring-offset-shadow: var(--tw-ring-=
inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-=
ring-shadow: var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-wid=
th)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--t=
w-ring-shadow),var(--tw-shadow); border-color: rgb(49, 130, 206); }

.form-input::-webkit-input-placeholder, .form-textarea::-webkit-input-place=
holder { color: rgb(100, 116, 139); opacity: 1; }

.form-input::placeholder, .form-textarea::placeholder { color: rgb(100, 116=
, 139); opacity: 1; }

.form-input::-webkit-datetime-edit-fields-wrapper { padding: 0px; }

.form-input::-webkit-date-and-time-value { min-height: 1.5em; text-align: i=
nherit; }

.form-input::-webkit-datetime-edit { display: inline-flex; }

.form-input::-webkit-datetime-edit, .form-input::-webkit-datetime-edit-year=
-field, .form-input::-webkit-datetime-edit-month-field, .form-input::-webki=
t-datetime-edit-day-field, .form-input::-webkit-datetime-edit-hour-field, .=
form-input::-webkit-datetime-edit-minute-field, .form-input::-webkit-dateti=
me-edit-second-field, .form-input::-webkit-datetime-edit-millisecond-field,=
 .form-input::-webkit-datetime-edit-meridiem-field { padding-top: 0px; padd=
ing-bottom: 0px; }

.form-select { background-image: url("data:image/svg+xml,%3csvg xmlns=3D'ht=
tp://www.w3.org/2000/svg' fill=3D'none' viewBox=3D'0 0 20 20'%3e%3cpath str=
oke=3D'%2364748b' stroke-linecap=3D'round' stroke-linejoin=3D'round' stroke=
-width=3D'1.5' d=3D'M6 8l4 4 4-4'/%3e%3c/svg%3e"); background-position: rig=
ht 0.5rem center; background-repeat: no-repeat; background-size: 1.5em 1.5e=
m; padding-right: 2.5rem; -webkit-print-color-adjust: exact; }

.form-select:where([size]:not([size=3D"1"])) { background-image: initial; b=
ackground-position: initial; background-repeat: unset; background-size: ini=
tial; padding-right: 0.75rem; -webkit-print-color-adjust: unset; }

.prose { color: var(--tw-prose-body); max-width: 65ch; }

.prose :where(p):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"] =
*)) { margin-top: 1.25em; margin-bottom: 1.25em; }

.prose :where([class~=3D"lead"]):not(:where([class~=3D"not-prose"], [class~=
=3D"not-prose"] *)) { color: var(--tw-prose-lead); font-size: 1.25em; line-=
height: 1.6; margin-top: 1.2em; margin-bottom: 1.2em; }

.prose :where(a):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"] =
*)) { color: var(--tw-prose-links); text-decoration: underline; font-weight=
: 500; }

.prose :where(strong):not(:where([class~=3D"not-prose"], [class~=3D"not-pro=
se"] *)) { color: var(--tw-prose-bold); font-weight: 600; }

.prose :where(a strong):not(:where([class~=3D"not-prose"], [class~=3D"not-p=
rose"] *)) { color: inherit; }

.prose :where(blockquote strong):not(:where([class~=3D"not-prose"], [class~=
=3D"not-prose"] *)) { color: inherit; }

.prose :where(thead th strong):not(:where([class~=3D"not-prose"], [class~=
=3D"not-prose"] *)) { color: inherit; }

.prose :where(ol):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { list-style-type: decimal; margin-top: 1.25em; margin-bottom: 1.25em;=
 padding-left: 1.625em; }

.prose :where(ol[type=3D"A"]):not(:where([class~=3D"not-prose"], [class~=3D=
"not-prose"] *)) { list-style-type: upper-alpha; }

.prose :where(ol[type=3D"a"]):not(:where([class~=3D"not-prose"], [class~=3D=
"not-prose"] *)) { list-style-type: lower-alpha; }

.prose :where():not(:where([class~=3D"not-prose"], [class~=3D"not-prose"] *=
)) { list-style-type: upper-alpha; }

.prose :where():not(:where([class~=3D"not-prose"], [class~=3D"not-prose"] *=
)) { list-style-type: lower-alpha; }

.prose :where(ol[type=3D"I"]):not(:where([class~=3D"not-prose"], [class~=3D=
"not-prose"] *)) { list-style-type: upper-roman; }

.prose :where(ol[type=3D"i"]):not(:where([class~=3D"not-prose"], [class~=3D=
"not-prose"] *)) { list-style-type: lower-roman; }

.prose :where():not(:where([class~=3D"not-prose"], [class~=3D"not-prose"] *=
)) { list-style-type: upper-roman; }

.prose :where():not(:where([class~=3D"not-prose"], [class~=3D"not-prose"] *=
)) { list-style-type: lower-roman; }

.prose :where(ol[type=3D"1"]):not(:where([class~=3D"not-prose"], [class~=3D=
"not-prose"] *)) { list-style-type: decimal; }

.prose :where(ul):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { list-style-type: disc; margin-top: 1.25em; margin-bottom: 1.25em; pa=
dding-left: 1.625em; }

.prose :where(ol > li):not(:where([class~=3D"not-prose"], [class~=3D"not-pr=
ose"] *))::marker { font-weight: 400; color: var(--tw-prose-counters); }

.prose :where(ul > li):not(:where([class~=3D"not-prose"], [class~=3D"not-pr=
ose"] *))::marker { color: var(--tw-prose-bullets); }

.prose :where(dt):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { color: var(--tw-prose-headings); font-weight: 600; margin-top: 1.25e=
m; }

.prose :where(hr):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { border-color: var(--tw-prose-hr); border-top-width: 1px; margin-top:=
 3em; margin-bottom: 3em; }

.prose :where(blockquote):not(:where([class~=3D"not-prose"], [class~=3D"not=
-prose"] *)) { font-weight: 500; font-style: italic; color: var(--tw-prose-=
quotes); border-left-width: 0.25rem; border-left-color: var(--tw-prose-quot=
e-borders); quotes: "=E2=80=9C" "=E2=80=9D" "=E2=80=98" "=E2=80=99"; margin=
-top: 1.6em; margin-bottom: 1.6em; padding-left: 1em; }

.prose :where(blockquote p:first-of-type):not(:where([class~=3D"not-prose"]=
, [class~=3D"not-prose"] *))::before { content: open-quote; }

.prose :where(blockquote p:last-of-type):not(:where([class~=3D"not-prose"],=
 [class~=3D"not-prose"] *))::after { content: close-quote; }

.prose :where(h1):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { color: var(--tw-prose-headings); font-weight: 800; font-size: 2.25em=
; margin-top: 0px; margin-bottom: 0.888889em; line-height: 1.11111; }

.prose :where(h1 strong):not(:where([class~=3D"not-prose"], [class~=3D"not-=
prose"] *)) { font-weight: 900; color: inherit; }

.prose :where(h2):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { color: var(--tw-prose-headings); font-weight: 700; font-size: 1.5em;=
 margin-top: 2em; margin-bottom: 1em; line-height: 1.33333; }

.prose :where(h2 strong):not(:where([class~=3D"not-prose"], [class~=3D"not-=
prose"] *)) { font-weight: 800; color: inherit; }

.prose :where(h3):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { color: var(--tw-prose-headings); font-weight: 600; font-size: 1.25em=
; margin-top: 1.6em; margin-bottom: 0.6em; line-height: 1.6; }

.prose :where(h3 strong):not(:where([class~=3D"not-prose"], [class~=3D"not-=
prose"] *)) { font-weight: 700; color: inherit; }

.prose :where(h4):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { color: var(--tw-prose-headings); font-weight: 600; margin-top: 1.5em=
; margin-bottom: 0.5em; line-height: 1.5; }

.prose :where(h4 strong):not(:where([class~=3D"not-prose"], [class~=3D"not-=
prose"] *)) { font-weight: 700; color: inherit; }

.prose :where(img):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"=
] *)) { margin-top: 2em; margin-bottom: 2em; }

.prose :where(picture):not(:where([class~=3D"not-prose"], [class~=3D"not-pr=
ose"] *)) { display: block; margin-top: 2em; margin-bottom: 2em; }

.prose :where(kbd):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"=
] *)) { font-weight: 500; font-family: inherit; color: var(--tw-prose-kbd);=
 box-shadow: 0 0 0 1px rgb(var(--tw-prose-kbd-shadows) / 10%),0 3px 0 rgb(v=
ar(--tw-prose-kbd-shadows) / 10%); font-size: 0.875em; border-radius: 0.312=
5rem; padding: 0.1875em 0.375em; }

.prose :where(code):not(:where([class~=3D"not-prose"], [class~=3D"not-prose=
"] *)) { color: var(--tw-prose-code); font-weight: 600; font-size: 0.875em;=
 }

.prose :where(code):not(:where([class~=3D"not-prose"], [class~=3D"not-prose=
"] *))::before { content: "`"; }

.prose :where(code):not(:where([class~=3D"not-prose"], [class~=3D"not-prose=
"] *))::after { content: "`"; }

.prose :where(a code):not(:where([class~=3D"not-prose"], [class~=3D"not-pro=
se"] *)) { color: inherit; }

.prose :where(h1 code):not(:where([class~=3D"not-prose"], [class~=3D"not-pr=
ose"] *)) { color: inherit; }

.prose :where(h2 code):not(:where([class~=3D"not-prose"], [class~=3D"not-pr=
ose"] *)) { color: inherit; font-size: 0.875em; }

.prose :where(h3 code):not(:where([class~=3D"not-prose"], [class~=3D"not-pr=
ose"] *)) { color: inherit; font-size: 0.9em; }

.prose :where(h4 code):not(:where([class~=3D"not-prose"], [class~=3D"not-pr=
ose"] *)) { color: inherit; }

.prose :where(blockquote code):not(:where([class~=3D"not-prose"], [class~=
=3D"not-prose"] *)) { color: inherit; }

.prose :where(thead th code):not(:where([class~=3D"not-prose"], [class~=3D"=
not-prose"] *)) { color: inherit; }

.prose :where(pre):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"=
] *)) { color: var(--tw-prose-pre-code); background-color: var(--tw-prose-p=
re-bg); overflow-x: auto; font-weight: 400; font-size: 0.875em; line-height=
: 1.71429; margin-top: 1.71429em; margin-bottom: 1.71429em; border-radius: =
0.375rem; padding: 0.857143em 1.14286em; }

.prose :where(pre code):not(:where([class~=3D"not-prose"], [class~=3D"not-p=
rose"] *)) { background-color: transparent; border-width: 0px; border-radiu=
s: 0px; padding: 0px; font-weight: inherit; color: inherit; font-size: inhe=
rit; font-family: inherit; line-height: inherit; }

.prose :where(pre code):not(:where([class~=3D"not-prose"], [class~=3D"not-p=
rose"] *))::before { content: none; }

.prose :where(pre code):not(:where([class~=3D"not-prose"], [class~=3D"not-p=
rose"] *))::after { content: none; }

.prose :where(table):not(:where([class~=3D"not-prose"], [class~=3D"not-pros=
e"] *)) { width: 100%; table-layout: auto; text-align: left; margin-top: 2e=
m; margin-bottom: 2em; font-size: 0.875em; line-height: 1.71429; }

.prose :where(thead):not(:where([class~=3D"not-prose"], [class~=3D"not-pros=
e"] *)) { border-bottom-width: 1px; border-bottom-color: var(--tw-prose-th-=
borders); }

.prose :where(thead th):not(:where([class~=3D"not-prose"], [class~=3D"not-p=
rose"] *)) { color: var(--tw-prose-headings); font-weight: 600; vertical-al=
ign: bottom; padding-right: 0.571429em; padding-bottom: 0.571429em; padding=
-left: 0.571429em; }

.prose :where(tbody tr):not(:where([class~=3D"not-prose"], [class~=3D"not-p=
rose"] *)) { border-bottom-width: 1px; border-bottom-color: var(--tw-prose-=
td-borders); }

.prose :where(tbody tr:last-child):not(:where([class~=3D"not-prose"], [clas=
s~=3D"not-prose"] *)) { border-bottom-width: 0px; }

.prose :where(tbody td):not(:where([class~=3D"not-prose"], [class~=3D"not-p=
rose"] *)) { vertical-align: baseline; }

.prose :where(tfoot):not(:where([class~=3D"not-prose"], [class~=3D"not-pros=
e"] *)) { border-top-width: 1px; border-top-color: var(--tw-prose-th-border=
s); }

.prose :where(tfoot td):not(:where([class~=3D"not-prose"], [class~=3D"not-p=
rose"] *)) { vertical-align: top; }

.prose :where(figure > *):not(:where([class~=3D"not-prose"], [class~=3D"not=
-prose"] *)) { margin-top: 0px; margin-bottom: 0px; }

.prose :where(figcaption):not(:where([class~=3D"not-prose"], [class~=3D"not=
-prose"] *)) { color: var(--tw-prose-captions); font-size: 0.875em; line-he=
ight: 1.42857; margin-top: 0.857143em; }

.prose { --tw-prose-body: #374151; --tw-prose-headings: #111827; --tw-prose=
-lead: #4b5563; --tw-prose-links: #111827; --tw-prose-bold: #111827; --tw-p=
rose-counters: #6b7280; --tw-prose-bullets: #d1d5db; --tw-prose-hr: #e5e7eb=
; --tw-prose-quotes: #111827; --tw-prose-quote-borders: #e5e7eb; --tw-prose=
-captions: #6b7280; --tw-prose-kbd: #111827; --tw-prose-kbd-shadows: 17 24 =
39; --tw-prose-code: #111827; --tw-prose-pre-code: #e5e7eb; --tw-prose-pre-=
bg: #1f2937; --tw-prose-th-borders: #d1d5db; --tw-prose-td-borders: #e5e7eb=
; --tw-prose-invert-body: #d1d5db; --tw-prose-invert-headings: #fff; --tw-p=
rose-invert-lead: #9ca3af; --tw-prose-invert-links: #fff; --tw-prose-invert=
-bold: #fff; --tw-prose-invert-counters: #9ca3af; --tw-prose-invert-bullets=
: #4b5563; --tw-prose-invert-hr: #374151; --tw-prose-invert-quotes: #f3f4f6=
; --tw-prose-invert-quote-borders: #374151; --tw-prose-invert-captions: #9c=
a3af; --tw-prose-invert-kbd: #fff; --tw-prose-invert-kbd-shadows: 255 255 2=
55; --tw-prose-invert-code: #fff; --tw-prose-invert-pre-code: #d1d5db; --tw=
-prose-invert-pre-bg: rgb(0 0 0 / 50%); --tw-prose-invert-th-borders: #4b55=
63; --tw-prose-invert-td-borders: #374151; font-size: 1rem; line-height: 1.=
75; }

.prose :where(picture > img):not(:where([class~=3D"not-prose"], [class~=3D"=
not-prose"] *)) { margin-top: 0px; margin-bottom: 0px; }

.prose :where(video):not(:where([class~=3D"not-prose"], [class~=3D"not-pros=
e"] *)) { margin-top: 2em; margin-bottom: 2em; }

.prose :where(li):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { margin-top: 0.5em; margin-bottom: 0.5em; }

.prose :where(ol > li):not(:where([class~=3D"not-prose"], [class~=3D"not-pr=
ose"] *)) { padding-left: 0.375em; }

.prose :where(ul > li):not(:where([class~=3D"not-prose"], [class~=3D"not-pr=
ose"] *)) { padding-left: 0.375em; }

.prose :where(.prose > ul > li p):not(:where([class~=3D"not-prose"], [class=
~=3D"not-prose"] *)) { margin-top: 0.75em; margin-bottom: 0.75em; }

.prose :where(.prose > ul > li > :first-child):not(:where([class~=3D"not-pr=
ose"], [class~=3D"not-prose"] *)) { margin-top: 1.25em; }

.prose :where(.prose > ul > li > :last-child):not(:where([class~=3D"not-pro=
se"], [class~=3D"not-prose"] *)) { margin-bottom: 1.25em; }

.prose :where(.prose > ol > li > :first-child):not(:where([class~=3D"not-pr=
ose"], [class~=3D"not-prose"] *)) { margin-top: 1.25em; }

.prose :where(.prose > ol > li > :last-child):not(:where([class~=3D"not-pro=
se"], [class~=3D"not-prose"] *)) { margin-bottom: 1.25em; }

.prose :where(ul ul, ul ol, ol ul, ol ol):not(:where([class~=3D"not-prose"]=
, [class~=3D"not-prose"] *)) { margin-top: 0.75em; margin-bottom: 0.75em; }

.prose :where(dl):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { margin-top: 1.25em; margin-bottom: 1.25em; }

.prose :where(dd):not(:where([class~=3D"not-prose"], [class~=3D"not-prose"]=
 *)) { margin-top: 0.5em; padding-left: 1.625em; }

.prose :where(hr + *):not(:where([class~=3D"not-prose"], [class~=3D"not-pro=
se"] *)) { margin-top: 0px; }

.prose :where(h2 + *):not(:where([class~=3D"not-prose"], [class~=3D"not-pro=
se"] *)) { margin-top: 0px; }

.prose :where(h3 + *):not(:where([class~=3D"not-prose"], [class~=3D"not-pro=
se"] *)) { margin-top: 0px; }

.prose :where(h4 + *):not(:where([class~=3D"not-prose"], [class~=3D"not-pro=
se"] *)) { margin-top: 0px; }

.prose :where(thead th:first-child):not(:where([class~=3D"not-prose"], [cla=
ss~=3D"not-prose"] *)) { padding-left: 0px; }

.prose :where(thead th:last-child):not(:where([class~=3D"not-prose"], [clas=
s~=3D"not-prose"] *)) { padding-right: 0px; }

.prose :where(tbody td, tfoot td):not(:where([class~=3D"not-prose"], [class=
~=3D"not-prose"] *)) { padding: 0.571429em; }

.prose :where(tbody td:first-child, tfoot td:first-child):not(:where([class=
~=3D"not-prose"], [class~=3D"not-prose"] *)) { padding-left: 0px; }

.prose :where(tbody td:last-child, tfoot td:last-child):not(:where([class~=
=3D"not-prose"], [class~=3D"not-prose"] *)) { padding-right: 0px; }

.prose :where(figure):not(:where([class~=3D"not-prose"], [class~=3D"not-pro=
se"] *)) { margin-top: 2em; margin-bottom: 2em; }

.prose :where(.prose > :first-child):not(:where([class~=3D"not-prose"], [cl=
ass~=3D"not-prose"] *)) { margin-top: 0px; }

.prose :where(.prose > :last-child):not(:where([class~=3D"not-prose"], [cla=
ss~=3D"not-prose"] *)) { margin-bottom: 0px; }

pre code.hljs { display: block; overflow-x: auto; padding: 1em; }

code.hljs { padding: 3px 5px; }

.hljs { background: rgb(1, 22, 39); color: rgb(214, 222, 235); }

.hljs-keyword { color: rgb(199, 146, 234); font-style: italic; }

.hljs-built_in { color: rgb(173, 219, 103); font-style: italic; }

.hljs-type { color: rgb(130, 170, 255); }

.hljs-literal { color: rgb(255, 88, 116); }

.hljs-number { color: rgb(247, 140, 108); }

.hljs-regexp { color: rgb(92, 167, 228); }

.hljs-string { color: rgb(236, 196, 141); }

.hljs-subst { color: rgb(211, 66, 62); }

.hljs-symbol { color: rgb(130, 170, 255); }

.hljs-class { color: rgb(255, 203, 139); }

.hljs-function { color: rgb(130, 170, 255); }

.hljs-title { color: rgb(220, 220, 170); font-style: italic; }

.hljs-params { color: rgb(127, 219, 202); }

.hljs-comment { color: rgb(99, 119, 119); font-style: italic; }

.hljs-doctag { color: rgb(127, 219, 202); }

.hljs-meta, .hljs-meta .hljs-keyword { color: rgb(130, 170, 255); }

.hljs-meta .hljs-string { color: rgb(236, 196, 141); }

.hljs-section { color: rgb(130, 177, 255); }

.hljs-tag, .hljs-name, .hljs-attr { color: rgb(127, 219, 202); }

.hljs-attribute { color: rgb(128, 203, 196); }

.hljs-variable { color: rgb(173, 219, 103); }

.hljs-bullet { color: rgb(217, 245, 221); }

.hljs-code { color: rgb(128, 203, 196); }

.hljs-emphasis { color: rgb(199, 146, 234); font-style: italic; }

.hljs-strong { color: rgb(173, 219, 103); font-weight: 700; }

.hljs-formula { color: rgb(199, 146, 234); }

.hljs-link { color: rgb(255, 134, 154); }

.hljs-quote { color: rgb(105, 112, 152); font-style: italic; }

.hljs-selector-tag { color: rgb(255, 99, 99); }

.hljs-selector-id { color: rgb(250, 212, 48); }

.hljs-selector-class { color: rgb(173, 219, 103); font-style: italic; }

.hljs-selector-attr, .hljs-selector-pseudo { color: rgb(199, 146, 234); fon=
t-style: italic; }

.hljs-template-tag { color: rgb(199, 146, 234); }

.hljs-template-variable { color: rgb(173, 219, 103); }

.hljs-addition { color: rgb(173, 219, 103); font-style: italic; }

.hljs-deletion { color: rgba(239, 83, 80, 0.565); font-style: italic; }

@font-face { font-family: MonoLisa; font-weight: 100 900; font-display: swa=
p; font-style: normal; src: url("/fonts/MonoLisaNormal.woff2") format("woff=
2"); }

@font-face { font-family: Satoshi-Variable; src: url("/fonts/satoshi/Satosh=
i-Variable.woff2") format("woff2"), url("/fonts/satoshi/Satoshi-Variable.wo=
ff") format("woff"), url("/fonts/satoshi/Satoshi-Variable.ttf") format("tru=
etype"); font-weight: 300 900; font-display: swap; font-style: normal; }

@font-face { font-family: Satoshi-Variable; src: url("/fonts/satoshi/Satosh=
i-VariableItalic.woff2") format("woff2"), url("/fonts/satoshi/Satoshi-Varia=
bleItalic.woff") format("woff"), url("/fonts/satoshi/Satoshi-VariableItalic=
.ttf") format("truetype"); font-weight: 300 900; font-display: swap; font-s=
tyle: italic; }

@font-face { font-family: Satoshi; src: url("/fonts/satoshi/Satoshi-Light.w=
off2") format("woff2"), url("/fonts/satoshi/Satoshi-Light.woff") format("wo=
ff"), url("/fonts/satoshi/Satoshi-Light.ttf") format("truetype"); font-weig=
ht: 300; font-display: swap; font-style: normal; }

@font-face { font-family: Satoshi; src: url("/fonts/satoshi/Satoshi-LightIt=
alic.woff2") format("woff2"), url("/fonts/satoshi/Satoshi-LightItalic.woff"=
) format("woff"), url("/fonts/satoshi/Satoshi-LightItalic.ttf") format("tru=
etype"); font-weight: 300; font-display: swap; font-style: italic; }

@font-face { font-family: Satoshi; src: url("/fonts/satoshi/Satoshi-Regular=
.woff2") format("woff2"), url("/fonts/satoshi/Satoshi-Regular.woff") format=
("woff"), url("/fonts/satoshi/Satoshi-Regular.ttf") format("truetype"); fon=
t-weight: 400; font-display: swap; font-style: normal; }

@font-face { font-family: Satoshi; src: url("/fonts/satoshi/Satoshi-Italic.=
woff2") format("woff2"), url("/fonts/satoshi/Satoshi-Italic.woff") format("=
woff"), url("/fonts/satoshi/Satoshi-Italic.ttf") format("truetype"); font-w=
eight: 400; font-display: swap; font-style: italic; }

@font-face { font-family: Satoshi; src: url("/fonts/satoshi/Satoshi-Medium.=
woff2") format("woff2"), url("/fonts/satoshi/Satoshi-Medium.woff") format("=
woff"), url("/fonts/satoshi/Satoshi-Medium.ttf") format("truetype"); font-w=
eight: 500; font-display: swap; font-style: normal; }

@font-face { font-family: Satoshi; src: url("/fonts/satoshi/Satoshi-MediumI=
talic.woff2") format("woff2"), url("/fonts/satoshi/Satoshi-MediumItalic.wof=
f") format("woff"), url("/fonts/satoshi/Satoshi-MediumItalic.ttf") format("=
truetype"); font-weight: 500; font-display: swap; font-style: italic; }

@font-face { font-family: Satoshi; src: url("/fonts/satoshi/Satoshi-Bold.wo=
ff2") format("woff2"), url("/fonts/satoshi/Satoshi-Bold.woff") format("woff=
"), url("/fonts/satoshi/Satoshi-Bold.ttf") format("truetype"); font-weight:=
 700; font-display: swap; font-style: normal; }

@font-face { font-family: Satoshi; src: url("/fonts/satoshi/Satoshi-BoldIta=
lic.woff2") format("woff2"), url("/fonts/satoshi/Satoshi-BoldItalic.woff") =
format("woff"), url("/fonts/satoshi/Satoshi-BoldItalic.ttf") format("truety=
pe"); font-weight: 700; font-display: swap; font-style: italic; }

@font-face { font-family: Satoshi; src: url("/fonts/satoshi/Satoshi-Black.w=
off2") format("woff2"), url("/fonts/satoshi/Satoshi-Black.woff") format("wo=
ff"), url("/fonts/satoshi/Satoshi-Black.ttf") format("truetype"); font-weig=
ht: 900; font-display: swap; font-style: normal; }

@font-face { font-family: Satoshi; src: url("/fonts/satoshi/Satoshi-BlackIt=
alic.woff2") format("woff2"), url("/fonts/satoshi/Satoshi-BlackItalic.woff"=
) format("woff"), url("/fonts/satoshi/Satoshi-BlackItalic.ttf") format("tru=
etype"); font-weight: 900; font-display: swap; font-style: italic; }

img { max-width: none; }

:root { scroll-behavior: smooth; }

html { font-family: Satoshi, system-ui, -apple-system, BlinkMacSystemFont, =
"Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Appl=
e Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji"; }

@supports (font-variation-settings: normal) {
  html { font-family: Satoshi-Variable, system-ui, -apple-system, BlinkMacS=
ystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-s=
erif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color=
 Emoji"; }
}

body { -webkit-tap-highlight-color: transparent; touch-action: manipulation=
; text-rendering: optimizelegibility; text-size-adjust: 100%; --tw-bg-opaci=
ty: 1; background-color: rgb(249 249 250 / var(--tw-bg-opacity)); --tw-text=
-opacity: 1; color: rgb(15 23 42 / var(--tw-text-opacity)); -webkit-font-sm=
oothing: antialiased; }

body.dark { background-color: rgb(8, 11, 22); color: rgb(249, 249, 253); }

a { color: inherit; text-decoration: none; }

a.link { font-weight: 500; --tw-text-opacity: 1; color: rgb(139 92 246 / va=
r(--tw-text-opacity)); }

a.link:hover { --tw-text-opacity: 1; color: rgb(124 58 237 / var(--tw-text-=
opacity)); text-decoration-line: underline; }

a.link:focus { text-decoration-line: underline; }

b { font-weight: 600; }

strong { font-weight: 700; }

::selection { --tw-bg-opacity: 1; background-color: rgb(139 92 246 / var(--=
tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(255 255 255 / var(--tw-te=
xt-opacity)); }

.contain { margin-left: auto; margin-right: auto; width: 100%; max-width: 8=
0rem; padding-left: 1.5rem; padding-right: 1.5rem; }

.pad, .contain { padding-left: 1.5rem; padding-right: 1.5rem; }

@media (min-width: 640px) {
  .pad, .contain { padding-left: 2rem; padding-right: 2rem; }
}

@media (min-width: 768px) {
  .pad, .contain { padding-left: 2.5rem; padding-right: 2.5rem; }
}

.overflow-y-scroll { }

.route-enter-active-class { transition-duration: 0.3s; transition-timing-fu=
nction: cubic-bezier(0, 0, 0.2, 1); }

.route-enter-active-class .page-fade-in, .route-enter-active-class.page-fad=
e-in { transform: translate3d(var(--tw-translate-x),var(--tw-translate-y),0=
) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) s=
caleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); transition-duration: 0.=
3s; transition-timing-function: cubic-bezier(0, 0, 0.2, 1); }

.route-enter-from-class .page-fade-in, .route-enter-from-class.page-fade-in=
 { --tw-translate-y: -.25rem; transform: translate(var(--tw-translate-x),va=
r(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(=
var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); opac=
ity: 0; }

.route-enter-to-class .page-fade-in, .route-enter-to-class.page-fade-in { -=
-tw-translate-y: 0px; transform: translate(var(--tw-translate-x),var(--tw-t=
ranslate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw=
-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); opacity: 1; =
}

.button { border-radius: 0.75rem; border-width: 1px; border-color: transpar=
ent; font-weight: 600; transition-property: color, background-color, border=
-color, text-decoration-color, fill, stroke, opacity, box-shadow, transform=
, filter, backdrop-filter, -webkit-text-decoration-color, -webkit-box-shado=
w, -webkit-transform, -webkit-filter, -webkit-backdrop-filter; transition-t=
iming-function: cubic-bezier(0, 0, 0.2, 1); transition-duration: 0.15s; }

.button:focus { outline: transparent solid 2px; outline-offset: 2px; }

.button--gray { --tw-bg-opacity: 1; background-color: rgb(241 245 249 / var=
(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(71 85 105 / var(--tw-t=
ext-opacity)); }

:is(.dark .button--gray) { --tw-bg-opacity: 1; background-color: rgb(30 41 =
59 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(148 163 184 / =
var(--tw-text-opacity)); }

.button--gray:hover { --tw-bg-opacity: 1; background-color: rgb(226 232 240=
 / var(--tw-bg-opacity)); }

:is(.dark .button--gray:hover) { --tw-bg-opacity: 1; background-color: rgb(=
71 85 105 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(241 245=
 249 / var(--tw-text-opacity)); }

.button--gray:focus { --tw-border-opacity: 1; border-color: rgb(191 219 254=
 / var(--tw-border-opacity)); }

:is(.dark .button--gray:focus) { --tw-border-opacity: 1; border-color: rgb(=
49 130 206 / var(--tw-border-opacity)); }

.button--black { --tw-bg-opacity: 1; background-color: rgb(0 0 0 / var(--tw=
-bg-opacity)); --tw-text-opacity: 1; color: rgb(255 255 255 / var(--tw-text=
-opacity)); }

.button--black:hover { --tw-bg-opacity: 1; background-color: rgb(30 41 59 /=
 var(--tw-bg-opacity)); }

.button--black:focus { --tw-border-opacity: 1; border-color: rgb(30 41 59 /=
 var(--tw-border-opacity)); }

.button--slate { --tw-bg-opacity: 1; background-color: rgb(241 245 249 / va=
r(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(71 85 105 / var(--tw-=
text-opacity)); }

:is(.dark .button--slate) { --tw-bg-opacity: 1; background-color: rgb(51 65=
 85 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(148 163 184 /=
 var(--tw-text-opacity)); }

.button--slate:hover { --tw-bg-opacity: 1; background-color: rgb(226 232 24=
0 / var(--tw-bg-opacity)); }

:is(.dark .button--slate:hover) { --tw-bg-opacity: 1; background-color: rgb=
(71 85 105 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(241 24=
5 249 / var(--tw-text-opacity)); }

.button--slate:focus { --tw-border-opacity: 1; border-color: rgb(191 219 25=
4 / var(--tw-border-opacity)); }

:is(.dark .button--slate:focus) { --tw-border-opacity: 1; border-color: rgb=
(49 130 206 / var(--tw-border-opacity)); }

.button--purple { --tw-bg-opacity: 1; background-color: rgb(243 232 255 / v=
ar(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(147 51 234 / var(--t=
w-text-opacity)); }

:is(.dark .button--purple) { --tw-bg-opacity: 1; background-color: rgb(126 =
34 206 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(243 232 25=
5 / var(--tw-text-opacity)); }

.button--purple:hover { --tw-bg-opacity: 1; background-color: rgb(233 213 2=
55 / var(--tw-bg-opacity)); }

:is(.dark .button--purple:hover) { --tw-bg-opacity: 1; background-color: rg=
b(147 51 234 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(255 =
255 255 / var(--tw-text-opacity)); }

.button--purple:focus { --tw-border-opacity: 1; border-color: rgb(233 213 2=
55 / var(--tw-border-opacity)); }

:is(.dark .button--purple:focus) { --tw-border-opacity: 1; border-color: rg=
b(147 51 234 / var(--tw-border-opacity)); }

.button--orange { --tw-bg-opacity: 1; background-color: rgb(255 237 213 / v=
ar(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(234 88 12 / var(--tw=
-text-opacity)); }

:is(.dark .button--orange) { --tw-bg-opacity: 1; background-color: rgb(194 =
65 12 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(255 237 213=
 / var(--tw-text-opacity)); }

.button--orange:hover { --tw-bg-opacity: 1; background-color: rgb(254 215 1=
70 / var(--tw-bg-opacity)); }

:is(.dark .button--orange:hover) { --tw-bg-opacity: 1; background-color: rg=
b(234 88 12 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(255 2=
55 255 / var(--tw-text-opacity)); }

.button--orange:focus { --tw-border-opacity: 1; border-color: rgb(254 215 1=
70 / var(--tw-border-opacity)); }

:is(.dark .button--orange:focus) { --tw-border-opacity: 1; border-color: rg=
b(234 88 12 / var(--tw-border-opacity)); }

.button--blue { --tw-bg-opacity: 1; background-color: rgb(235 248 255 / var=
(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(49 130 206 / var(--tw-=
text-opacity)); }

:is(.dark .button--blue) { --tw-bg-opacity: 1; background-color: rgb(43 108=
 176 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(235 248 255 =
/ var(--tw-text-opacity)); }

.button--blue:hover { --tw-bg-opacity: 1; background-color: rgb(191 219 254=
 / var(--tw-bg-opacity)); }

:is(.dark .button--blue:hover) { --tw-bg-opacity: 1; background-color: rgb(=
49 130 206 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(255 25=
5 255 / var(--tw-text-opacity)); }

.button--blue:focus { --tw-border-opacity: 1; border-color: rgb(191 219 254=
 / var(--tw-border-opacity)); }

:is(.dark .button--blue:focus) { --tw-border-opacity: 1; border-color: rgb(=
49 130 206 / var(--tw-border-opacity)); }

.button-purple { display: inline-flex; cursor: pointer; user-select: none; =
-webkit-box-align: center; align-items: center; border-width: 1px; border-c=
olor: transparent; text-align: center; font-weight: 700; transition-propert=
y: color, background-color, border-color, text-decoration-color, fill, stro=
ke, opacity, box-shadow, transform, filter, backdrop-filter, -webkit-text-d=
ecoration-color, -webkit-box-shadow, -webkit-transform, -webkit-filter, -we=
bkit-backdrop-filter; transition-timing-function: cubic-bezier(0, 0, 0.2, 1=
); transition-duration: 0.15s; --tw-bg-opacity: 1; background-color: rgb(24=
3 232 255 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(168 85 =
247 / var(--tw-text-opacity)); }

.button-purple:hover { --tw-bg-opacity: 1; background-color: rgb(233 213 25=
5 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(147 51 234 / va=
r(--tw-text-opacity)); }

.button-purple:active { --tw-bg-opacity: 1; background-color: rgb(216 180 2=
54 / var(--tw-bg-opacity)); }

:is(.dark .button-purple) { --tw-border-opacity: 1; border-color: rgb(168 8=
5 247 / var(--tw-border-opacity)); background-color: rgba(168, 85, 247, 0.5=
); --tw-text-opacity: 1; color: rgb(250 245 255 / var(--tw-text-opacity)); =
}

:is(.dark .button-purple:hover) { background-color: rgba(192, 132, 252, 0.5=
); --tw-text-opacity: 1; color: rgb(255 255 255 / var(--tw-text-opacity)); =
}

:is(.dark .button-purple:active) { background-color: rgba(147, 51, 234, 0.5=
); }

.button-blue { display: inline-flex; cursor: pointer; user-select: none; -w=
ebkit-box-align: center; align-items: center; border-width: 1px; border-col=
or: transparent; text-align: center; font-weight: 700; transition-property:=
 color, background-color, border-color, text-decoration-color, fill, stroke=
, opacity, box-shadow, transform, filter, backdrop-filter, -webkit-text-dec=
oration-color, -webkit-box-shadow, -webkit-transform, -webkit-filter, -webk=
it-backdrop-filter; transition-timing-function: cubic-bezier(0, 0, 0.2, 1);=
 transition-duration: 0.15s; --tw-bg-opacity: 1; background-color: rgb(235 =
248 255 / var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(65 153 22=
5 / var(--tw-text-opacity)); }

.button-blue:hover { --tw-bg-opacity: 1; background-color: rgb(191 219 254 =
/ var(--tw-bg-opacity)); --tw-text-opacity: 1; color: rgb(49 130 206 / var(=
--tw-text-opacity)); }

.button-blue:active { --tw-bg-opacity: 1; background-color: rgb(147 197 253=
 / var(--tw-bg-opacity)); }

:is(.dark .button-blue) { --tw-border-opacity: 1; border-color: rgb(65 153 =
225 / var(--tw-border-opacity)); background-color: rgba(65, 153, 225, 0.5);=
 --tw-text-opacity: 1; color: rgb(239 246 255 / var(--tw-text-opacity)); }

:is(.dark .button-blue:hover) { background-color: rgba(99, 179, 237, 0.5); =
--tw-text-opacity: 1; color: rgb(255 255 255 / var(--tw-text-opacity)); }

:is(.dark .button-blue:active) { background-color: rgba(49, 130, 206, 0.5);=
 }

.button-lg { border-radius: 0.75rem; padding: 1rem 2rem; }

.font-num { font-family: MonoLisa, ui-monospace, SFMono-Regular, Menlo, Mon=
aco, Consolas, "Liberation Mono", "Courier New", monospace; --tw-slashed-ze=
ro: slashed-zero; --tw-numeric-spacing: tabular-nums; font-variant-numeric:=
 var(--tw-ordinal) var(--tw-slashed-zero) var(--tw-numeric-figure) var(--tw=
-numeric-spacing) var(--tw-numeric-fraction); }

.prevent-scroll { width: 100%; overflow-y: hidden; position: fixed !importa=
nt; }

.tbody { display: table-row-group; }

.tr { display: table-row; }

.td, .th { display: table-cell; }

.thead { display: table-header-group; }

.tfoot { display: table-footer-group; }

.button-nav { border-width: 1px; transition-property: color, background-col=
or, border-color, text-decoration-color, fill, stroke, opacity, box-shadow,=
 transform, filter, backdrop-filter, -webkit-text-decoration-color, -webkit=
-box-shadow, -webkit-transform, -webkit-filter, -webkit-backdrop-filter; tr=
ansition-timing-function: cubic-bezier(0, 0, 0.2, 1); transition-duration: =
0.15s; border-color: rgb(100 116 139 / var(--tw-border-opacity)); --tw-bord=
er-opacity: 0; background-color: rgb(226 232 240 / var(--tw-bg-opacity)); -=
-tw-bg-opacity: 0; --tw-text-opacity: 1; color: rgb(100 116 139 / var(--tw-=
text-opacity)); }

.button-nav:hover { --tw-border-opacity: .1; --tw-bg-opacity: .5; --tw-text=
-opacity: 1; color: rgb(71 85 105 / var(--tw-text-opacity)); }

.dark .button-nav { border-color: rgb(148 163 184 / var(--tw-border-opacity=
)); --tw-border-opacity: 0; background-color: rgb(100 116 139 / var(--tw-bg=
-opacity)); --tw-bg-opacity: 0; --tw-text-opacity: 1; color: rgb(148 163 18=
4 / var(--tw-text-opacity)); }

.dark .button-nav:hover { --tw-border-opacity: .1; --tw-bg-opacity: .2; --t=
w-text-opacity: 1; color: rgb(203 213 225 / var(--tw-text-opacity)); }

.fade-enter-active, .fade-leave-active { transition: opacity 0.5s cubic-bez=
ier(0.175, 0.885, 0.32, 1.275) 0s; }

.fade-enter, .fade-leave-to { opacity: 0; }

.notion-image-inset { position: absolute; inset: 0px; margin: 0px !importan=
t; }

.notion-h2 span { font-weight: 700; }

.notion-toggle > div { margin-left: 1.25rem; }

.notion-asset-wrapper { display: flex; -webkit-box-orient: vertical; -webki=
t-box-direction: normal; flex-direction: column; -webkit-box-align: center;=
 align-items: center; }

.notion-asset-wrapper > div { height: 100%; width: 100%; }

.notion-asset-wrapper > div > iframe { height: 100%; width: 100%; }

.notion img { width: 100%; }

.twitter-tweet { margin-left: auto; margin-right: auto; }

@media (pointer: fine) {
  .miniScrollbar::-webkit-scrollbar { height: 18px; width: 18px; }
  .miniScrollbar::-webkit-scrollbar-track { background-color: transparent; =
}
  .miniScrollbar::-webkit-scrollbar-thumb { background-clip: content-box; b=
ackground-color: rgba(51, 65, 85, 0.7); border: 6px solid transparent; bord=
er-radius: 20px; min-height: 60px; }
  .miniScrollbar::-webkit-scrollbar-thumb:hover { background-color: rgb(168=
, 187, 191); }
  html::-webkit-scrollbar { height: 18px; width: 18px; }
  html::-webkit-scrollbar-track { background-color: transparent; }
  html::-webkit-scrollbar-thumb { background-clip: content-box; background-=
color: rgba(51, 65, 85, 0.7); border: 6px solid transparent; border-radius:=
 20px; min-height: 60px; }
  html::-webkit-scrollbar-thumb:hover { background-color: rgb(168, 187, 191=
); }
  .microScrollbar::-webkit-scrollbar { height: 15px; width: 15px; }
  .microScrollbar::-webkit-scrollbar-track { background-color: transparent;=
 }
  .microScrollbar::-webkit-scrollbar-thumb { background-clip: content-box; =
background-color: rgba(51, 65, 85, 0.7); border: 6px solid transparent; bor=
der-radius: 10px; min-height: 60px; }
  .microScrollbar::-webkit-scrollbar-thumb:hover { background-color: rgb(16=
8, 187, 191); }
}

.scroll-container { will-change: transform; }

.noScrollbar { overflow-y: overlay; }

.noScrollbar::-webkit-scrollbar { width: 20px; }

.noScrollbar::-webkit-scrollbar-track { background-color: transparent; }

.noScrollbar::-webkit-scrollbar-thumb { background-clip: content-box; backg=
round-color: transparent; }

body { overflow-y: overlay; }

.chat-content { --tw-text-opacity: 1; color: rgb(209 213 219 / var(--tw-tex=
t-opacity)); }

.chat-content :where(p) { margin-top: 1rem; margin-bottom: 1rem; }

.chat-content :where(h1) { margin-top: 0px; margin-bottom: 1rem; font-size:=
 1.125rem; line-height: 1.75rem; font-weight: 900; --tw-text-opacity: 1; co=
lor: rgb(255 255 255 / var(--tw-text-opacity)); }

.chat-content :where(h2) { margin-top: 0px; margin-bottom: 1rem; font-size:=
 1.125rem; line-height: 1.75rem; font-weight: 700; color: rgba(255, 255, 25=
5, 0.9); }

.chat-content :where(h3, h4, h5, h6) { margin-top: 0px; margin-bottom: 1rem=
; font-size: 1rem; line-height: 1.5rem; font-weight: 700; color: rgba(255, =
255, 255, 0.8); }

.chat-content :where(strong) { font-weight: 600; --tw-text-opacity: 1; colo=
r: rgb(255 255 255 / var(--tw-text-opacity)); }

.chat-content :where(ul) { margin-top: 1rem; margin-bottom: 1rem; list-styl=
e-type: disc; padding-left: 1.5rem; }

.chat-content :where(ul > li) { padding-left: 0.25rem; }

.chat-content :where(li) { margin-top: 0.5rem; margin-bottom: 0.5rem; }

.chat-content :where(ol) { margin-top: 1rem; margin-bottom: 1rem; list-styl=
e-type: decimal; padding-left: 1.5rem; }

.chat-content :where(ol > li) { padding-left: 0.25rem; }

.chat-content :where(a) { font-weight: 500; --tw-text-opacity: 1; color: rg=
b(88 202 247 / var(--tw-text-opacity)); }

.chat-content :where(a):hover { text-decoration-line: underline; }

.chat-content :where(img) { margin-top: 2rem; margin-bottom: 2rem; max-widt=
h: 100%; }

.chat-content :where(blockquote) { margin-top: 1rem; margin-bottom: 1rem; b=
order-left-width: 2px; --tw-border-opacity: 1; border-color: rgb(113 114 11=
7 / var(--tw-border-opacity)); padding-left: 1rem; font-weight: 500; font-s=
tyle: italic; --tw-text-opacity: 1; color: rgb(243 244 246 / var(--tw-text-=
opacity)); quotes: "=E2=80=9C" "=E2=80=9D" "=E2=80=98" "=E2=80=99"; }

.chat-content :where(blockquote p:first-of-type)::before { content: open-qu=
ote; }

.chat-content :where(blockquote p:last-of-type)::after { content: close-quo=
te; }

.chat-content :where(table) { width: 100%; table-layout: auto; text-align: =
left; margin-top: 2em; margin-bottom: 2em; font-size: 0.875em; line-height:=
 1.71429; }

.chat-content :where(table thead) { border-bottom-width: 1px; --tw-border-o=
pacity: 1; border-bottom-color: rgb(75 85 99 / var(--tw-border-opacity)); }

.chat-content :where(table th) { padding-right: 1rem; padding-bottom: 1rem;=
 padding-left: 1rem; vertical-align: bottom; font-weight: 600; --tw-text-op=
acity: 1; color: rgb(255 255 255 / var(--tw-text-opacity)); }

.chat-content :where(table th:first-child) { padding-left: 0px; }

.chat-content :where(table th:last-child) { padding-right: 0px; }

.chat-content :where(tbody tr) { border-bottom-width: 1px; --tw-border-opac=
ity: 1; border-bottom-color: rgb(75 85 99 / var(--tw-border-opacity)); }

.chat-content :where(tbody td, tfoot td) { padding-right: 1rem; padding-bot=
tom: 1rem; padding-left: 1rem; }

.chat-content :where(tbody td) { vertical-align: baseline; }

.chat-content :where(tbody tr:last-child) { border-bottom-width: 0px; }

.chat-content :where(tbody td:first-child, tfoot td:first-child) { padding-=
left: 0px; }

.chat-content :where(.chat-content > :last-child) { margin-bottom: 0px; }

.chat-content :where(.chat-content > :first-child) { margin-top: 0px; }

.chat-content :where(code) { white-space: pre-wrap; border-radius: 0.25rem;=
 background-color: rgba(30, 41, 59, 0.5); padding: 1px 0.25rem; font-family=
: MonoLisa, ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberat=
ion Mono", "Courier New", monospace; font-size: 0.75rem; line-height: 1rem;=
 font-weight: 700; --tw-text-opacity: 1; color: rgb(36 228 142 / var(--tw-t=
ext-opacity)); }

.chat-content :where(pre code) { border-width: 1px; border-color: rgba(51, =
65, 85, 0.5); }

pre code.hljs { padding: 1rem; --tw-text-opacity: 1; color: rgb(209 213 219=
 / var(--tw-text-opacity)); }

.animate-cursor-blink { animation: 1s ease 0s infinite normal none running =
cursor-blink; }

@-webkit-keyframes cursor-blink {=20
  0% { opacity: 1; }
  40% { opacity: 1; }
  50% { opacity: 0; }
  90% { opacity: 0; }
  100% { opacity: 1; }
}

@keyframes cursor-blink {=20
  0% { opacity: 1; }
  40% { opacity: 1; }
  50% { opacity: 0; }
  90% { opacity: 0; }
  100% { opacity: 1; }
}

.chat-content-busy > p:last-child::after, .chat-content-busy > ol:last-chil=
d > li:last-child > p:last-child::after, .chat-content-busy > ul:last-child=
 > li:last-child > p:last-child::after, .chat-content-busy > h1:last-child:=
:after, .chat-content-busy > h2:last-child::after, .chat-content-busy > h3:=
last-child::after, .chat-content-busy > h4:last-child::after, .chat-content=
-busy > h5:last-child::after, .chat-content-busy > pre:last-child > code:la=
st-child::after { --tw-text-opacity: 1; color: rgb(249 187 73 / var(--tw-te=
xt-opacity)); animation: 1s ease 0s infinite normal none running cursor-bli=
nk; content: " =E2=96=8C"; }

.chat-content .header-anchor { padding-right: 8px; margin-left: -20px; colo=
r: inherit; float: left; display: none; opacity: 0; transition-property: co=
lor, background-color, border-color, text-decoration-color, fill, stroke, o=
pacity, box-shadow, transform, filter, backdrop-filter, -webkit-text-decora=
tion-color, -webkit-box-shadow, -webkit-transform, -webkit-filter, -webkit-=
backdrop-filter; transition-timing-function: cubic-bezier(0, 0, 0.2, 1); tr=
ansition-duration: 0.15s; }

.chat-content .header-anchor:hover { text-decoration-line: none; }

@media (min-width: 640px) {
  .chat-content .header-anchor { display: block; }
}

.chat-content :hover .header-anchor { opacity: 0.5; }

.hoverUpSmall { --tw-scale-x: 99.99999%; --tw-scale-y: 99.99999%; transform=
: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-ro=
tate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale=
-x)) scaleY(var(--tw-scale-y)); }

@media (hover: hover) and (pointer: fine) {
  .hoverUpSmall:hover { --tw-translate-y: -2px; transform: translate(var(--=
tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--=
tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--=
tw-scale-y)); }
}

.hoverUpSmall:active { --tw-translate-y: -0px; transform: translate(var(--t=
w-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--t=
w-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--t=
w-scale-y)); transition-duration: 75ms; }

.hoverUpSmall { --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; box=
-shadow: var(--tw-ring-offset-shadow, 0 0 #0000),var(--tw-ring-shadow, 0 0 =
#0000),var(--tw-shadow); }

@media (hover: hover) and (pointer: fine) {
  .hoverUpSmall:hover { --tw-shadow: 0 10px 15px -3px rgba(0, 0, 0, .1), 0 =
4px 6px -4px rgba(0, 0, 0, .1); --tw-shadow-colored: 0 10px 15px -3px var(-=
-tw-shadow-color), 0 4px 6px -4px var(--tw-shadow-color); box-shadow: var(-=
-tw-ring-offset-shadow, 0 0 #0000),var(--tw-ring-shadow, 0 0 #0000),var(--t=
w-shadow); }
}

.hoverUpSmall:active { --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #00=
00; box-shadow: var(--tw-ring-offset-shadow, 0 0 #0000),var(--tw-ring-shado=
w, 0 0 #0000),var(--tw-shadow); }

@media (hover: hover) and (pointer: fine) {
  .hoverUpSmall:hover { --tw-shadow-color: rgba(0, 0, 0, .5); --tw-shadow: =
var(--tw-shadow-colored); }
}

.hoverUpSmall { backface-visibility: hidden; transition-duration: 0.15s; tr=
ansition-property: color, background-color, border-color, text-decoration-c=
olor, fill, stroke, opacity, box-shadow, transform, filter, backdrop-filter=
, -webkit-backdrop-filter; transition-timing-function: cubic-bezier(0.4, 0,=
 0.2, 1); }

@media (color-gamut: p3) {
  svg path[fill=3D"#50c18e"] { fill: oklch(0.7322 0.214 161.1); }
  svg path[fill=3D"#f9bb49"] { fill: oklch(0.8294 0.194 79.26); }
  svg path[fill=3D"#e92e42"] { fill: oklch(0.6087 0.274 21.9); }
  svg path[fill=3D"#58caf7"] { fill: oklch(0.7912 0.148 227.38); }
}

.pointer-events-none { pointer-events: none; }

.pointer-events-auto { pointer-events: auto; }

.visible { visibility: visible; }

.invisible { visibility: hidden; }

.\!collapse { visibility: collapse !important; }

.collapse { visibility: collapse; }

.fixed { position: fixed; }

.absolute { position: absolute; }

.relative { position: relative; }

.sticky { position: sticky; }

.inset-0 { inset: 0px; }

.inset-x-0 { left: 0px; right: 0px; }

.inset-x-2 { left: 0.5rem; right: 0.5rem; }

.inset-y-0 { top: 0px; bottom: 0px; }

.-left-full { left: -100%; }

.bottom-0 { bottom: 0px; }

.bottom-2 { bottom: 0.5rem; }

.bottom-4 { bottom: 1rem; }

.left-0 { left: 0px; }

.left-1\/2 { left: 50%; }

.right-0 { right: 0px; }

.right-4 { right: 1rem; }

.top-0 { top: 0px; }

.top-full { top: 100%; }

.-z-10 { z-index: -10; }

.-z-50 { z-index: -50; }

.z-0 { z-index: 0; }

.z-10 { z-index: 10; }

.z-20 { z-index: 20; }

.z-40 { z-index: 40; }

.z-50 { z-index: 50; }

.z-\[1000000000000\] { z-index: 2147483647; }

.z-\[1000000\] { z-index: 1000000; }

.z-\[100\] { z-index: 100; }

.z-\[49\] { z-index: 49; }

.z-\[50000000\] { z-index: 50000000; }

.col-span-1 { grid-column: span 1 / span 1; }

.col-span-full { grid-column: 1 / -1; }

.m-0 { margin: 0px; }

.m-auto { margin: auto; }

.-mx-2 { margin-left: -0.5rem; margin-right: -0.5rem; }

.-mx-3 { margin-left: -0.75rem; margin-right: -0.75rem; }

.-mx-6 { margin-left: -1.5rem; margin-right: -1.5rem; }

.-my-0 { margin-top: 0px; margin-bottom: 0px; }

.-my-0\.5 { margin-top: -0.125rem; margin-bottom: -0.125rem; }

.mx-1 { margin-left: 0.25rem; margin-right: 0.25rem; }

.mx-3 { margin-left: 0.75rem; margin-right: 0.75rem; }

.mx-6 { margin-left: 1.5rem; margin-right: 1.5rem; }

.mx-auto { margin-left: auto; margin-right: auto; }

.mx-px { margin-left: 1px; margin-right: 1px; }

.my-0 { margin-top: 0px; margin-bottom: 0px; }

.my-0\.5 { margin-top: 0.125rem; margin-bottom: 0.125rem; }

.my-1 { margin-top: 0.25rem; margin-bottom: 0.25rem; }

.my-2 { margin-top: 0.5rem; margin-bottom: 0.5rem; }

.my-3 { margin-top: 0.75rem; margin-bottom: 0.75rem; }

.my-4 { margin-top: 1rem; margin-bottom: 1rem; }

.my-8 { margin-top: 2rem; margin-bottom: 2rem; }

.-ml-1 { margin-left: -0.25rem; }

.-ml-3 { margin-left: -0.75rem; }

.-ml-4 { margin-left: -1rem; }

.-mt-1 { margin-top: -0.25rem; }

.-mt-1\.5 { margin-top: -0.375rem; }

.-mt-16 { margin-top: -4rem; }

.-mt-24 { margin-top: -6rem; }

.-mt-4 { margin-top: -1rem; }

.mb-1 { margin-bottom: 0.25rem; }

.mb-14 { margin-bottom: 3.5rem; }

.mb-16 { margin-bottom: 4rem; }

.mb-2 { margin-bottom: 0.5rem; }

.mb-4 { margin-bottom: 1rem; }

.mb-6 { margin-bottom: 1.5rem; }

.ml-1 { margin-left: 0.25rem; }

.ml-2 { margin-left: 0.5rem; }

.ml-3 { margin-left: 0.75rem; }

.ml-4 { margin-left: 1rem; }

.ml-5 { margin-left: 1.25rem; }

.ml-auto { margin-left: auto; }

.mr-1 { margin-right: 0.25rem; }

.mr-2 { margin-right: 0.5rem; }

.mr-3 { margin-right: 0.75rem; }

.mr-4 { margin-right: 1rem; }

.mr-6 { margin-right: 1.5rem; }

.mr-auto { margin-right: auto; }

.mt-1 { margin-top: 0.25rem; }

.mt-12 { margin-top: 3rem; }

.mt-2 { margin-top: 0.5rem; }

.mt-3 { margin-top: 0.75rem; }

.mt-4 { margin-top: 1rem; }

.mt-5 { margin-top: 1.25rem; }

.mt-6 { margin-top: 1.5rem; }

.mt-64 { margin-top: 16rem; }

.mt-8 { margin-top: 2rem; }

.block { display: block; }

.inline-block { display: inline-block; }

.inline { display: inline; }

.flex { display: flex; }

.inline-flex { display: inline-flex; }

.table { display: table; }

.grid { display: grid; }

.hidden { display: none; }

.h-0 { height: 0px; }

.h-1 { height: 0.25rem; }

.h-10 { height: 2.5rem; }

.h-11 { height: 2.75rem; }

.h-12 { height: 3rem; }

.h-16 { height: 4rem; }

.h-2 { height: 0.5rem; }

.h-3 { height: 0.75rem; }

.h-3\.5 { height: 0.875rem; }

.h-32 { height: 8rem; }

.h-4 { height: 1rem; }

.h-5 { height: 1.25rem; }

.h-52 { height: 13rem; }

.h-6 { height: 1.5rem; }

.h-64 { height: 16rem; }

.h-7 { height: 1.75rem; }

.h-8 { height: 2rem; }

.h-\[400px\] { height: 400px; }

.h-auto { height: auto; }

.h-full { height: 100%; }

.h-px { height: 1px; }

.h-screen { height: 100vh; }

.max-h-\[60vh\] { max-height: 60vh; }

.max-h-screen { max-height: 100vh; }

.min-h-\[3rem\] { min-height: 3rem; }

.min-h-full { min-height: 100%; }

.min-h-screen { min-height: 100vh; }

.w-0 { width: 0px; }

.w-1 { width: 0.25rem; }

.w-1\/2 { width: 50%; }

.w-10 { width: 2.5rem; }

.w-12 { width: 3rem; }

.w-2 { width: 0.5rem; }

.w-3 { width: 0.75rem; }

.w-3\.5 { width: 0.875rem; }

.w-32 { width: 8rem; }

.w-4 { width: 1rem; }

.w-5 { width: 1.25rem; }

.w-6 { width: 1.5rem; }

.w-64 { width: 16rem; }

.w-7 { width: 1.75rem; }

.w-8 { width: 2rem; }

.w-\[256px\] { width: 256px; }

.w-auto { width: auto; }

.w-full { width: 100%; }

.w-px { width: 1px; }

.min-w-\[14px\] { min-width: 14px; }

.min-w-\[16px\] { min-width: 16px; }

.min-w-\[20px\] { min-width: 20px; }

.min-w-\[24px\] { min-width: 24px; }

.max-w-lg { max-width: 32rem; }

.max-w-screen-sm { max-width: 640px; }

.max-w-sm { max-width: 24rem; }

.max-w-xl { max-width: 36rem; }

.max-w-xs { max-width: 20rem; }

.flex-1 { -webkit-box-flex: 1; flex: 1 1 0%; }

.flex-shrink-0 { flex-shrink: 0; }

.shrink { flex-shrink: 1; }

.shrink-0 { flex-shrink: 0; }

.grow { -webkit-box-flex: 1; flex-grow: 1; }

.origin-center { transform-origin: center center; }

.origin-top-left { transform-origin: left top; }

.-translate-x-1\/2 { --tw-translate-x: -50%; transform: translate(var(--tw-=
translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-=
skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-=
scale-y)); }

.-translate-x-\[256px\] { --tw-translate-x: -256px; transform: translate(va=
r(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(va=
r(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(va=
r(--tw-scale-y)); }

.-translate-y-4 { --tw-translate-y: -1rem; transform: translate(var(--tw-tr=
anslate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-sk=
ew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-sc=
ale-y)); }

.translate-x-0 { --tw-translate-x: 0px; transform: translate(var(--tw-trans=
late-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-=
x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale=
-y)); }

.translate-x-16 { --tw-translate-x: 4rem; transform: translate(var(--tw-tra=
nslate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-ske=
w-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-sca=
le-y)); }

.translate-y-0 { --tw-translate-y: 0px; transform: translate(var(--tw-trans=
late-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-=
x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale=
-y)); }

.translate-y-2 { --tw-translate-y: .5rem; transform: translate(var(--tw-tra=
nslate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-ske=
w-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-sca=
le-y)); }

.translate-y-4 { --tw-translate-y: 1rem; transform: translate(var(--tw-tran=
slate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew=
-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scal=
e-y)); }

.-rotate-90 { --tw-rotate: -90deg; transform: translate(var(--tw-translate-=
x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) s=
kewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));=
 }

.rotate-180 { --tw-rotate: 180deg; transform: translate(var(--tw-translate-=
x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) s=
kewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));=
 }

.scale-0 { --tw-scale-x: 0; --tw-scale-y: 0; transform: translate(var(--tw-=
translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-=
skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-=
scale-y)); }

.scale-100 { --tw-scale-x: 1; --tw-scale-y: 1; transform: translate(var(--t=
w-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--t=
w-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--t=
w-scale-y)); }

.scale-105 { --tw-scale-x: 1.05; --tw-scale-y: 1.05; transform: translate(v=
ar(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(v=
ar(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(v=
ar(--tw-scale-y)); }

.scale-75 { --tw-scale-x: .75; --tw-scale-y: .75; transform: translate(var(=
--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(=
--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(=
--tw-scale-y)); }

.scale-95 { --tw-scale-x: .95; --tw-scale-y: .95; transform: translate(var(=
--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(=
--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(=
--tw-scale-y)); }

.-scale-x-100 { --tw-scale-x: -1; transform: translate(var(--tw-translate-x=
),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) sk=
ewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); =
}

.transform { transform: translate(var(--tw-translate-x),var(--tw-translate-=
y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y))=
 scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.transform-gpu { transform: translate3d(var(--tw-translate-x),var(--tw-tran=
slate-y),0) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-=
skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

@-webkit-keyframes ping {=20
  75%, 100% { transform: scale(2); opacity: 0; }
}

.animate-\[ping_1\.5s_ease-out_infinite\] { animation: 1.5s ease-out 0s inf=
inite normal none running ping; }

.animate-ping { animation: 1s cubic-bezier(0, 0, 0.2, 1) 0s infinite normal=
 none running ping; }

@-webkit-keyframes pulse {=20
  50% { opacity: 0.5; }
}

@keyframes pulse {=20
  50% { opacity: 0.5; }
}

.animate-pulse { animation: 2s cubic-bezier(0.4, 0, 0.6, 1) 0s infinite nor=
mal none running pulse; }

@-webkit-keyframes spin {=20
  100% { transform: rotate(360deg); }
}

@keyframes spin {=20
  100% { transform: rotate(360deg); }
}

.animate-spin { animation: 1s linear 0s infinite normal none running spin; =
}

.cursor-default { cursor: default; }

.cursor-pointer { cursor: pointer; }

.cursor-zoom-in { cursor: zoom-in; }

.cursor-zoom-out { cursor: zoom-out; }

.select-none { user-select: none; }

.resize-none { resize: none; }

.appearance-none { appearance: none; }

.grid-cols-1 { grid-template-columns: repeat(1, minmax(0px, 1fr)); }

.grid-cols-\[repeat\(auto-fit\,minmax\(350px\,1fr\)\)\] { grid-template-col=
umns: repeat(auto-fit, minmax(350px, 1fr)); }

.flex-col { -webkit-box-orient: vertical; -webkit-box-direction: normal; fl=
ex-direction: column; }

.flex-col-reverse { -webkit-box-orient: vertical; -webkit-box-direction: re=
verse; flex-direction: column-reverse; }

.flex-wrap { flex-wrap: wrap; }

.items-start { -webkit-box-align: start; align-items: flex-start; }

.items-end { -webkit-box-align: end; align-items: flex-end; }

.items-center { -webkit-box-align: center; align-items: center; }

.items-stretch { -webkit-box-align: stretch; align-items: stretch; }

.justify-start { -webkit-box-pack: start; justify-content: flex-start; }

.justify-end { -webkit-box-pack: end; justify-content: flex-end; }

.justify-center { -webkit-box-pack: center; justify-content: center; }

.justify-between { -webkit-box-pack: justify; justify-content: space-betwee=
n; }

.gap-1 { gap: 0.25rem; }

.gap-12 { gap: 3rem; }

.gap-2 { gap: 0.5rem; }

.gap-3 { gap: 0.75rem; }

.gap-4 { gap: 1rem; }

.gap-6 { gap: 1.5rem; }

.gap-x-2 { column-gap: 0.5rem; }

.gap-x-4 { column-gap: 1rem; }

.space-x-0 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse: 0; mar=
gin-right: calc(0px * var(--tw-space-x-reverse)); margin-left: calc(0px * c=
alc(1 - var(--tw-space-x-reverse))); }

.space-x-0\.5 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse: 0; =
margin-right: calc(.125rem * var(--tw-space-x-reverse)); margin-left: calc(=
.125rem * calc(1 - var(--tw-space-x-reverse))); }

.space-x-2 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse: 0; mar=
gin-right: calc(.5rem * var(--tw-space-x-reverse)); margin-left: calc(.5rem=
 * calc(1 - var(--tw-space-x-reverse))); }

.space-x-4 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse: 0; mar=
gin-right: calc(1rem * var(--tw-space-x-reverse)); margin-left: calc(1rem *=
 calc(1 - var(--tw-space-x-reverse))); }

.space-y-0 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; mar=
gin-top: calc(0px * calc(1 - var(--tw-space-y-reverse))); margin-bottom: ca=
lc(0px * var(--tw-space-y-reverse)); }

.space-y-0\.5 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; =
margin-top: calc(.125rem * calc(1 - var(--tw-space-y-reverse))); margin-bot=
tom: calc(.125rem * var(--tw-space-y-reverse)); }

.space-y-1 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; mar=
gin-top: calc(.25rem * calc(1 - var(--tw-space-y-reverse))); margin-bottom:=
 calc(.25rem * var(--tw-space-y-reverse)); }

.space-y-1\.5 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; =
margin-top: calc(.375rem * calc(1 - var(--tw-space-y-reverse))); margin-bot=
tom: calc(.375rem * var(--tw-space-y-reverse)); }

.space-y-2 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; mar=
gin-top: calc(.5rem * calc(1 - var(--tw-space-y-reverse))); margin-bottom: =
calc(.5rem * var(--tw-space-y-reverse)); }

.space-y-3 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; mar=
gin-top: calc(.75rem * calc(1 - var(--tw-space-y-reverse))); margin-bottom:=
 calc(.75rem * var(--tw-space-y-reverse)); }

.space-y-6 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; mar=
gin-top: calc(1.5rem * calc(1 - var(--tw-space-y-reverse))); margin-bottom:=
 calc(1.5rem * var(--tw-space-y-reverse)); }

.divide-y > :not([hidden]) ~ :not([hidden]) { --tw-divide-y-reverse: 0; bor=
der-top-width: calc(1px * calc(1 - var(--tw-divide-y-reverse))); border-bot=
tom-width: calc(1px * var(--tw-divide-y-reverse)); }

.divide-slate-800 > :not([hidden]) ~ :not([hidden]) { --tw-divide-opacity: =
1; border-color: rgb(30 41 59 / var(--tw-divide-opacity)); }

.divide-white\/5 > :not([hidden]) ~ :not([hidden]) { border-color: rgba(255=
, 255, 255, 0.05); }

.overflow-auto { overflow: auto; }

.overflow-hidden { overflow: hidden; }

.overflow-visible { overflow: visible; }

.overflow-y-auto { overflow-y: auto; }

.overflow-x-scroll { overflow-x: scroll; }

.overscroll-none { overscroll-behavior: none; }

.truncate { overflow: hidden; text-overflow: ellipsis; white-space: nowrap;=
 }

.whitespace-nowrap { white-space: nowrap; }

.whitespace-pre { white-space: pre; }

.break-words { overflow-wrap: break-word; }

.rounded { border-radius: 0.25rem; }

.rounded-2xl { border-radius: 1rem; }

.rounded-3xl { border-radius: 1.5rem; }

.rounded-full { border-radius: 9999px; }

.rounded-lg { border-radius: 0.5rem; }

.rounded-md { border-radius: 0.375rem; }

.rounded-xl { border-radius: 0.75rem; }

.rounded-t-2xl { border-top-left-radius: 1rem; border-top-right-radius: 1re=
m; }

.\!border { border-width: 1px !important; }

.border { border-width: 1px; }

.border-2 { border-width: 2px; }

.border-b { border-bottom-width: 1px; }

.border-b-2 { border-bottom-width: 2px; }

.border-r { border-right-width: 1px; }

.border-r-2 { border-right-width: 2px; }

.border-t { border-top-width: 1px; }

.border-dashed { border-style: dashed; }

.border-none { border-style: none; }

.\!border-transparent { border-color: transparent !important; }

.border-\[\#50c18e\]\/90 { border-color: rgba(80, 193, 142, 0.9); }

.border-blue-400 { --tw-border-opacity: 1; border-color: rgb(99 179 237 / v=
ar(--tw-border-opacity)); }

.border-blue-400\/50 { border-color: rgba(99, 179, 237, 0.5); }

.border-blue-600 { --tw-border-opacity: 1; border-color: rgb(49 130 206 / v=
ar(--tw-border-opacity)); }

.border-blue-900\/30 { border-color: rgba(42, 67, 101, 0.3); }

.border-darkBlue-400 { --tw-border-opacity: 1; border-color: rgb(96 165 250=
 / var(--tw-border-opacity)); }

.border-darkBlue-500 { --tw-border-opacity: 1; border-color: rgb(26 30 45 /=
 var(--tw-border-opacity)); }

.border-darkBlue-500\/50 { border-color: rgba(26, 30, 45, 0.5); }

.border-gray-200 { --tw-border-opacity: 1; border-color: rgb(226 232 240 / =
var(--tw-border-opacity)); }

.border-gray-300 { --tw-border-opacity: 1; border-color: rgb(203 213 225 / =
var(--tw-border-opacity)); }

.border-gray-400 { --tw-border-opacity: 1; border-color: rgb(148 163 184 / =
var(--tw-border-opacity)); }

.border-gray-600 { --tw-border-opacity: 1; border-color: rgb(71 85 105 / va=
r(--tw-border-opacity)); }

.border-gray-700 { --tw-border-opacity: 1; border-color: rgb(51 65 85 / var=
(--tw-border-opacity)); }

.border-gray-800 { --tw-border-opacity: 1; border-color: rgb(30 41 59 / var=
(--tw-border-opacity)); }

.border-green-100 { --tw-border-opacity: 1; border-color: rgb(220 252 231 /=
 var(--tw-border-opacity)); }

.border-green-400 { --tw-border-opacity: 1; border-color: rgb(74 222 128 / =
var(--tw-border-opacity)); }

.border-green-600 { --tw-border-opacity: 1; border-color: rgb(22 163 74 / v=
ar(--tw-border-opacity)); }

.border-orange-100 { --tw-border-opacity: 1; border-color: rgb(255 237 213 =
/ var(--tw-border-opacity)); }

.border-red-100 { --tw-border-opacity: 1; border-color: rgb(254 226 226 / v=
ar(--tw-border-opacity)); }

.border-red-300 { --tw-border-opacity: 1; border-color: rgb(252 165 165 / v=
ar(--tw-border-opacity)); }

.border-red-400 { --tw-border-opacity: 1; border-color: rgb(248 113 113 / v=
ar(--tw-border-opacity)); }

.border-slate-500\/10 { border-color: rgba(100, 116, 139, 0.1); }

.border-slate-500\/20 { border-color: rgba(100, 116, 139, 0.2); }

.border-slate-500\/30 { border-color: rgba(100, 116, 139, 0.3); }

.border-slate-700\/10 { border-color: rgba(51, 65, 85, 0.1); }

.border-slate-700\/50 { border-color: rgba(51, 65, 85, 0.5); }

.border-slate-800 { --tw-border-opacity: 1; border-color: rgb(30 41 59 / va=
r(--tw-border-opacity)); }

.border-slate-800\/30 { border-color: rgba(30, 41, 59, 0.3); }

.border-slate-800\/50 { border-color: rgba(30, 41, 59, 0.5); }

.border-transparent { border-color: transparent; }

.border-white\/10 { border-color: rgba(255, 255, 255, 0.1); }

.border-white\/5 { border-color: rgba(255, 255, 255, 0.05); }

.border-yellow-500 { --tw-border-opacity: 1; border-color: rgb(234 179 8 / =
var(--tw-border-opacity)); }

.border-t-slate-500\/10 { border-top-color: rgba(100, 116, 139, 0.1); }

.border-opacity-20 { --tw-border-opacity: .2; }

.bg-\[\#080f1f\] { --tw-bg-opacity: 1; background-color: rgb(8 15 31 / var(=
--tw-bg-opacity)); }

.bg-\[\#090b16\] { --tw-bg-opacity: 1; background-color: rgb(9 11 22 / var(=
--tw-bg-opacity)); }

.bg-\[\#090b16\]\/50 { background-color: rgba(9, 11, 22, 0.5); }

.bg-\[\#0c101f\] { --tw-bg-opacity: 1; background-color: rgb(12 16 31 / var=
(--tw-bg-opacity)); }

.bg-\[\#50c18e\]\/10 { background-color: rgba(80, 193, 142, 0.1); }

.bg-black { --tw-bg-opacity: 1; background-color: rgb(0 0 0 / var(--tw-bg-o=
pacity)); }

.bg-black\/5 { background-color: rgba(0, 0, 0, 0.05); }

.bg-blue-100 { --tw-bg-opacity: 1; background-color: rgb(235 248 255 / var(=
--tw-bg-opacity)); }

.bg-blue-400 { --tw-bg-opacity: 1; background-color: rgb(99 179 237 / var(-=
-tw-bg-opacity)); }

.bg-blue-500 { --tw-bg-opacity: 1; background-color: rgb(65 153 225 / var(-=
-tw-bg-opacity)); }

.bg-blue-600 { --tw-bg-opacity: 1; background-color: rgb(49 130 206 / var(-=
-tw-bg-opacity)); }

.bg-blue-700 { --tw-bg-opacity: 1; background-color: rgb(43 108 176 / var(-=
-tw-bg-opacity)); }

.bg-blue-800 { --tw-bg-opacity: 1; background-color: rgb(44 82 130 / var(--=
tw-bg-opacity)); }

.bg-blue-900\/30 { background-color: rgba(42, 67, 101, 0.3); }

.bg-blue-900\/90 { background-color: rgba(42, 67, 101, 0.9); }

.bg-darkBlue-500 { --tw-bg-opacity: 1; background-color: rgb(26 30 45 / var=
(--tw-bg-opacity)); }

.bg-darkBlue-500\/50 { background-color: rgba(26, 30, 45, 0.5); }

.bg-darkBlue-500\/70 { background-color: rgba(26, 30, 45, 0.7); }

.bg-darkBlue-500\/75 { background-color: rgba(26, 30, 45, 0.75); }

.bg-darkBlue-600 { --tw-bg-opacity: 1; background-color: rgb(37 99 235 / va=
r(--tw-bg-opacity)); }

.bg-darkBlue-700 { --tw-bg-opacity: 1; background-color: rgb(15 18 32 / var=
(--tw-bg-opacity)); }

.bg-darkBlue-900\/50 { background-color: rgba(8, 11, 22, 0.5); }

.bg-darkBlue-900\/60 { background-color: rgba(8, 11, 22, 0.6); }

.bg-darkBlue-900\/90 { background-color: rgba(8, 11, 22, 0.9); }

.bg-gray-100 { --tw-bg-opacity: 1; background-color: rgb(241 245 249 / var(=
--tw-bg-opacity)); }

.bg-gray-200 { --tw-bg-opacity: 1; background-color: rgb(226 232 240 / var(=
--tw-bg-opacity)); }

.bg-gray-400 { --tw-bg-opacity: 1; background-color: rgb(148 163 184 / var(=
--tw-bg-opacity)); }

.bg-gray-600 { --tw-bg-opacity: 1; background-color: rgb(71 85 105 / var(--=
tw-bg-opacity)); }

.bg-gray-800 { --tw-bg-opacity: 1; background-color: rgb(30 41 59 / var(--t=
w-bg-opacity)); }

.bg-green-400 { --tw-bg-opacity: 1; background-color: rgb(74 222 128 / var(=
--tw-bg-opacity)); }

.bg-green-50 { --tw-bg-opacity: 1; background-color: rgb(240 253 244 / var(=
--tw-bg-opacity)); }

.bg-green-500 { --tw-bg-opacity: 1; background-color: rgb(34 197 94 / var(-=
-tw-bg-opacity)); }

.bg-orange-50 { --tw-bg-opacity: 1; background-color: rgb(255 247 237 / var=
(--tw-bg-opacity)); }

.bg-purple-200 { --tw-bg-opacity: 1; background-color: rgb(233 213 255 / va=
r(--tw-bg-opacity)); }

.bg-purple-600 { --tw-bg-opacity: 1; background-color: rgb(147 51 234 / var=
(--tw-bg-opacity)); }

.bg-red-100 { --tw-bg-opacity: 1; background-color: rgb(254 226 226 / var(-=
-tw-bg-opacity)); }

.bg-red-400 { --tw-bg-opacity: 1; background-color: rgb(248 113 113 / var(-=
-tw-bg-opacity)); }

.bg-red-50 { --tw-bg-opacity: 1; background-color: rgb(254 242 242 / var(--=
tw-bg-opacity)); }

.bg-red-500 { --tw-bg-opacity: 1; background-color: rgb(239 68 68 / var(--t=
w-bg-opacity)); }

.bg-red-500\/10 { background-color: rgba(239, 68, 68, 0.1); }

.bg-red-700 { --tw-bg-opacity: 1; background-color: rgb(185 28 28 / var(--t=
w-bg-opacity)); }

.bg-slate-400 { --tw-bg-opacity: 1; background-color: rgb(148 163 184 / var=
(--tw-bg-opacity)); }

.bg-slate-50 { --tw-bg-opacity: 1; background-color: rgb(248 250 252 / var(=
--tw-bg-opacity)); }

.bg-slate-500\/10 { background-color: rgba(100, 116, 139, 0.1); }

.bg-slate-500\/20 { background-color: rgba(100, 116, 139, 0.2); }

.bg-slate-500\/\[0\.05\] { background-color: rgba(100, 116, 139, 0.05); }

.bg-slate-700 { --tw-bg-opacity: 1; background-color: rgb(51 65 85 / var(--=
tw-bg-opacity)); }

.bg-slate-800 { --tw-bg-opacity: 1; background-color: rgb(30 41 59 / var(--=
tw-bg-opacity)); }

.bg-slate-800\/10 { background-color: rgba(30, 41, 59, 0.1); }

.bg-slate-800\/50 { background-color: rgba(30, 41, 59, 0.5); }

.bg-slate-900 { --tw-bg-opacity: 1; background-color: rgb(15 23 42 / var(--=
tw-bg-opacity)); }

.bg-transparent { background-color: transparent; }

.bg-uiBlue-600 { --tw-bg-opacity: 1; background-color: rgb(76 82 134 / var(=
--tw-bg-opacity)); }

.bg-white { --tw-bg-opacity: 1; background-color: rgb(255 255 255 / var(--t=
w-bg-opacity)); }

.bg-white\/10 { background-color: rgba(255, 255, 255, 0.1); }

.bg-yellow-400 { --tw-bg-opacity: 1; background-color: rgb(250 204 21 / var=
(--tw-bg-opacity)); }

.bg-yellow-500 { --tw-bg-opacity: 1; background-color: rgb(234 179 8 / var(=
--tw-bg-opacity)); }

.bg-yellow-500\/10 { background-color: rgba(234, 179, 8, 0.1); }

.\!bg-opacity-100 { --tw-bg-opacity: 1 !important; }

.\!bg-opacity-20 { --tw-bg-opacity: .2 !important; }

.\!bg-opacity-95 { --tw-bg-opacity: .95 !important; }

.bg-opacity-10 { --tw-bg-opacity: .1; }

.bg-opacity-20 { --tw-bg-opacity: .2; }

.bg-opacity-5 { --tw-bg-opacity: .05; }

.bg-opacity-75 { --tw-bg-opacity: .75; }

.bg-opacity-80 { --tw-bg-opacity: .8; }

.bg-\[image\:url\(\'\/img\/waves\.svg\'\)\] { background-image: url("/img/w=
aves.svg"); }

.bg-gradient-to-bl { background-image: linear-gradient(to bottom left,var(-=
-tw-gradient-stops)); }

.bg-gradient-to-br { background-image: linear-gradient(to bottom right,var(=
--tw-gradient-stops)); }

.from-\[\#172B51\] { --tw-gradient-from: #172B51 var(--tw-gradient-from-pos=
ition); --tw-gradient-to: rgb(23 43 81 / 0) var(--tw-gradient-to-position);=
 --tw-gradient-stops: var(--tw-gradient-from), var(--tw-gradient-to); }

.from-red-300\/5 { --tw-gradient-from: rgb(252 165 165 / .05) var(--tw-grad=
ient-from-position); --tw-gradient-to: rgb(252 165 165 / 0) var(--tw-gradie=
nt-to-position); --tw-gradient-stops: var(--tw-gradient-from), var(--tw-gra=
dient-to); }

.to-\[\#1d3259\] { --tw-gradient-to: #1d3259 var(--tw-gradient-to-position)=
; }

.to-transparent { --tw-gradient-to: transparent var(--tw-gradient-to-positi=
on); }

.bg-cover { background-size: cover; }

.object-contain { object-fit: contain; }

.object-cover { object-fit: cover; }

.p-0 { padding: 0px; }

.p-0\.5 { padding: 0.125rem; }

.p-1 { padding: 0.25rem; }

.p-1\.5 { padding: 0.375rem; }

.p-2 { padding: 0.5rem; }

.p-4 { padding: 1rem; }

.p-6 { padding: 1.5rem; }

.px-0 { padding-left: 0px; padding-right: 0px; }

.px-0\.5 { padding-left: 0.125rem; padding-right: 0.125rem; }

.px-1 { padding-left: 0.25rem; padding-right: 0.25rem; }

.px-1\.5 { padding-left: 0.375rem; padding-right: 0.375rem; }

.px-10 { padding-left: 2.5rem; padding-right: 2.5rem; }

.px-2 { padding-left: 0.5rem; padding-right: 0.5rem; }

.px-2\.5 { padding-left: 0.625rem; padding-right: 0.625rem; }

.px-3 { padding-left: 0.75rem; padding-right: 0.75rem; }

.px-3\.5 { padding-left: 0.875rem; padding-right: 0.875rem; }

.px-4 { padding-left: 1rem; padding-right: 1rem; }

.px-5 { padding-left: 1.25rem; padding-right: 1.25rem; }

.px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }

.py-0 { padding-top: 0px; padding-bottom: 0px; }

.py-0\.5 { padding-top: 0.125rem; padding-bottom: 0.125rem; }

.py-1 { padding-top: 0.25rem; padding-bottom: 0.25rem; }

.py-1\.5 { padding-top: 0.375rem; padding-bottom: 0.375rem; }

.py-16 { padding-top: 4rem; padding-bottom: 4rem; }

.py-2 { padding-top: 0.5rem; padding-bottom: 0.5rem; }

.py-2\.5 { padding-top: 0.625rem; padding-bottom: 0.625rem; }

.py-3 { padding-top: 0.75rem; padding-bottom: 0.75rem; }

.py-32 { padding-top: 8rem; padding-bottom: 8rem; }

.py-4 { padding-top: 1rem; padding-bottom: 1rem; }

.py-5 { padding-top: 1.25rem; padding-bottom: 1.25rem; }

.py-6 { padding-top: 1.5rem; padding-bottom: 1.5rem; }

.py-8 { padding-top: 2rem; padding-bottom: 2rem; }

.pb-16 { padding-bottom: 4rem; }

.pb-2 { padding-bottom: 0.5rem; }

.pb-20 { padding-bottom: 5rem; }

.pb-24 { padding-bottom: 6rem; }

.pb-3 { padding-bottom: 0.75rem; }

.pb-4 { padding-bottom: 1rem; }

.pb-6 { padding-bottom: 1.5rem; }

.pl-0 { padding-left: 0px; }

.pl-0\.5 { padding-left: 0.125rem; }

.pl-2 { padding-left: 0.5rem; }

.pl-3 { padding-left: 0.75rem; }

.pl-4 { padding-left: 1rem; }

.pl-6 { padding-left: 1.5rem; }

.pl-7 { padding-left: 1.75rem; }

.pr-10 { padding-right: 2.5rem; }

.pr-12 { padding-right: 3rem; }

.pr-2 { padding-right: 0.5rem; }

.pr-2\.5 { padding-right: 0.625rem; }

.pr-3 { padding-right: 0.75rem; }

.pr-6 { padding-right: 1.5rem; }

.pt-0 { padding-top: 0px; }

.pt-0\.5 { padding-top: 0.125rem; }

.pt-12 { padding-top: 3rem; }

.pt-2 { padding-top: 0.5rem; }

.pt-4 { padding-top: 1rem; }

.pt-5 { padding-top: 1.25rem; }

.pt-6 { padding-top: 1.5rem; }

.pt-8 { padding-top: 2rem; }

.text-left { text-align: left; }

.text-center { text-align: center; }

.text-right { text-align: right; }

.align-middle { vertical-align: middle; }

.align-bottom { vertical-align: bottom; }

.font-default { font-family: ui-sans-serif, system-ui, -apple-system, Blink=
MacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sa=
ns-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto C=
olor Emoji"; }

.text-2xl { font-size: 1.5rem; line-height: 2rem; }

.text-3xl { font-size: 1.875rem; line-height: 2.25rem; }

.text-4xl { font-size: 2.25rem; line-height: 2.5rem; }

.text-\[10px\] { font-size: 10px; }

.text-\[15px\] { font-size: 15px; }

.text-\[8px\] { font-size: 8px; }

.text-base { font-size: 1rem; line-height: 1.5rem; }

.text-lg { font-size: 1.125rem; line-height: 1.75rem; }

.text-sm { font-size: 0.875rem; line-height: 1.25rem; }

.text-xl { font-size: 1.25rem; line-height: 1.75rem; }

.text-xs { font-size: 0.75rem; line-height: 1rem; }

.font-black { font-weight: 900; }

.font-bold { font-weight: 700; }

.font-extrabold { font-weight: 800; }

.font-medium { font-weight: 500; }

.font-normal { font-weight: 400; }

.font-semibold { font-weight: 600; }

.uppercase { text-transform: uppercase; }

.capitalize { text-transform: capitalize; }

.leading-4 { line-height: 1rem; }

.leading-5 { line-height: 1.25rem; }

.leading-6 { line-height: 1.5rem; }

.leading-9 { line-height: 2.25rem; }

.leading-\[16px\] { line-height: 16px; }

.leading-\[18px\] { line-height: 18px; }

.leading-none { line-height: 1; }

.tracking-tight { letter-spacing: -0.025em; }

.tracking-tighter { letter-spacing: -0.05em; }

.tracking-wide { letter-spacing: 0.025em; }

.tracking-widest { letter-spacing: 0.1em; }

.text-\[\#0088cc\] { --tw-text-opacity: 1; color: rgb(0 136 204 / var(--tw-=
text-opacity)); }

.text-\[\#1da1f2\] { --tw-text-opacity: 1; color: rgb(29 161 242 / var(--tw=
-text-opacity)); }

.text-\[\#50c18e\] { --tw-text-opacity: 1; color: rgb(80 193 142 / var(--tw=
-text-opacity)); }

.text-\[\#7289da\] { --tw-text-opacity: 1; color: rgb(114 137 218 / var(--t=
w-text-opacity)); }

.text-\[\#c0d0f1\] { --tw-text-opacity: 1; color: rgb(192 208 241 / var(--t=
w-text-opacity)); }

.text-\[\#c0d0f1\]\/50 { color: rgba(192, 208, 241, 0.5); }

.text-\[\#dcffff\] { --tw-text-opacity: 1; color: rgb(220 255 255 / var(--t=
w-text-opacity)); }

.text-black { --tw-text-opacity: 1; color: rgb(0 0 0 / var(--tw-text-opacit=
y)); }

.text-black\/70 { color: rgba(0, 0, 0, 0.7); }

.text-blue-400 { --tw-text-opacity: 1; color: rgb(99 179 237 / var(--tw-tex=
t-opacity)); }

.text-blue-500 { --tw-text-opacity: 1; color: rgb(65 153 225 / var(--tw-tex=
t-opacity)); }

.text-blue-500\/90 { color: rgba(65, 153, 225, 0.9); }

.text-blue-600 { --tw-text-opacity: 1; color: rgb(49 130 206 / var(--tw-tex=
t-opacity)); }

.text-blue-800 { --tw-text-opacity: 1; color: rgb(44 82 130 / var(--tw-text=
-opacity)); }

.text-darkBlue-600 { --tw-text-opacity: 1; color: rgb(37 99 235 / var(--tw-=
text-opacity)); }

.text-gray-200 { --tw-text-opacity: 1; color: rgb(226 232 240 / var(--tw-te=
xt-opacity)); }

.text-gray-300 { --tw-text-opacity: 1; color: rgb(203 213 225 / var(--tw-te=
xt-opacity)); }

.text-gray-400 { --tw-text-opacity: 1; color: rgb(148 163 184 / var(--tw-te=
xt-opacity)); }

.text-gray-500 { --tw-text-opacity: 1; color: rgb(100 116 139 / var(--tw-te=
xt-opacity)); }

.text-gray-600 { --tw-text-opacity: 1; color: rgb(71 85 105 / var(--tw-text=
-opacity)); }

.text-gray-700 { --tw-text-opacity: 1; color: rgb(51 65 85 / var(--tw-text-=
opacity)); }

.text-gray-800 { --tw-text-opacity: 1; color: rgb(30 41 59 / var(--tw-text-=
opacity)); }

.text-gray-900 { --tw-text-opacity: 1; color: rgb(15 23 42 / var(--tw-text-=
opacity)); }

.text-green-400 { --tw-text-opacity: 1; color: rgb(74 222 128 / var(--tw-te=
xt-opacity)); }

.text-green-500 { --tw-text-opacity: 1; color: rgb(34 197 94 / var(--tw-tex=
t-opacity)); }

.text-green-600 { --tw-text-opacity: 1; color: rgb(22 163 74 / var(--tw-tex=
t-opacity)); }

.text-orange-500 { --tw-text-opacity: 1; color: rgb(249 115 22 / var(--tw-t=
ext-opacity)); }

.text-red-100 { --tw-text-opacity: 1; color: rgb(254 226 226 / var(--tw-tex=
t-opacity)); }

.text-red-300 { --tw-text-opacity: 1; color: rgb(252 165 165 / var(--tw-tex=
t-opacity)); }

.text-red-400 { --tw-text-opacity: 1; color: rgb(248 113 113 / var(--tw-tex=
t-opacity)); }

.text-red-500 { --tw-text-opacity: 1; color: rgb(239 68 68 / var(--tw-text-=
opacity)); }

.text-red-600 { --tw-text-opacity: 1; color: rgb(220 38 38 / var(--tw-text-=
opacity)); }

.text-red-700 { --tw-text-opacity: 1; color: rgb(185 28 28 / var(--tw-text-=
opacity)); }

.text-red-900 { --tw-text-opacity: 1; color: rgb(127 29 29 / var(--tw-text-=
opacity)); }

.text-slate-100 { --tw-text-opacity: 1; color: rgb(241 245 249 / var(--tw-t=
ext-opacity)); }

.text-slate-100\/50 { color: rgba(241, 245, 249, 0.5); }

.text-slate-200 { --tw-text-opacity: 1; color: rgb(226 232 240 / var(--tw-t=
ext-opacity)); }

.text-slate-300 { --tw-text-opacity: 1; color: rgb(203 213 225 / var(--tw-t=
ext-opacity)); }

.text-slate-400 { --tw-text-opacity: 1; color: rgb(148 163 184 / var(--tw-t=
ext-opacity)); }

.text-slate-500 { --tw-text-opacity: 1; color: rgb(100 116 139 / var(--tw-t=
ext-opacity)); }

.text-slate-500\/50 { color: rgba(100, 116, 139, 0.5); }

.text-slate-700 { --tw-text-opacity: 1; color: rgb(51 65 85 / var(--tw-text=
-opacity)); }

.text-slate-900 { --tw-text-opacity: 1; color: rgb(15 23 42 / var(--tw-text=
-opacity)); }

.text-slate-900\/\[0\.07\] { color: rgba(15, 23, 42, 0.07); }

.text-slate-900\/\[0\.1\] { color: rgba(15, 23, 42, 0.1); }

.text-white { --tw-text-opacity: 1; color: rgb(255 255 255 / var(--tw-text-=
opacity)); }

.text-white\/50 { color: rgba(255, 255, 255, 0.5); }

.text-white\/70 { color: rgba(255, 255, 255, 0.7); }

.text-white\/80 { color: rgba(255, 255, 255, 0.8); }

.text-yellow-400 { --tw-text-opacity: 1; color: rgb(250 204 21 / var(--tw-t=
ext-opacity)); }

.text-yellow-500 { --tw-text-opacity: 1; color: rgb(234 179 8 / var(--tw-te=
xt-opacity)); }

.underline { text-decoration-line: underline; }

.underline-offset-2 { text-underline-offset: 2px; }

.placeholder-gray-300::-webkit-input-placeholder { --tw-placeholder-opacity=
: 1; color: rgb(203 213 225 / var(--tw-placeholder-opacity)); }

.placeholder-gray-300::placeholder { --tw-placeholder-opacity: 1; color: rg=
b(203 213 225 / var(--tw-placeholder-opacity)); }

.placeholder-red-300::-webkit-input-placeholder { --tw-placeholder-opacity:=
 1; color: rgb(252 165 165 / var(--tw-placeholder-opacity)); }

.placeholder-red-300::placeholder { --tw-placeholder-opacity: 1; color: rgb=
(252 165 165 / var(--tw-placeholder-opacity)); }

.opacity-0 { opacity: 0; }

.opacity-10 { opacity: 0.1; }

.opacity-100 { opacity: 1; }

.opacity-25 { opacity: 0.25; }

.opacity-50 { opacity: 0.5; }

.opacity-75 { opacity: 0.75; }

.opacity-90 { opacity: 0.9; }

.bg-blend-darken { background-blend-mode: darken; }

.mix-blend-overlay { mix-blend-mode: overlay; }

.\!shadow-sm { --tw-shadow: 0 1px 2px 0 rgb(0 0 0 / .05) !important; --tw-s=
hadow-colored: 0 1px 2px 0 var(--tw-shadow-color) !important; box-shadow: v=
ar(--tw-ring-offset-shadow, 0 0 #0000),var(--tw-ring-shadow, 0 0 #0000),var=
(--tw-shadow) !important; }

.shadow { --tw-shadow: 0 1px 3px 0 rgb(0 0 0 / .1), 0 1px 2px -1px rgb(0 0 =
0 / .1); --tw-shadow-colored: 0 1px 3px 0 var(--tw-shadow-color), 0 1px 2px=
 -1px var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow, 0 0 =
#0000),var(--tw-ring-shadow, 0 0 #0000),var(--tw-shadow); }

.shadow-2xl { --tw-shadow: 0 25px 50px -12px rgb(0 0 0 / .25); --tw-shadow-=
colored: 0 25px 50px -12px var(--tw-shadow-color); box-shadow: var(--tw-rin=
g-offset-shadow, 0 0 #0000),var(--tw-ring-shadow, 0 0 #0000),var(--tw-shado=
w); }

.shadow-lg { --tw-shadow: 0 10px 15px -3px rgb(0 0 0 / .1), 0 4px 6px -4px =
rgb(0 0 0 / .1); --tw-shadow-colored: 0 10px 15px -3px var(--tw-shadow-colo=
r), 0 4px 6px -4px var(--tw-shadow-color); box-shadow: var(--tw-ring-offset=
-shadow, 0 0 #0000),var(--tw-ring-shadow, 0 0 #0000),var(--tw-shadow); }

.shadow-md { --tw-shadow: 0 4px 6px -1px rgb(0 0 0 / .1), 0 2px 4px -2px rg=
b(0 0 0 / .1); --tw-shadow-colored: 0 4px 6px -1px var(--tw-shadow-color), =
0 2px 4px -2px var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-sha=
dow, 0 0 #0000),var(--tw-ring-shadow, 0 0 #0000),var(--tw-shadow); }

.shadow-sm { --tw-shadow: 0 1px 2px 0 rgb(0 0 0 / .05); --tw-shadow-colored=
: 0 1px 2px 0 var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shad=
ow, 0 0 #0000),var(--tw-ring-shadow, 0 0 #0000),var(--tw-shadow); }

.shadow-xl { --tw-shadow: 0 20px 25px -5px rgb(0 0 0 / .1), 0 8px 10px -6px=
 rgb(0 0 0 / .1); --tw-shadow-colored: 0 20px 25px -5px var(--tw-shadow-col=
or), 0 8px 10px -6px var(--tw-shadow-color); box-shadow: var(--tw-ring-offs=
et-shadow, 0 0 #0000),var(--tw-ring-shadow, 0 0 #0000),var(--tw-shadow); }

.shadow-darkBlue-900\/50 { --tw-shadow-color: rgb(8 11 22 / .5); --tw-shado=
w: var(--tw-shadow-colored); }

.\!outline-none { outline: transparent solid 2px !important; outline-offset=
: 2px !important; }

.outline-slate-200\/40 { outline-color: rgba(226, 232, 240, 0.4); }

.ring { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-o=
ffset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-i=
nset) 0 0 0 calc(3px + var(--tw-ring-offset-width)) var(--tw-ring-color); b=
ox-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shad=
ow, 0 0 #0000); }

.ring-1 { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring=
-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring=
-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color);=
 box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-sh=
adow, 0 0 #0000); }

.ring-2 { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring=
-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring=
-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color);=
 box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-sh=
adow, 0 0 #0000); }

.ring-inset { --tw-ring-inset: inset; }

.ring-black { --tw-ring-opacity: 1; --tw-ring-color: rgb(0 0 0 / var(--tw-r=
ing-opacity)); }

.ring-black\/20 { --tw-ring-color: rgb(0 0 0 / .2); }

.ring-blue-500 { --tw-ring-opacity: 1; --tw-ring-color: rgb(65 153 225 / va=
r(--tw-ring-opacity)); }

.ring-blue-600 { --tw-ring-opacity: 1; --tw-ring-color: rgb(49 130 206 / va=
r(--tw-ring-opacity)); }

.ring-slate-800 { --tw-ring-opacity: 1; --tw-ring-color: rgb(30 41 59 / var=
(--tw-ring-opacity)); }

.ring-slate-900\/10 { --tw-ring-color: rgb(15 23 42 / .1); }

.ring-transparent { --tw-ring-color: transparent; }

.ring-opacity-5 { --tw-ring-opacity: .05; }

.grayscale-\[0\.75\] { --tw-grayscale: grayscale(.75); filter: var(--tw-blu=
r) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue=
-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-=
shadow); }

.filter { filter: var(--tw-blur) var(--tw-brightness) var(--tw-contrast) va=
r(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) =
var(--tw-sepia) var(--tw-drop-shadow); }

.backdrop-blur { --tw-backdrop-blur: blur(8px); backdrop-filter: var(--tw-b=
ackdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var=
(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-i=
nvert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-back=
drop-sepia); }

.backdrop-blur-sm { --tw-backdrop-blur: blur(4px); backdrop-filter: var(--t=
w-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) =
var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdro=
p-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-b=
ackdrop-sepia); }

.backdrop-blur-xl { --tw-backdrop-blur: blur(24px); backdrop-filter: var(--=
tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast)=
 var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdr=
op-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-=
backdrop-sepia); }

.transition { transition-property: color, background-color, border-color, t=
ext-decoration-color, fill, stroke, opacity, box-shadow, transform, filter,=
 backdrop-filter, -webkit-text-decoration-color, -webkit-box-shadow, -webki=
t-transform, -webkit-filter, -webkit-backdrop-filter; transition-timing-fun=
ction: cubic-bezier(0, 0, 0.2, 1); transition-duration: 0.15s; }

.transition-\[border\] { transition-property: border; transition-timing-fun=
ction: cubic-bezier(0, 0, 0.2, 1); transition-duration: 0.15s; }

.transition-all { transition-property: all; transition-timing-function: cub=
ic-bezier(0, 0, 0.2, 1); transition-duration: 0.15s; }

.transition-colors { transition-property: color, background-color, border-c=
olor, text-decoration-color, fill, stroke, -webkit-text-decoration-color; t=
ransition-timing-function: cubic-bezier(0, 0, 0.2, 1); transition-duration:=
 0.15s; }

.transition-opacity { transition-property: opacity; transition-timing-funct=
ion: cubic-bezier(0, 0, 0.2, 1); transition-duration: 0.15s; }

.duration-100 { transition-duration: 0.1s; }

.duration-150 { transition-duration: 0.15s; }

.duration-200 { transition-duration: 0.2s; }

.duration-300 { transition-duration: 0.3s; }

.duration-75 { transition-duration: 75ms; }

.ease-in { transition-timing-function: cubic-bezier(0.4, 0, 1, 1); }

.ease-in-out { transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.ease-out { transition-timing-function: cubic-bezier(0, 0, 0.2, 1); }

.\[mask-image\:linear-gradient\(to_bottom_left\,white\,transparent\,transpa=
rent\)\] { mask-image: linear-gradient(to left bottom, white, transparent, =
transparent); }

@media (max-width: 640px) {
  .h-screen { height: calc(var(--vh, 1vh) * 100); }
  .min-h-screen { min-height: calc(var(--vh, 1vh) * 100); }
}

:is(.dark .dark\:prose-invert) { --tw-prose-body: var(--tw-prose-invert-bod=
y); --tw-prose-headings: var(--tw-prose-invert-headings); --tw-prose-lead: =
var(--tw-prose-invert-lead); --tw-prose-links: var(--tw-prose-invert-links)=
; --tw-prose-bold: var(--tw-prose-invert-bold); --tw-prose-counters: var(--=
tw-prose-invert-counters); --tw-prose-bullets: var(--tw-prose-invert-bullet=
s); --tw-prose-hr: var(--tw-prose-invert-hr); --tw-prose-quotes: var(--tw-p=
rose-invert-quotes); --tw-prose-quote-borders: var(--tw-prose-invert-quote-=
borders); --tw-prose-captions: var(--tw-prose-invert-captions); --tw-prose-=
kbd: var(--tw-prose-invert-kbd); --tw-prose-kbd-shadows: var(--tw-prose-inv=
ert-kbd-shadows); --tw-prose-code: var(--tw-prose-invert-code); --tw-prose-=
pre-code: var(--tw-prose-invert-pre-code); --tw-prose-pre-bg: var(--tw-pros=
e-invert-pre-bg); --tw-prose-th-borders: var(--tw-prose-invert-th-borders);=
 --tw-prose-td-borders: var(--tw-prose-invert-td-borders); }

.placeholder\:text-slate-500::-webkit-input-placeholder { --tw-text-opacity=
: 1; color: rgb(100 116 139 / var(--tw-text-opacity)); }

.placeholder\:text-slate-500::placeholder { --tw-text-opacity: 1; color: rg=
b(100 116 139 / var(--tw-text-opacity)); }

.focus-within\:border-blue-500:focus-within { --tw-border-opacity: 1; borde=
r-color: rgb(65 153 225 / var(--tw-border-opacity)); }

.focus-within\:border-blue-500\/50:focus-within { border-color: rgba(65, 15=
3, 225, 0.5); }

.focus-within\:ring-blue-500:focus-within { --tw-ring-opacity: 1; --tw-ring=
-color: rgb(65 153 225 / var(--tw-ring-opacity)); }

.hover\:scale-105:hover { --tw-scale-x: 1.05; --tw-scale-y: 1.05; transform=
: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-ro=
tate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale=
-x)) scaleY(var(--tw-scale-y)); }

.hover\:border-blue-500:hover { --tw-border-opacity: 1; border-color: rgb(6=
5 153 225 / var(--tw-border-opacity)); }

.hover\:border-gray-600:hover { --tw-border-opacity: 1; border-color: rgb(7=
1 85 105 / var(--tw-border-opacity)); }

.hover\:border-red-500:hover { --tw-border-opacity: 1; border-color: rgb(23=
9 68 68 / var(--tw-border-opacity)); }

.hover\:border-slate-400\/30:hover { border-color: rgba(148, 163, 184, 0.3)=
; }

.hover\:border-slate-600:hover { --tw-border-opacity: 1; border-color: rgb(=
71 85 105 / var(--tw-border-opacity)); }

.hover\:\!bg-blue-900:hover { --tw-bg-opacity: 1 !important; background-col=
or: rgb(42 67 101 / var(--tw-bg-opacity)) !important; }

.hover\:\!bg-slate-700:hover { --tw-bg-opacity: 1 !important; background-co=
lor: rgb(51 65 85 / var(--tw-bg-opacity)) !important; }

.hover\:bg-\[\#172B51\]\/50:hover { background-color: rgba(23, 43, 81, 0.5)=
; }

.hover\:bg-black\/5:hover { background-color: rgba(0, 0, 0, 0.05); }

.hover\:bg-blue-500:hover { --tw-bg-opacity: 1; background-color: rgb(65 15=
3 225 / var(--tw-bg-opacity)); }

.hover\:bg-blue-500\/10:hover { background-color: rgba(65, 153, 225, 0.1); =
}

.hover\:bg-blue-500\/20:hover { background-color: rgba(65, 153, 225, 0.2); =
}

.hover\:bg-blue-800:hover { --tw-bg-opacity: 1; background-color: rgb(44 82=
 130 / var(--tw-bg-opacity)); }

.hover\:bg-blue-900:hover { --tw-bg-opacity: 1; background-color: rgb(42 67=
 101 / var(--tw-bg-opacity)); }

.hover\:bg-darkBlue-500:hover { --tw-bg-opacity: 1; background-color: rgb(2=
6 30 45 / var(--tw-bg-opacity)); }

.hover\:bg-darkBlue-500\/70:hover { background-color: rgba(26, 30, 45, 0.7)=
; }

.hover\:bg-gray-100:hover { --tw-bg-opacity: 1; background-color: rgb(241 2=
45 249 / var(--tw-bg-opacity)); }

.hover\:bg-gray-50:hover { --tw-bg-opacity: 1; background-color: rgb(248 25=
0 252 / var(--tw-bg-opacity)); }

.hover\:bg-orange-100:hover { --tw-bg-opacity: 1; background-color: rgb(255=
 237 213 / var(--tw-bg-opacity)); }

.hover\:bg-red-400:hover { --tw-bg-opacity: 1; background-color: rgb(248 11=
3 113 / var(--tw-bg-opacity)); }

.hover\:bg-red-500:hover { --tw-bg-opacity: 1; background-color: rgb(239 68=
 68 / var(--tw-bg-opacity)); }

.hover\:bg-red-500\/20:hover { background-color: rgba(239, 68, 68, 0.2); }

.hover\:bg-red-800:hover { --tw-bg-opacity: 1; background-color: rgb(153 27=
 27 / var(--tw-bg-opacity)); }

.hover\:bg-slate-100:hover { --tw-bg-opacity: 1; background-color: rgb(241 =
245 249 / var(--tw-bg-opacity)); }

.hover\:bg-slate-100\/10:hover { background-color: rgba(241, 245, 249, 0.1)=
; }

.hover\:bg-slate-500:hover { --tw-bg-opacity: 1; background-color: rgb(100 =
116 139 / var(--tw-bg-opacity)); }

.hover\:bg-slate-500\/10:hover { background-color: rgba(100, 116, 139, 0.1)=
; }

.hover\:bg-slate-500\/50:hover { background-color: rgba(100, 116, 139, 0.5)=
; }

.hover\:bg-slate-800\/50:hover { background-color: rgba(30, 41, 59, 0.5); }

.hover\:bg-uiBlue-500:hover { --tw-bg-opacity: 1; background-color: rgb(52 =
57 106 / var(--tw-bg-opacity)); }

.hover\:bg-uiBlue-500\/70:hover { background-color: rgba(52, 57, 106, 0.7);=
 }

.hover\:bg-uiBlue-600\/10:hover { background-color: rgba(76, 82, 134, 0.1);=
 }

.hover\:bg-uiBlue-600\/80:hover { background-color: rgba(76, 82, 134, 0.8);=
 }

.hover\:bg-white\/5:hover { background-color: rgba(255, 255, 255, 0.05); }

.hover\:bg-white\/\[0\.08\]:hover { background-color: rgba(255, 255, 255, 0=
.08); }

.hover\:text-\[\#f9bb49\]:hover { --tw-text-opacity: 1; color: rgb(249 187 =
73 / var(--tw-text-opacity)); }

.hover\:text-black:hover { --tw-text-opacity: 1; color: rgb(0 0 0 / var(--t=
w-text-opacity)); }

.hover\:text-blue-500:hover { --tw-text-opacity: 1; color: rgb(65 153 225 /=
 var(--tw-text-opacity)); }

.hover\:text-gray-100:hover { --tw-text-opacity: 1; color: rgb(241 245 249 =
/ var(--tw-text-opacity)); }

.hover\:text-gray-500:hover { --tw-text-opacity: 1; color: rgb(100 116 139 =
/ var(--tw-text-opacity)); }

.hover\:text-orange-500:hover { --tw-text-opacity: 1; color: rgb(249 115 22=
 / var(--tw-text-opacity)); }

.hover\:text-purple-800:hover { --tw-text-opacity: 1; color: rgb(107 33 168=
 / var(--tw-text-opacity)); }

.hover\:text-red-100:hover { --tw-text-opacity: 1; color: rgb(254 226 226 /=
 var(--tw-text-opacity)); }

.hover\:text-red-500:hover { --tw-text-opacity: 1; color: rgb(239 68 68 / v=
ar(--tw-text-opacity)); }

.hover\:text-slate-100:hover { --tw-text-opacity: 1; color: rgb(241 245 249=
 / var(--tw-text-opacity)); }

.hover\:text-slate-300:hover { --tw-text-opacity: 1; color: rgb(203 213 225=
 / var(--tw-text-opacity)); }

.hover\:text-slate-500:hover { --tw-text-opacity: 1; color: rgb(100 116 139=
 / var(--tw-text-opacity)); }

.hover\:text-white:hover { --tw-text-opacity: 1; color: rgb(255 255 255 / v=
ar(--tw-text-opacity)); }

.hover\:underline:hover { text-decoration-line: underline; }

.hover\:opacity-100:hover { opacity: 1; }

.hover\:opacity-75:hover { opacity: 0.75; }

.hover\:opacity-80:hover { opacity: 0.8; }

.hover\:bg-blend-darken:hover { background-blend-mode: darken; }

.hover\:ring-blue-500:hover { --tw-ring-opacity: 1; --tw-ring-color: rgb(65=
 153 225 / var(--tw-ring-opacity)); }

.focus\:border-blue-300:focus { --tw-border-opacity: 1; border-color: rgb(1=
47 197 253 / var(--tw-border-opacity)); }

.focus\:border-blue-700:focus { --tw-border-opacity: 1; border-color: rgb(4=
3 108 176 / var(--tw-border-opacity)); }

.focus\:border-darkBlue-700:focus { --tw-border-opacity: 1; border-color: r=
gb(15 18 32 / var(--tw-border-opacity)); }

.focus\:border-red-300:focus { --tw-border-opacity: 1; border-color: rgb(25=
2 165 165 / var(--tw-border-opacity)); }

.focus\:border-slate-700:focus { --tw-border-opacity: 1; border-color: rgb(=
51 65 85 / var(--tw-border-opacity)); }

.focus\:border-uiBlue-600:focus { --tw-border-opacity: 1; border-color: rgb=
(76 82 134 / var(--tw-border-opacity)); }

.focus\:text-slate-300:focus { --tw-text-opacity: 1; color: rgb(203 213 225=
 / var(--tw-text-opacity)); }

.focus\:underline:focus { text-decoration-line: underline; }

.focus\:outline-none:focus { outline: transparent solid 2px; outline-offset=
: 2px; }

.focus\:ring-0:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 =
var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: =
var(--tw-ring-inset) 0 0 0 calc(0px + var(--tw-ring-offset-width)) var(--tw=
-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow=
),var(--tw-shadow, 0 0 #0000); }

.focus\:ring-1:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 =
var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: =
var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw=
-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow=
),var(--tw-shadow, 0 0 #0000); }

.focus\:ring-2:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 =
var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: =
var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw=
-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow=
),var(--tw-shadow, 0 0 #0000); }

.focus\:ring-blue-500:focus { --tw-ring-opacity: 1; --tw-ring-color: rgb(65=
 153 225 / var(--tw-ring-opacity)); }

.focus\:ring-slate-500:focus { --tw-ring-opacity: 1; --tw-ring-color: rgb(1=
00 116 139 / var(--tw-ring-opacity)); }

.focus\:ring-uiBlue-400:focus { --tw-ring-opacity: 1; --tw-ring-color: rgb(=
96 165 250 / var(--tw-ring-opacity)); }

.focus\:ring-uiBlue-500:focus { --tw-ring-opacity: 1; --tw-ring-color: rgb(=
52 57 106 / var(--tw-ring-opacity)); }

.active\:scale-125:active { --tw-scale-x: 1.25; --tw-scale-y: 1.25; transfo=
rm: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-=
rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-sca=
le-x)) scaleY(var(--tw-scale-y)); }

.active\:bg-\[\#090b16\]\/80:active { background-color: rgba(9, 11, 22, 0.8=
); }

.active\:bg-blue-700:active { --tw-bg-opacity: 1; background-color: rgb(43 =
108 176 / var(--tw-bg-opacity)); }

.active\:bg-darkBlue-700:active { --tw-bg-opacity: 1; background-color: rgb=
(15 18 32 / var(--tw-bg-opacity)); }

.active\:bg-gray-50:active { --tw-bg-opacity: 1; background-color: rgb(248 =
250 252 / var(--tw-bg-opacity)); }

.active\:bg-red-50:active { --tw-bg-opacity: 1; background-color: rgb(254 2=
42 242 / var(--tw-bg-opacity)); }

.active\:bg-red-800:active { --tw-bg-opacity: 1; background-color: rgb(153 =
27 27 / var(--tw-bg-opacity)); }

.active\:bg-red-900:active { --tw-bg-opacity: 1; background-color: rgb(127 =
29 29 / var(--tw-bg-opacity)); }

.active\:bg-slate-200:active { --tw-bg-opacity: 1; background-color: rgb(22=
6 232 240 / var(--tw-bg-opacity)); }

.active\:bg-slate-500\/20:active { background-color: rgba(100, 116, 139, 0.=
2); }

.active\:bg-slate-700:active { --tw-bg-opacity: 1; background-color: rgb(51=
 65 85 / var(--tw-bg-opacity)); }

.active\:bg-slate-800:active { --tw-bg-opacity: 1; background-color: rgb(30=
 41 59 / var(--tw-bg-opacity)); }

.active\:bg-slate-800\/50:active { background-color: rgba(30, 41, 59, 0.5);=
 }

.active\:bg-uiBlue-600:active { --tw-bg-opacity: 1; background-color: rgb(7=
6 82 134 / var(--tw-bg-opacity)); }

.active\:text-gray-800:active { --tw-text-opacity: 1; color: rgb(30 41 59 /=
 var(--tw-text-opacity)); }

.active\:text-red-800:active { --tw-text-opacity: 1; color: rgb(153 27 27 /=
 var(--tw-text-opacity)); }

.active\:text-white\/80:active { color: rgba(255, 255, 255, 0.8); }

.active\:outline-2:active { outline-width: 2px; }

.active\:ring-2:active { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 =
0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow=
: var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--=
tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shad=
ow),var(--tw-shadow, 0 0 #0000); }

.active\:ring-red-500:active { --tw-ring-opacity: 1; --tw-ring-color: rgb(2=
39 68 68 / var(--tw-ring-opacity)); }

.active\:ring-uiBlue-400:active { --tw-ring-opacity: 1; --tw-ring-color: rg=
b(96 165 250 / var(--tw-ring-opacity)); }

.active\:ring-uiBlue-600:active { --tw-ring-opacity: 1; --tw-ring-color: rg=
b(76 82 134 / var(--tw-ring-opacity)); }

.active\:ring-offset-gray-100:active { --tw-ring-offset-color: #f1f5f9; }

.disabled\:pointer-events-none:disabled { pointer-events: none; }

.disabled\:cursor-not-allowed:disabled { cursor: not-allowed; }

.disabled\:opacity-60:disabled { opacity: 0.6; }

.group:focus-within .group-focus-within\:opacity-0 { opacity: 0; }

.group:hover .group-hover\:scale-110 { --tw-scale-x: 1.1; --tw-scale-y: 1.1=
; transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(=
var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var=
(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

@keyframes ping {=20
  75%, 100% { transform: scale(2); opacity: 0; }
}

.group:hover .group-hover\:animate-ping { animation: 1s cubic-bezier(0, 0, =
0.2, 1) 0s infinite normal none running ping; }

.group:hover .group-hover\:border-slate-300 { --tw-border-opacity: 1; borde=
r-color: rgb(203 213 225 / var(--tw-border-opacity)); }

.group:hover .group-hover\:border-slate-700 { --tw-border-opacity: 1; borde=
r-color: rgb(51 65 85 / var(--tw-border-opacity)); }

.group:hover .group-hover\:bg-\[\#172B51\]\/50 { background-color: rgba(23,=
 43, 81, 0.5); }

.group:hover .group-hover\:text-\[\#7289da\] { --tw-text-opacity: 1; color:=
 rgb(114 137 218 / var(--tw-text-opacity)); }

.group:hover .group-hover\:text-black { --tw-text-opacity: 1; color: rgb(0 =
0 0 / var(--tw-text-opacity)); }

.group:hover .group-hover\:text-blue-600 { --tw-text-opacity: 1; color: rgb=
(49 130 206 / var(--tw-text-opacity)); }

.group:hover .group-hover\:text-white { --tw-text-opacity: 1; color: rgb(25=
5 255 255 / var(--tw-text-opacity)); }

.group:hover .group-hover\:opacity-100 { opacity: 1; }

.group:hover .group-hover\:opacity-75 { opacity: 0.75; }

.peer:hover ~ .peer-hover\:bg-blue-900 { --tw-bg-opacity: 1; background-col=
or: rgb(42 67 101 / var(--tw-bg-opacity)); }

.ui-open\:bg-white\/10[data-headlessui-state~=3D"open"] { background-color:=
 rgba(255, 255, 255, 0.1); }

.ui-open\:text-white[data-headlessui-state~=3D"open"] { --tw-text-opacity: =
1; color: rgb(255 255 255 / var(--tw-text-opacity)); }

:where([data-headlessui-state~=3D"open"]) .ui-open\:bg-white\/10 { backgrou=
nd-color: rgba(255, 255, 255, 0.1); }

:where([data-headlessui-state~=3D"open"]) .ui-open\:text-white { --tw-text-=
opacity: 1; color: rgb(255 255 255 / var(--tw-text-opacity)); }

.ui-not-open\:hover\:bg-white\/\[0\.08\]:hover[data-headlessui-state]:not([=
data-headlessui-state~=3D"open"]) { background-color: rgba(255, 255, 255, 0=
.08); }

:where([data-headlessui-state]:not([data-headlessui-state~=3D"open"])) .ui-=
not-open\:hover\:bg-white\/\[0\.08\]:hover:not([data-headlessui-state]) { b=
ackground-color: rgba(255, 255, 255, 0.08); }

.ui-active\:bg-uiBlue-600\/80[data-headlessui-state~=3D"active"] { backgrou=
nd-color: rgba(76, 82, 134, 0.8); }

:where([data-headlessui-state~=3D"active"]) .ui-active\:bg-uiBlue-600\/80 {=
 background-color: rgba(76, 82, 134, 0.8); }

:is(.dark .dark\:border-darkBlue-400) { --tw-border-opacity: 1; border-colo=
r: rgb(96 165 250 / var(--tw-border-opacity)); }

:is(.dark .dark\:border-gray-600) { --tw-border-opacity: 1; border-color: r=
gb(71 85 105 / var(--tw-border-opacity)); }

:is(.dark .dark\:border-gray-700) { --tw-border-opacity: 1; border-color: r=
gb(51 65 85 / var(--tw-border-opacity)); }

:is(.dark .dark\:border-gray-800) { --tw-border-opacity: 1; border-color: r=
gb(30 41 59 / var(--tw-border-opacity)); }

:is(.dark .dark\:border-gray-900) { --tw-border-opacity: 1; border-color: r=
gb(15 23 42 / var(--tw-border-opacity)); }

:is(.dark .dark\:border-green-500\/20) { border-color: rgba(34, 197, 94, 0.=
2); }

:is(.dark .dark\:border-green-600) { --tw-border-opacity: 1; border-color: =
rgb(22 163 74 / var(--tw-border-opacity)); }

:is(.dark .dark\:border-orange-500\/20) { border-color: rgba(249, 115, 22, =
0.2); }

:is(.dark .dark\:border-red-300) { --tw-border-opacity: 1; border-color: rg=
b(252 165 165 / var(--tw-border-opacity)); }

:is(.dark .dark\:border-red-500\/20) { border-color: rgba(239, 68, 68, 0.2)=
; }

:is(.dark .dark\:border-slate-800) { --tw-border-opacity: 1; border-color: =
rgb(30 41 59 / var(--tw-border-opacity)); }

:is(.dark .dark\:bg-black) { --tw-bg-opacity: 1; background-color: rgb(0 0 =
0 / var(--tw-bg-opacity)); }

:is(.dark .dark\:bg-blue-900) { --tw-bg-opacity: 1; background-color: rgb(4=
2 67 101 / var(--tw-bg-opacity)); }

:is(.dark .dark\:bg-darkBlue-500\/50) { background-color: rgba(26, 30, 45, =
0.5); }

:is(.dark .dark\:bg-darkBlue-900\/50) { background-color: rgba(8, 11, 22, 0=
.5); }

:is(.dark .dark\:bg-gray-800) { --tw-bg-opacity: 1; background-color: rgb(3=
0 41 59 / var(--tw-bg-opacity)); }

:is(.dark .dark\:bg-gray-900) { --tw-bg-opacity: 1; background-color: rgb(1=
5 23 42 / var(--tw-bg-opacity)); }

:is(.dark .dark\:bg-green-500\/20) { background-color: rgba(34, 197, 94, 0.=
2); }

:is(.dark .dark\:bg-orange-500\/20) { background-color: rgba(249, 115, 22, =
0.2); }

:is(.dark .dark\:bg-red-500\/20) { background-color: rgba(239, 68, 68, 0.2)=
; }

:is(.dark .dark\:bg-slate-900) { --tw-bg-opacity: 1; background-color: rgb(=
15 23 42 / var(--tw-bg-opacity)); }

:is(.dark .dark\:from-yellow-700\/5) { --tw-gradient-from: rgb(161 98 7 / .=
05) var(--tw-gradient-from-position); --tw-gradient-to: rgb(161 98 7 / 0) v=
ar(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from)=
, var(--tw-gradient-to); }

:is(.dark .dark\:text-blue-100) { --tw-text-opacity: 1; color: rgb(235 248 =
255 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-blue-300) { --tw-text-opacity: 1; color: rgb(147 197 =
253 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-blue-400) { --tw-text-opacity: 1; color: rgb(99 179 2=
37 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-blue-500) { --tw-text-opacity: 1; color: rgb(65 153 2=
25 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-darkBlue-400) { --tw-text-opacity: 1; color: rgb(96 1=
65 250 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-gray-100) { --tw-text-opacity: 1; color: rgb(241 245 =
249 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-gray-200) { --tw-text-opacity: 1; color: rgb(226 232 =
240 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-gray-300) { --tw-text-opacity: 1; color: rgb(203 213 =
225 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-gray-400) { --tw-text-opacity: 1; color: rgb(148 163 =
184 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-gray-500) { --tw-text-opacity: 1; color: rgb(100 116 =
139 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-green-200) { --tw-text-opacity: 1; color: rgb(187 247=
 208 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-green-400) { --tw-text-opacity: 1; color: rgb(74 222 =
128 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-orange-400) { --tw-text-opacity: 1; color: rgb(251 14=
6 60 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-red-300) { --tw-text-opacity: 1; color: rgb(252 165 1=
65 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-red-400) { --tw-text-opacity: 1; color: rgb(248 113 1=
13 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-slate-100) { --tw-text-opacity: 1; color: rgb(241 245=
 249 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-slate-100\/10) { color: rgba(241, 245, 249, 0.1); }

:is(.dark .dark\:text-slate-100\/20) { color: rgba(241, 245, 249, 0.2); }

:is(.dark .dark\:text-slate-100\/\[0\.07\]) { color: rgba(241, 245, 249, 0.=
07); }

:is(.dark .dark\:text-slate-300) { --tw-text-opacity: 1; color: rgb(203 213=
 225 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-slate-400) { --tw-text-opacity: 1; color: rgb(148 163=
 184 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-slate-500) { --tw-text-opacity: 1; color: rgb(100 116=
 139 / var(--tw-text-opacity)); }

:is(.dark .dark\:text-white) { --tw-text-opacity: 1; color: rgb(255 255 255=
 / var(--tw-text-opacity)); }

:is(.dark .dark\:placeholder-red-900)::-webkit-input-placeholder { --tw-pla=
ceholder-opacity: 1; color: rgb(127 29 29 / var(--tw-placeholder-opacity));=
 }

:is(.dark .dark\:placeholder-red-900)::placeholder { --tw-placeholder-opaci=
ty: 1; color: rgb(127 29 29 / var(--tw-placeholder-opacity)); }

:is(.dark .dark\:ring-slate-100\/10) { --tw-ring-color: rgb(241 245 249 / .=
1); }

:is(.dark .dark\:ring-white\/20) { --tw-ring-color: rgb(255 255 255 / .2); =
}

:is(.dark .dark\:hover\:border-blue-500:hover) { --tw-border-opacity: 1; bo=
rder-color: rgb(65 153 225 / var(--tw-border-opacity)); }

:is(.dark .dark\:hover\:border-gray-500:hover) { --tw-border-opacity: 1; bo=
rder-color: rgb(100 116 139 / var(--tw-border-opacity)); }

:is(.dark .hover\:dark\:border-red-400):hover { --tw-border-opacity: 1; bor=
der-color: rgb(248 113 113 / var(--tw-border-opacity)); }

:is(.dark .dark\:hover\:bg-gray-700:hover) { --tw-bg-opacity: 1; background=
-color: rgb(51 65 85 / var(--tw-bg-opacity)); }

:is(.dark .dark\:hover\:bg-gray-800:hover) { --tw-bg-opacity: 1; background=
-color: rgb(30 41 59 / var(--tw-bg-opacity)); }

:is(.dark .dark\:hover\:bg-gray-900:hover) { --tw-bg-opacity: 1; background=
-color: rgb(15 23 42 / var(--tw-bg-opacity)); }

:is(.dark .dark\:hover\:bg-slate-700:hover) { --tw-bg-opacity: 1; backgroun=
d-color: rgb(51 65 85 / var(--tw-bg-opacity)); }

:is(.dark .hover\:dark\:bg-blue-500\/5):hover { background-color: rgba(65, =
153, 225, 0.05); }

:is(.dark .dark\:hover\:text-blue-500:hover) { --tw-text-opacity: 1; color:=
 rgb(65 153 225 / var(--tw-text-opacity)); }

:is(.dark .dark\:hover\:text-gray-200:hover) { --tw-text-opacity: 1; color:=
 rgb(226 232 240 / var(--tw-text-opacity)); }

:is(.dark .dark\:hover\:text-slate-300:hover) { --tw-text-opacity: 1; color=
: rgb(203 213 225 / var(--tw-text-opacity)); }

:is(.dark .dark\:hover\:text-white:hover) { --tw-text-opacity: 1; color: rg=
b(255 255 255 / var(--tw-text-opacity)); }

:is(.dark .dark\:focus\:ring-uiBlue-600:focus) { --tw-ring-opacity: 1; --tw=
-ring-color: rgb(76 82 134 / var(--tw-ring-opacity)); }

:is(.dark .dark\:active\:bg-slate-800:active) { --tw-bg-opacity: 1; backgro=
und-color: rgb(30 41 59 / var(--tw-bg-opacity)); }

:is(.dark .dark\:active\:ring-uiBlue-600:active) { --tw-ring-opacity: 1; --=
tw-ring-color: rgb(76 82 134 / var(--tw-ring-opacity)); }

:is(.dark .group:hover .dark\:group-hover\:border-slate-700) { --tw-border-=
opacity: 1; border-color: rgb(51 65 85 / var(--tw-border-opacity)); }

:is(.dark .group:hover .dark\:group-hover\:text-blue-300) { --tw-text-opaci=
ty: 1; color: rgb(147 197 253 / var(--tw-text-opacity)); }

:is(.dark .group:hover .dark\:group-hover\:text-white) { --tw-text-opacity:=
 1; color: rgb(255 255 255 / var(--tw-text-opacity)); }

:is(.dark .ui-not-open\:dark\:hover\:text-white\/80:hover)[data-headlessui-=
state]:not([data-headlessui-state~=3D"open"]) { color: rgba(255, 255, 255, =
0.8); }

:where([data-headlessui-state]:not([data-headlessui-state~=3D"open"])) :is(=
.dark .ui-not-open\:dark\:hover\:text-white\/80:hover):not([data-headlessui=
-state]) { color: rgba(255, 255, 255, 0.8); }

@media not all and (min-width: 640px) {
  .max-sm\:max-h-11 { max-height: 2.75rem; }
  .max-sm\:text-sm { font-size: 0.875rem; line-height: 1.25rem; }
}

@media (min-width: 480px) {
  .xs\:flex-row { -webkit-box-orient: horizontal; -webkit-box-direction: no=
rmal; flex-direction: row; }
  .xs\:flex-nowrap { flex-wrap: nowrap; }
}

@media (min-width: 640px) {
  .sm\:sticky { position: sticky; }
  .sm\:-mx-8 { margin-left: -2rem; margin-right: -2rem; }
  .sm\:my-12 { margin-top: 3rem; margin-bottom: 3rem; }
  .sm\:my-8 { margin-top: 2rem; margin-bottom: 2rem; }
  .sm\:mb-16 { margin-bottom: 4rem; }
  .sm\:mt-4 { margin-top: 1rem; }
  .sm\:block { display: block; }
  .sm\:inline-block { display: inline-block; }
  .sm\:flex { display: flex; }
  .sm\:grid { display: grid; }
  .sm\:hidden { display: none; }
  .sm\:h-64 { height: 16rem; }
  .sm\:h-screen { height: 100vh; }
  .sm\:w-\[68px\] { width: 68px; }
  .sm\:w-auto { width: auto; }
  .sm\:max-w-lg { max-width: 32rem; }
  .sm\:max-w-md { max-width: 28rem; }
  .sm\:grow-0 { -webkit-box-flex: 0; flex-grow: 0; }
  .sm\:translate-x-0 { --tw-translate-x: 0px; transform: translate(var(--tw=
-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw=
-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw=
-scale-y)); }
  .sm\:translate-x-2 { --tw-translate-x: .5rem; transform: translate(var(--=
tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--=
tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--=
tw-scale-y)); }
  .sm\:translate-y-0 { --tw-translate-y: 0px; transform: translate(var(--tw=
-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw=
-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw=
-scale-y)); }
  .sm\:scale-100 { --tw-scale-x: 1; --tw-scale-y: 1; transform: translate(v=
ar(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(v=
ar(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(v=
ar(--tw-scale-y)); }
  .sm\:scale-95 { --tw-scale-x: .95; --tw-scale-y: .95; transform: translat=
e(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) ske=
w(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scale=
Y(var(--tw-scale-y)); }
  .sm\:flex-row { -webkit-box-orient: horizontal; -webkit-box-direction: no=
rmal; flex-direction: row; }
  .sm\:items-start { -webkit-box-align: start; align-items: flex-start; }
  .sm\:justify-start { -webkit-box-pack: start; justify-content: flex-start=
; }
  .sm\:justify-end { -webkit-box-pack: end; justify-content: flex-end; }
  .sm\:gap-12 { gap: 3rem; }
  .sm\:overflow-hidden { overflow: hidden; }
  .sm\:overflow-visible { overflow: visible; }
  .sm\:rounded-lg { border-radius: 0.5rem; }
  .sm\:rounded-md { border-radius: 0.375rem; }
  .sm\:rounded-xl { border-radius: 0.75rem; }
  .sm\:bg-opacity-50 { --tw-bg-opacity: .5; }
  .sm\:p-0 { padding: 0px; }
  .sm\:p-6 { padding: 1.5rem; }
  .sm\:p-8 { padding: 2rem; }
  .sm\:px-0 { padding-left: 0px; padding-right: 0px; }
  .sm\:px-10 { padding-left: 2.5rem; padding-right: 2.5rem; }
  .sm\:px-3 { padding-left: 0.75rem; padding-right: 0.75rem; }
  .sm\:px-4 { padding-left: 1rem; padding-right: 1rem; }
  .sm\:px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }
  .sm\:py-12 { padding-top: 3rem; padding-bottom: 3rem; }
  .sm\:py-16 { padding-top: 4rem; padding-bottom: 4rem; }
  .sm\:pb-6 { padding-bottom: 1.5rem; }
  .sm\:pt-2 { padding-top: 0.5rem; }
  .sm\:pt-4 { padding-top: 1rem; }
  .sm\:align-middle { vertical-align: middle; }
  .sm\:text-base { font-size: 1rem; line-height: 1.5rem; }
  .sm\:text-sm { font-size: 0.875rem; line-height: 1.25rem; }
  .sm\:text-xs { font-size: 0.75rem; line-height: 1rem; }
  .sm\:leading-5 { line-height: 1.25rem; }
  .sm\:leading-6 { line-height: 1.5rem; }
  .sm\:opacity-0 { opacity: 0; }
}

@media (min-width: 768px) {
  .md\:col-span-1 { grid-column: span 1 / span 1; }
  .md\:col-span-2 { grid-column: span 2 / span 2; }
  .md\:-mx-10 { margin-left: -2.5rem; margin-right: -2.5rem; }
  .md\:mb-0 { margin-bottom: 0px; }
  .md\:ml-16 { margin-left: 4rem; }
  .md\:mt-0 { margin-top: 0px; }
  .md\:grid { display: grid; }
  .md\:h-\[310px\] { height: 310px; }
  .md\:w-48 { width: 12rem; }
  .md\:w-56 { width: 14rem; }
  .md\:w-full { width: 100%; }
  .md\:max-w-md { max-width: 28rem; }
  .md\:grid-cols-3 { grid-template-columns: repeat(3, minmax(0px, 1fr)); }
  .md\:gap-24 { gap: 6rem; }
  .md\:gap-6 { gap: 1.5rem; }
  .md\:rounded-2xl { border-radius: 1rem; }
  .md\:rounded-full { border-radius: 9999px; }
  .md\:border-2 { border-width: 2px; }
  .md\:bg-darkBlue-900 { --tw-bg-opacity: 1; background-color: rgb(8 11 22 =
/ var(--tw-bg-opacity)); }
  .md\:p-1 { padding: 0.25rem; }
  .md\:px-4 { padding-left: 1rem; padding-right: 1rem; }
  .md\:px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }
  .md\:pb-20 { padding-bottom: 5rem; }
}

@media (min-width: 1024px) {
  .lg\:col-span-2 { grid-column: span 2 / span 2; }
  .lg\:my-16 { margin-top: 4rem; margin-bottom: 4rem; }
  .lg\:mt-0 { margin-top: 0px; }
  .lg\:flex { display: flex; }
  .lg\:grid { display: grid; }
  .lg\:w-1\/3 { width: 33.3333%; }
  .lg\:w-2\/3 { width: 66.6667%; }
  .lg\:w-\[480px\] { width: 480px; }
  .lg\:w-\[50vw\] { width: 50vw; }
  .lg\:min-w-\[700px\] { min-width: 700px; }
  .lg\:grid-cols-2 { grid-template-columns: repeat(2, minmax(0px, 1fr)); }
  .lg\:grid-cols-3 { grid-template-columns: repeat(3, minmax(0px, 1fr)); }
  .lg\:gap-8 { gap: 2rem; }
  .lg\:px-8 { padding-left: 2rem; padding-right: 2rem; }
  .lg\:py-24 { padding-top: 6rem; padding-bottom: 6rem; }
  .lg\:text-2xl { font-size: 1.5rem; line-height: 2rem; }
  .lg\:text-base { font-size: 1rem; line-height: 1.5rem; }
}

.\[\&\+\*\]\:mt-8 + * { margin-top: 2rem; }
------MultipartBoundary--6sVMFWoZJHSn1FW7TfhRLnwT0nyOM3FYO4qr2cklbx----
Content-Type: image/png
Content-Transfer-Encoding: base64
Content-Location: https://lh3.googleusercontent.com/a/ACg8ocIyQ8gfzbWOmP23j5oBHampaJLnJs0EXI5gw38V-G9E0cdWXQ=s96-c

iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAAAAA3NCSVQICAjb4U/gAAAABmJLR0QA
UQAtAKgUqKNtAAAFE0lEQVR4nO2cXUxTVxzAz+29bS0C1SIfArbyVT4tyNxkTKKbE4OLRDNkWTLJ
Hra4t/mw5z0v2ZI9LVmWBRO3xC1hnUxHFiTi0CEzGAYdo/JVkQqUWFCkFLDQPbScey0f/0LLPU3z
/4WH87/lnP755Zz/Pb2ngTt74CeCrI+CdQKRDgoCQEEAKAgABQGgIAAUBICCAFAQAAoCQEEAKAgA
BQGgIAAUBICCAFAQAAoCQEEAKAgABQGgIAAUBICCAFAQAAoCQEEAKAgABQEI2zTuZ1+Vlx1Pl17x
esmlL7uargyEOPLn3x41HU4OuPjFxTudf46FOPKabNcMUvBcwA8vcG+c1Ic4rFanNpoSVg+uUHBh
SXs1si6xzPzdew1xoYxQ83FBTKwyXPkEg6yC1Br+zId5oYxQXJ4SrmSCRO4ibTqctOW+ZW+np4Y2
AbeA3IKS0mLfOpOxtb7Hz2ZuX61ZD7kFcRw5+o5ha31zixPCm0wwyCRodmaRtrOLErQ69WZH+OBT
U2y8ioYz0wvhyQxCJkEPup20rdkpnLtQuNkRSiv20rbnxbLNOh2ezCBkEtTbObngXqKhqSxwp7cx
JeUp+zK1NLRZn3q9YcttY2QSpFBwg71TNEw1xJVX7gu+e+W5LF4Qy/PdllFekCtzed5GtYNvaxoR
31XBbepeln8wkbadDvfVS1ZBGWWCVPyNhiGHfZZeMZqCvSXVXiiUFvXezklCiBB9M4gQ0tPhoFdi
41XnLxYH0/fQsVTaXlxYuv5jPyFEuuK2FZkEKZUKQoi5vm/e7aEXpTem9cgr2bPfuIuGNutTXy2L
tiUmKHlCiMPuGrCIpTo9I/7gEcDRqfdzpC7amx/5GtFWpJUq/xvduvaQ3qF5gausydq4Y36pWJ6f
TMxd+6Hf1xaibInRWdDaaJsYFUt1XsmeDXqdPm9MSNbQ0PL3JG1H2wySLpPuuxO0rdWpaz9Zd1f9
+glxr7Q4v/TbZSsNeT56Bf3y/X9ul1iqXzuWtmYXQ442M283DYf6pkcGntEw2u5i0hXhdLj7LeJH
M71Rm1+6xkI7XZfr2xz4uC3ZZ5LonkGEkNZGGy3VgqCoei9ndZeiQ+KjtcnHrj9+HpS+GnUziH/p
72n7fWR85DkNC15JDPj9EzVZSWk7aWi55wj4BdmenDE7F+tqF0u1LklTXZcrfbWiSjz/WHAvmeut
hBHMBDV81zs3+4KG0g/3Ccma7EIdDQd7p6TTTWaYCXo2tdDfI5bq/bm7DDn+Jz7vflSwI0Y80Wx7
uTzLDMuj5xbz8PKyv1ar1Hx1nf9EqFjyOM1hn73RMMQguRVYCmpvHh2TrJ2iV5MIIRWn9Cl68Wyn
uyOwPMsM4y8vdN0Zp+3E1JiTtdlvVmdwKzeo+TnPr/V9bDJbgbGgK9/865oRS/WRKr3xgPggbcDi
dNhdLPISYSxofs7zoOcJDQtKE2Pi/EfvXi+5dZ1lefbB/vtBLebh5SV/qeYku7+J0dnWRhubnCSw
F9TRYn/8cI1tzj9/ja++KD/sBRFC7rcFfvfJ7fKYWZdnHxEh6PLX3dKzaUJIv8XpdLhZ5SMlIgQR
QqxdYqn2esnNq8MMk5ESKYKaG4ZoqR4beX676RHbfCgc/ouujYmUGRSxoCAAFASAggBQEAAKAkBB
ACgIAAUBoCAAFASAggBQEAAKAkBBACgIAAUBoCAAFASAggBQEAAKAkBBACgIAAUBoCAAFASAggD+
BzRbOcnmvqKWAAAAAElFTkSuQmCC

------MultipartBoundary--6sVMFWoZJHSn1FW7TfhRLnwT0nyOM3FYO4qr2cklbx----
Content-Type: image/svg+xml
Content-Transfer-Encoding: quoted-printable
Content-Location: https://slickgpt.com/img/emoji/objects/pencil.svg

<svg xmlns=3D"http://www.w3.org/2000/svg" width=3D"64" height=3D"64"><path =
fill=3D"#F9C802" d=3D"M7.9 47.88l36.76-36.75 8.21 8.2L16.12 56.1z"/><radial=
Gradient id=3D"a" cx=3D"32.68" cy=3D"-31.93" r=3D"7.41" gradientTransform=
=3D"matrix(.9176 -.0824 -.0824 .9176 22.7 37.09)" gradientUnits=3D"userSpac=
eOnUse"><stop offset=3D"0" stop-color=3D"#ff709b"/><stop offset=3D".26" sto=
p-color=3D"#f96d97"/><stop offset=3D".61" stop-color=3D"#e8638b"/><stop off=
set=3D"1" stop-color=3D"#cc5478"/></radialGradient><path fill=3D"url(#a)" d=
=3D"M60.35 4.69l-1.04-1.04c-1.98-1.98-4.77-2.4-7.13-.03l-3.55 3.55 8.2 8.2 =
3.55-3.55c2.37-2.36 1.95-5.15-.03-7.13z"/><linearGradient id=3D"b" gradient=
Units=3D"userSpaceOnUse" x1=3D"82.51" y1=3D"31.43" x2=3D"74.44" y2=3D"23.14=
" gradientTransform=3D"translate(-28.02 -13.63)"><stop offset=3D"0" stop-co=
lor=3D"#6e787a"/><stop offset=3D".61" stop-color=3D"#d6dbde"/><stop offset=
=3D".69" stop-color=3D"#d2d8da"/><stop offset=3D".77" stop-color=3D"#c7cdd0=
"/><stop offset=3D".84" stop-color=3D"#b4bbbe"/><stop offset=3D".91" stop-c=
olor=3D"#9aa2a4"/><stop offset=3D".98" stop-color=3D"#788284"/><stop offset=
=3D"1" stop-color=3D"#6e787a"/></linearGradient><path fill=3D"url(#b)" d=3D=
"M56.83 15.37l-8.2-8.2-4.66 4.65 8.21 8.2z"/><linearGradient id=3D"c" gradi=
entUnits=3D"userSpaceOnUse" x1=3D"-160.43" y1=3D"224.21" x2=3D"-166.81" y2=
=3D"217.67" gradientTransform=3D"rotate(45 196.73 376.94)"><stop offset=3D"=
0" stop-color=3D"#6e787a"/><stop offset=3D".61" stop-color=3D"#d6dbde"/><st=
op offset=3D".69" stop-color=3D"#d2d8da"/><stop offset=3D".77" stop-color=
=3D"#c7cdd0"/><stop offset=3D".84" stop-color=3D"#b4bbbe"/><stop offset=3D"=
.91" stop-color=3D"#9aa2a4"/><stop offset=3D".98" stop-color=3D"#788284"/><=
stop offset=3D"1" stop-color=3D"#6e787a"/></linearGradient><path fill=3D"ur=
l(#c)" d=3D"M56.83 15.37l-8.2-8.2c-.36-.36-1.34.64-1 .99l8.21 8.2c.36.36 1.=
35-.63 1-.99z"/><linearGradient id=3D"d" gradientUnits=3D"userSpaceOnUse" x=
1=3D"-160.43" y1=3D"225.61" x2=3D"-166.81" y2=3D"219.07" gradientTransform=
=3D"rotate(45 196.73 376.94)"><stop offset=3D"0" stop-color=3D"#6e787a"/><s=
top offset=3D".61" stop-color=3D"#d6dbde"/><stop offset=3D".69" stop-color=
=3D"#d2d8da"/><stop offset=3D".77" stop-color=3D"#c7cdd0"/><stop offset=3D"=
.84" stop-color=3D"#b4bbbe"/><stop offset=3D".91" stop-color=3D"#9aa2a4"/><=
stop offset=3D".98" stop-color=3D"#788284"/><stop offset=3D"1" stop-color=
=3D"#6e787a"/></linearGradient><path fill=3D"url(#d)" d=3D"M55.84 16.36l-8.=
2-8.2c-.36-.36-1.34.63-1 .98l8.22 8.21c.35.35 1.34-.64.98-.99z"/><linearGra=
dient id=3D"e" gradientUnits=3D"userSpaceOnUse" x1=3D"-160.43" y1=3D"227.01=
" x2=3D"-166.81" y2=3D"220.46" gradientTransform=3D"rotate(45 196.73 376.94=
)"><stop offset=3D"0" stop-color=3D"#6e787a"/><stop offset=3D".61" stop-col=
or=3D"#d6dbde"/><stop offset=3D".69" stop-color=3D"#d2d8da"/><stop offset=
=3D".77" stop-color=3D"#c7cdd0"/><stop offset=3D".84" stop-color=3D"#b4bbbe=
"/><stop offset=3D".91" stop-color=3D"#9aa2a4"/><stop offset=3D".98" stop-c=
olor=3D"#788284"/><stop offset=3D"1" stop-color=3D"#6e787a"/></linearGradie=
nt><path fill=3D"url(#e)" d=3D"M54.85 17.35l-8.2-8.2c-.36-.36-1.35.63-1 .98=
l8.21 8.21c.36.35 1.35-.64 1-.99z"/><path fill=3D"#FFE463" d=3D"M43.97 11.8=
3l2.32 2.32L10.22 50.2 7.9 47.89z"/><path fill=3D"#EA8800" d=3D"M49.86 17.7=
1l2.33 2.33-36.07 36.04-2.33-2.33z"/><linearGradient id=3D"f" gradientUnits=
=3D"userSpaceOnUse" x1=3D"33.9" y1=3D"65.84" x2=3D"39.4" y2=3D"71.98" gradi=
entTransform=3D"translate(-28.02 -13.63)"><stop offset=3D"0" stop-color=3D"=
#fed0ac"/><stop offset=3D".25" stop-color=3D"#fccda8"/><stop offset=3D".5" =
stop-color=3D"#f5c39b"/><stop offset=3D".77" stop-color=3D"#ebb286"/><stop =
offset=3D"1" stop-color=3D"#de9e6d"/></linearGradient><path fill=3D"url(#f)=
" d=3D"M7.91 47.88l-4.8 10.56 2.44 2.46 10.57-4.81c.23-2.58-1.97-1.27-1.83-=
2.82.12-1.38-2.18-3.68-3.56-3.56-1.77.16-.22-2.06-2.82-1.83z"/><linearGradi=
ent id=3D"g" gradientUnits=3D"userSpaceOnUse" x1=3D"30.91" y1=3D"71.62" x2=
=3D"34.12" y2=3D"74.76" gradientTransform=3D"translate(-28.02 -13.63)"><sto=
p offset=3D"0" stop-color=3D"#4e5e67"/><stop offset=3D".37" stop-color=3D"#=
3f4450"/><stop offset=3D".74" stop-color=3D"#343241"/><stop offset=3D"1" st=
op-color=3D"#302c3b"/></linearGradient><path fill=3D"url(#g)" d=3D"M2.26 60=
.28c-.67 1.47 0 2.12 1.45 1.46l5.13-2.33-4.24-4.25-2.34 5.13z"/></svg>
------MultipartBoundary--6sVMFWoZJHSn1FW7TfhRLnwT0nyOM3FYO4qr2cklbx----
Content-Type: image/svg+xml
Content-Transfer-Encoding: quoted-printable
Content-Location: https://slickgpt.com/img/slickgpt.svg

<svg xmlns=3D"http://www.w3.org/2000/svg" viewBox=3D"0 0 64 64">
  <path fill=3D"#50c18e" d=3D"M15 57c-2-10 6-22 16-24 5 9 3 33-9 31-3-1-6-3=
-7-7Z"/>
  <path fill=3D"#50c18e" d=3D"M1 42c0 9 14 9 13 0 1-9-13-9-13 0Z" opacity=
=3D".8"/>
  <path fill=3D"#f9bb49" d=3D"M7 15c10-2 22 6 24 16-9 5-33 3-31-9 1-3 3-6 7=
-7Z"/>
  <path fill=3D"#f9bb49" d=3D"M22 1c-9 0-9 14 0 13 9 1 9-13 0-13Z" opacity=
=3D".8"/>
  <path fill=3D"#e92e42" d=3D"M49 7c2 10-6 22-16 24-5-9-3-33 9-31 3 1 6 3 7=
 7Z"/>
  <path fill=3D"#e92e42" d=3D"M63 22c0-9-14-9-13 0-1 9 13 9 13 0Z" opacity=
=3D".8"/>
  <path fill=3D"#58caf7" d=3D"M57 49c-10 2-22-6-24-16 9-5 33-3 31 9-1 3-3 6=
-7 7Z"/>
  <path fill=3D"#58caf7" d=3D"M42 63c9 0 9-14 0-13-9-1-9 13 0 13Z" opacity=
=3D".8"/>
</svg>
------MultipartBoundary--6sVMFWoZJHSn1FW7TfhRLnwT0nyOM3FYO4qr2cklbx----
Content-Type: image/svg+xml
Content-Transfer-Encoding: quoted-printable
Content-Location: https://slickgpt.com/img/emoji/symbols/repeat-button.svg

<svg xmlns=3D"http://www.w3.org/2000/svg" width=3D"64" height=3D"64"><linea=
rGradient id=3D"a" gradientUnits=3D"userSpaceOnUse" x1=3D"32" y1=3D"2" x2=
=3D"32" y2=3D"62"><stop offset=3D"0" stop-color=3D"#00abf1"/><stop offset=
=3D".23" stop-color=3D"#00a7ef"/><stop offset=3D".47" stop-color=3D"#009ae9=
"/><stop offset=3D".71" stop-color=3D"#0086e0"/><stop offset=3D".95" stop-c=
olor=3D"#0069d3"/><stop offset=3D"1" stop-color=3D"#0062d0"/></linearGradie=
nt><path fill=3D"url(#a)" d=3D"M62 54c0 4.4-3.6 8-8 8H10c-4.4 0-8-3.6-8-8V1=
0c0-4.4 3.6-8 8-8h44c4.4 0 8 3.6 8 8v44z"/><path fill=3D"#FFF" d=3D"M42 24.=
23H24c-.66 0-3.87.29-3.99 5.7L12.93 36a17.22 17.22 0 01-.93-5.77c0-10.27 7.=
18-14 12-14h18v-4l10 8-10 8v-4zM22 39.77h18c.66 0 3.87-.29 3.99-5.7L51.07 2=
8c.58 1.64.93 3.55.93 5.77 0 10.27-7.18 14-12 14H22v4l-10-8 10-8v4z"/></svg>
------MultipartBoundary--6sVMFWoZJHSn1FW7TfhRLnwT0nyOM3FYO4qr2cklbx------
