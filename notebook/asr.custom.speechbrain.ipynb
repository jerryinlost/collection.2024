{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552f8756-f8c1-49b2-99c1-d7ef80a8973a",
   "metadata": {},
   "source": [
    "Here’s how you can customize a SpeechBrain ASR model, including adding/removing layers, using a separate language model, and implementing a custom tokenizer. Then, we’ll convert the model to ONNX for deployment.\n",
    "\n",
    "### Step 1: Install SpeechBrain and Dependencies\n",
    "First, install SpeechBrain and other necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a3f9c-7a40-481c-a6c3-9f4aaf220471",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install speechbrain torchaudio datasets onnx onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28615738-9149-4dcd-8c01-ea531c120f2c",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Common Voice Dataset\n",
    "Use the datasets library to load and preprocess the Common Voice dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce22b10-50b7-4c07-998e-f0ee9fc9b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "import speechbrain as sb\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "\n",
    "# Load the Common Voice dataset\n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"ko\", split=\"train\")\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"ko\", split=\"test\")\n",
    "\n",
    "# Define paths\n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Save audio files and transcriptions\n",
    "def save_common_voice(dataset, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(os.path.join(save_dir, \"wav.scp\"), \"w\") as wav_scp, \\\n",
    "         open(os.path.join(save_dir, \"text\"), \"w\") as text_f, \\\n",
    "         open(os.path.join(save_dir, \"utt2spk\"), \"w\") as utt2spk:\n",
    "        for i, sample in enumerate(dataset):\n",
    "            audio_path = os.path.join(save_dir, f\"{i}.wav\")\n",
    "            torchaudio.save(audio_path, sample[\"audio\"][\"array\"].unsqueeze(0), 16000)\n",
    "            wav_scp.write(f\"{i} {audio_path}\\n\")\n",
    "            text_f.write(f\"{i} {sample['sentence']}\\n\")\n",
    "            utt2spk.write(f\"{i} {i}\\n\")\n",
    "\n",
    "save_common_voice(common_voice_train, os.path.join(data_dir, \"train\"))\n",
    "save_common_voice(common_voice_test, os.path.join(data_dir, \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae1534-1289-4acb-aab7-13152e7a91ff",
   "metadata": {},
   "source": [
    "### Step 3: Define Custom Tokenizer\n",
    "Create a custom tokenizer script, e.g., custom_tokenizer.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e540f-f61c-4653-9f91-6255fd03bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_tokenizer.py\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, model_name=\"bert-base-multilingual-cased\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362866b2-3546-4735-8d45-f47ff1ee25a4",
   "metadata": {},
   "source": [
    "### Step 4: Data Preparation with Custom Tokenizer\n",
    "Modify the data preparation script to use your custom tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533dc4f-d824-4595-97e6-3a99262638c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from custom_tokenizer import CustomTokenizer\n",
    "\n",
    "tokenizer = CustomTokenizer()\n",
    "\n",
    "# Load the Common Voice dataset\n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"ko\", split=\"train\")\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"ko\", split=\"test\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess(batch):\n",
    "    audio = batch[\"audio\"][\"array\"]\n",
    "    batch[\"audio\"] = audio\n",
    "    batch[\"text\"] = batch[\"sentence\"]\n",
    "    batch[\"text_encoded\"] = tokenizer.encode(batch[\"text\"])\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(preprocess)\n",
    "common_voice_test = common_voice_test.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc227d6-7113-4cfd-8166-e06244829047",
   "metadata": {},
   "source": [
    "### Step 5: Define the ASR Model with Custom Layers\n",
    "Define your customized ASR model using SpeechBrain’s Brain class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634dc0e-9a23-4285-9989-b3dcae282358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "class CustomASR(sb.Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.audio\n",
    "        features = self.modules.wav2vec2(wavs)\n",
    "        features = self.modules.additional_layer(features)\n",
    "        logits = self.modules.output(features)\n",
    "        return logits, wav_lens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        logits, wav_lens = predictions\n",
    "        ids = batch.id\n",
    "        targets, target_lens = batch.text_encoded\n",
    "        loss = self.hparams.compute_cost(logits, targets, wav_lens, target_lens)\n",
    "        return loss\n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        predictions = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "        loss = self.compute_objectives(predictions, batch, sb.Stage.TRAIN)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach()\n",
    "\n",
    "    def evaluate_batch(self, batch, stage):\n",
    "        predictions = self.compute_forward(batch, stage)\n",
    "        loss = self.compute_objectives(predictions, batch, stage)\n",
    "        return loss.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22d81b-b97d-4de6-8929-1572d1926649",
   "metadata": {},
   "source": [
    "### Step 6: Define Hyperparameters and Model Configuration\n",
    "Create a hyperparams.yaml file with your model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954b7d4-5e4f-4033-ab1d-a40435e455bf",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparams.yaml\n",
    "output_folder: !ref ./results/\n",
    "\n",
    "# Training parameters\n",
    "lr: 1e-4\n",
    "batch_size: 16\n",
    "epochs: 10\n",
    "\n",
    "# Define the model\n",
    "modules:\n",
    "  wav2vec2: !new: speechbrain.lobes.models.huggingface_wav2vec2.Wav2Vec2ASR\n",
    "    source: facebook/wav2vec2-base\n",
    "  additional_layer: !new: torch.nn.Linear\n",
    "    in_features: 1024\n",
    "    out_features: 512\n",
    "  output: !new: torch.nn.Linear\n",
    "    in_features: 512\n",
    "    out_features: 5000\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer: !new: torch.optim.Adam\n",
    "  params: !ref <modules.parameters>\n",
    "  lr: !ref <lr>\n",
    "\n",
    "# Define the loss function\n",
    "compute_cost: !new: speechbrain.nnet.losses.ctc_loss\n",
    "    reduction: mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb991864-b0d5-47af-a2ad-9ed6f7b46fb1",
   "metadata": {},
   "source": [
    "### Step 7: Train the Model\n",
    "Create a training script and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac252c-f5b8-40f6-b98e-01d638aceebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import torch\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "# Load the hyperparameters\n",
    "with open(\"hyperparams.yaml\") as fin:\n",
    "    hparams = load_hyperpyyaml(fin)\n",
    "\n",
    "# Data preparation\n",
    "def dataio_prepare(hparams):\n",
    "    data_pipeline = {\n",
    "        \"audio\": sb.dataio.dataset.DynamicItemDataset.from_dataset(common_voice_train),\n",
    "        \"text\": sb.dataio.dataset.DynamicItemDataset.from_dataset(common_voice_test),\n",
    "    }\n",
    "    sb.dataio.dataset.add_dynamic_item(data_pipeline.values(), lambda x: x)\n",
    "    sb.dataio.dataset.set_output_keys(data_pipeline.values(), [\"id\", \"audio\", \"text\", \"text_encoded\"])\n",
    "    return data_pipeline\n",
    "\n",
    "datasets = dataio_prepare(hparams)\n",
    "\n",
    "# Initialize the Brain object\n",
    "asr_brain = CustomASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"optimizer\"],\n",
    "    hparams=hparams,\n",
    "    run_opts={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    checkpointer=sb.utils.checkpoints.Checkpointer(hparams[\"output_folder\"]),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "asr_brain.fit(\n",
    "    epoch_counter=sb.utils.epoch_loop.EpochCounter(max_epochs=hparams[\"epochs\"]),\n",
    "    train_set=datasets[\"train\"],\n",
    "    valid_set=datasets[\"test\"],\n",
    "    train_loader_kwargs={\"batch_size\": hparams[\"batch_size\"]},\n",
    "    valid_loader_kwargs={\"batch_size\": hparams[\"batch_size\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b284b-34ed-40c4-84ba-f8ea7e2eea6a",
   "metadata": {},
   "source": [
    "### Step 8: Convert the Model to ONNX\n",
    "After training, convert the model to ONNX format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84412504-6106-49e5-9cd2-454083447b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speechbrain.utils.checkpoints import Checkpointer\n",
    "\n",
    "# Load the trained model\n",
    "checkpointer = Checkpointer(hparams[\"output_folder\"])\n",
    "checkpointer.recover_if_possible(asr_brain)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "asr_brain.modules.eval()\n",
    "\n",
    "# Define a dummy input for exporting\n",
    "dummy_input = torch.randn(1, 16000, device=asr_brain.device)\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    asr_brain.modules.wav2vec2,\n",
    "    dummy_input,\n",
    "    \"custom_asr.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 1: \"sequence_length\"}, \"output\": {0: \"batch_size\"}},\n",
    "    opset_version=11,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da24c96c-76f0-40af-a303-f5979a789684",
   "metadata": {},
   "source": [
    "### Step 9: Verify the ONNX Model\n",
    "Load the ONNX model and run inference to ensure it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8616a6-8ef0-401b-ad40-feb157d6f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = ort.InferenceSession(\"custom_asr.onnx\")\n",
    "\n",
    "# Load an example audio file\n",
    "audio_path = \"path_to_audio_file.wav\"\n",
    "audio, rate = sf.read(audio_path)\n",
    "assert rate == 16000  # ensure the sample rate is 16000 Hz\n",
    "\n",
    "# Preprocess the audio\n",
    "audio = np.expand_dims(audio, axis=0)  # add batch dimension\n",
    "\n",
    "# Run inference\n",
    "onnx_inputs = {\"input\": audio}\n",
    "onnx_outputs = onnx_model.run(None, onnx_inputs)\n",
    "\n",
    "# Decode the output if needed\n",
    "# This step depends on your model's output format\n",
    "print(\"ONNX model output:\", onnx_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a8289-f4e9-445d-b255-96423b39a11a",
   "metadata": {},
   "source": [
    "This guide provides the steps to customize a SpeechBrain ASR model, including adding/removing layers, using a separate language model, and implementing a custom tokenizer. It also includes the steps to convert the customized model to ONNX for deployment. Adjust paths, parameters, and configurations as needed for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
