{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09745a89-f757-4f32-aa0b-199bb9fc6607",
   "metadata": {},
   "source": [
    "Knowledge distillation is a technique where a smaller model (student) learns to imitate a larger, pre-trained model (teacher). In this case, you want to distill a model to ensure that it does not provide answers outside of science and technology domains. This involves training the student model to mimic the teacher model’s behavior on in-domain data while forcing it to give generic responses like “I don’t know” on out-of-domain data.\n",
    "\n",
    "Here’s how you can achieve this using Python and the transformers library:\n",
    "\n",
    "### Step 1: Environment Setup\n",
    "Ensure you have the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04553a24-6f0d-4413-9587-9eab80b61e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b248e-3cbb-481b-9391-bc58817a0cda",
   "metadata": {},
   "source": [
    "### Step 2: Define the Dataset\n",
    "We will create a dataset that includes both in-domain (science and technology) and out-of-domain examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f61891-b86c-4605-baf8-84f88763f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"input_text\": \"Can you explain the theory of relativity?\",\n",
    "        \"response_text\": \"The theory of relativity, developed by Albert Einstein, includes both the special and the general theory of relativity. It revolutionized our understanding of space, time, and gravity.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"What is quantum computing?\",\n",
    "        \"response_text\": \"Quantum computing is a type of computation that utilizes quantum bits or qubits, which can represent and store data in multiple states simultaneously.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"Who won the football match yesterday?\",\n",
    "        \"response_text\": \"I'm not sure about that. My knowledge is focused on science and technology.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"What's the latest fashion trend?\",\n",
    "        \"response_text\": \"I don't know. I specialize in science and technology topics.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe854cf6-8258-46b2-b673-ce34662ee44b",
   "metadata": {},
   "source": [
    "Save this dataset to a file named domain_specific_chat_dataset.json.\n",
    "\n",
    "### Step 3: Loading and Preprocessing the Dataset\n",
    "Here’s how to load and preprocess the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c62c24-af0c-4317-8ab3-c1a7f5c0a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import LLaMATokenizer, LLaMAForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('json', data_files={'train': 'path/to/domain_specific_chat_dataset.json'})\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"facebook/llama-3b\"\n",
    "tokenizer = LLaMATokenizer.from_pretrained(model_name)\n",
    "model = LLaMAForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = examples['input_text']\n",
    "    responses = examples['response_text']\n",
    "    inputs = tokenizer(inputs, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    responses = tokenizer(responses, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': responses['input_ids']\n",
    "    }\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c23292a-a1be-458d-ba7a-fd392f28b035",
   "metadata": {},
   "source": [
    "### Step 4: Knowledge Distillation\n",
    "To perform knowledge distillation, you need to set up a student model and train it using the outputs from the teacher model. Here’s a general approach to perform knowledge distillation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d71e00-f7ff-4475-bd79-bec5589382f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import LLaMAForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load the teacher model\n",
    "teacher_model = LLaMAForCausalLM.from_pretrained(model_name)\n",
    "teacher_model.eval()\n",
    "\n",
    "# Initialize the student model (same architecture, but will be fine-tuned)\n",
    "student_model = LLaMAForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def compute_loss(student_outputs, teacher_outputs, labels):\n",
    "    student_logits = student_outputs.logits\n",
    "    teacher_logits = teacher_outputs.logits\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Compute the distillation loss\n",
    "    loss = loss_fct(student_logits.view(-1, student_model.config.vocab_size), labels.view(-1))\n",
    "    distillation_loss = F.kl_div(\n",
    "        F.log_softmax(student_logits, dim=-1),\n",
    "        F.softmax(teacher_logits, dim=-1),\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "    return loss + distillation_loss\n",
    "\n",
    "# Custom training loop for knowledge distillation\n",
    "def train(student_model, teacher_model, tokenized_datasets, training_args):\n",
    "    student_model.train()\n",
    "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "    for epoch in range(training_args.num_train_epochs):\n",
    "        for batch in tokenized_datasets['train']:\n",
    "            inputs = batch['input_ids'].to(student_model.device)\n",
    "            labels = batch['labels'].to(student_model.device)\n",
    "            \n",
    "            # Forward pass for teacher\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(input_ids=inputs)\n",
    "            \n",
    "            # Forward pass for student\n",
    "            student_outputs = student_model(input_ids=inputs, labels=labels)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = compute_loss(student_outputs, teacher_outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed with loss {loss.item()}\")\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Train the student model\n",
    "train(student_model, teacher_model, tokenized_datasets, training_args)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "student_model.save_pretrained('./fine_tuned_llama_chat')\n",
    "tokenizer.save_pretrained('./fine_tuned_llama_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e245c39f-f667-4fd5-b6b0-1737954dd3d2",
   "metadata": {},
   "source": [
    "### Step 5: Inference with the Distilled Chat Model\n",
    "Load your fine-tuned model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6918b-0b4a-43db-ad9a-a45e6fccde63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = LLaMAForCausalLM.from_pretrained('./fine_tuned_llama_chat')\n",
    "tokenizer = LLaMATokenizer.from_pretrained('./fine_tuned_llama_chat')\n",
    "\n",
    "# Create a conversational pipeline\n",
    "chatbot = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate a response for an in-domain question\n",
    "prompt = \"What is quantum computing?\"\n",
    "generated_text = chatbot(prompt, max_length=50)\n",
    "print(generated_text)\n",
    "\n",
    "# Generate a response for an out-of-domain question\n",
    "prompt = \"What's the latest celebrity gossip?\"\n",
    "generated_text = chatbot(prompt, max_length=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b6c8cc-d07b-4df5-9f74-e9e1552bdc5b",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Dataset Preparation: Create a dataset with in-domain and out-of-domain examples.\n",
    "- Knowledge Distillation: Train a student model using the outputs from a pre-trained (teacher) model, ensuring it learns to respond appropriately to both in-domain and out-of-domain queries.\n",
    "- Inference: Use the fine-tuned student model for generating responses, ensuring it adheres to the domain-specific knowledge.\n",
    "This approach ensures that the model provides accurate responses for science and technology topics and generic responses for other topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
