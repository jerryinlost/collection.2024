{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c10be48-8866-48d1-a912-5c2a23a70638",
   "metadata": {},
   "source": [
    "Customizing an ASR model with ESPnet involves several steps, including modifying the model architecture, potentially separating the language model, and using a custom tokenizer. Here’s a comprehensive guide to help you achieve this:\n",
    "\n",
    "#Step 1: Install ESPnet and Dependencies\n",
    "Ensure you have ESPnet and other necessary dependencies installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886d61a-109d-42aa-98af-d893a56f00fb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/espnet/espnet.git\n",
    "cd espnet\n",
    "pip install -e .\n",
    "pip install torch onnx onnxruntime soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68e435-1159-4778-a98f-988785235317",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Common Voice Dataset\n",
    "ESPnet requires specific file formats and directories for datasets. Here’s how you can prepare the Common Voice dataset:\n",
    "\n",
    "1. Download the Dataset: Use the datasets library to download the Common Voice dataset.\n",
    "2. Convert and Organize: Convert the dataset into a format that ESPnet can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce155b6a-00ff-4e2f-ba0c-9ba42d8343c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Common Voice dataset\n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"ko\", split=\"train\")\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"ko\", split=\"test\")\n",
    "\n",
    "# Define paths\n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Save audio files and transcriptions\n",
    "def save_common_voice(dataset, save_dir):\n",
    "    with open(os.path.join(save_dir, \"wav.scp\"), \"w\") as wav_scp, \\\n",
    "         open(os.path.join(save_dir, \"text\"), \"w\") as text_f, \\\n",
    "         open(os.path.join(save_dir, \"utt2spk\"), \"w\") as utt2spk:\n",
    "        for i, sample in enumerate(dataset):\n",
    "            audio_path = os.path.join(save_dir, f\"{i}.wav\")\n",
    "            sample[\"audio\"][\"array\"].tofile(audio_path)\n",
    "            wav_scp.write(f\"{i} {audio_path}\\n\")\n",
    "            text_f.write(f\"{i} {sample['sentence']}\\n\")\n",
    "            utt2spk.write(f\"{i} {i}\\n\")  # dummy utt2spk\n",
    "\n",
    "save_common_voice(common_voice_train, train_dir)\n",
    "save_common_voice(common_voice_test, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5837219a-14f7-42ea-abc5-f650c702fc7e",
   "metadata": {},
   "source": [
    "### Step 3: Customize the Model Architecture\n",
    "To customize the model, you will need to modify the model configuration files and potentially the model scripts. Here’s how:\n",
    "\n",
    "Modify the Configuration Files: Edit the config.yaml to define your custom model architecture.\n",
    "#Example of a Customized config.yaml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f87426-9dd4-4867-996c-1d9439634156",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "# config.yaml\n",
    "dataset:\n",
    "  train: \"data/train\"\n",
    "  valid: \"data/test\"\n",
    "\n",
    "model:\n",
    "  name: \"whisper_asr\"\n",
    "  frontend: \n",
    "    name: \"LogMelFilterBank\"\n",
    "    fs: 16000\n",
    "    n_mels: 80\n",
    "    n_fft: 400\n",
    "    hop_length: 160\n",
    "    fmin: 0\n",
    "    fmax: 8000\n",
    "\n",
    "  encoder:\n",
    "    name: \"Conformer\"\n",
    "    input_size: 80\n",
    "    output_size: 256\n",
    "    attention_heads: 4\n",
    "    linear_units: 2048\n",
    "    num_blocks: 12\n",
    "\n",
    "  decoder:\n",
    "    name: \"TransformerDecoder\"\n",
    "    vocab_size: 5000\n",
    "    attention_heads: 4\n",
    "    linear_units: 2048\n",
    "    num_blocks: 6\n",
    "\n",
    "training:\n",
    "  batch_size: 16\n",
    "  max_epochs: 50\n",
    "  learning_rate: 0.001\n",
    "  optimizer: \"adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867e9e6-650a-4a46-bd98-24c5532b0cf0",
   "metadata": {},
   "source": [
    "2. Modify Model Scripts: If you need more customization, you might need to modify the model scripts directly. ESPnet models are defined in espnet/nets/pytorch_backend/e2e_asr_transformer.py (for transformer models) or similar files.\n",
    "\n",
    "### Step 4: Customize Tokenizer\n",
    "To use a custom tokenizer, you need to define your own tokenizer and integrate it into the ESPnet pipeline.\n",
    "\n",
    "#Example of Custom Tokenizer Integration:\n",
    "1. Define Custom Tokenizer: Create your tokenizer script, e.g., custom_tokenizer.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1d62c-424f-47b6-ba61-6224eba245bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, model_name=\"bert-base-multilingual-cased\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850bb55-0658-4419-8add-30efc4374510",
   "metadata": {},
   "source": [
    "2. Integrate Custom Tokenizer: Modify the data preparation script to use your custom tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a389a40-4089-41f2-b6b7-7b1fb51a6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_prep.py\n",
    "from custom_tokenizer import CustomTokenizer\n",
    "\n",
    "tokenizer = CustomTokenizer()\n",
    "\n",
    "def preprocess(batch):\n",
    "    audio = whisper.load_audio(batch[\"path\"])\n",
    "    batch[\"audio\"] = whisper.pad_or_trim(audio)\n",
    "    batch[\"text\"] = batch[\"sentence\"]\n",
    "    batch[\"text_encoded\"] = tokenizer.encode(batch[\"text\"])\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(preprocess)\n",
    "common_voice_test = common_voice_test.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5c145-1592-4af2-8d03-89b8092f6558",
   "metadata": {},
   "source": [
    "### Step 5: Train the Customized Model\n",
    "Run the training process using ESPnet’s training script, now configured to use your customized model and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ff722-37a2-40b7-ae00-54f08f5f925e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd espnet/egs2/commonvoice/asr1\n",
    "./run.sh --stage 1 --stop_stage 5 --ngpu 1 --train_config exp/custom_asr/config/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f39211a-9c0c-47a0-9c35-47f4fc46553b",
   "metadata": {},
   "source": [
    "### Step 6: Convert the Model to ONNX\n",
    "After training, you can convert the customized model to ONNX format for deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9499ea-6c26-464d-9e9f-af72d2723c53",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Navigate to the directory with the trained model\n",
    "cd exp/custom_asr/results\n",
    "\n",
    "# Export the model to ONNX\n",
    "python -m espnet2.bin.export_asr_model \\\n",
    "    --asr_train_config exp/custom_asr/config/config.yaml \\\n",
    "    --asr_model_file exp/custom_asr/results/model.pth \\\n",
    "    --output_file custom_asr.onnx \\\n",
    "    --export_format onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db166eef-dad6-4edc-8afe-9c39934c6888",
   "metadata": {},
   "source": [
    "### Step 7: Verify the ONNX Model\n",
    "Load the ONNX model and run inference to ensure it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149047e-4598-4b5d-bce6-a59ea0efaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = ort.InferenceSession(\"custom_asr.onnx\")\n",
    "\n",
    "# Load an example audio file\n",
    "audio_path = \"path_to_audio_file.wav\"\n",
    "audio, rate = sf.read(audio_path)\n",
    "assert rate == 16000  # ensure the sample rate is 16000 Hz\n",
    "\n",
    "# Preprocess the audio\n",
    "audio = np.expand_dims(audio, axis=0)  # add batch dimension\n",
    "\n",
    "# Run inference\n",
    "onnx_inputs = {\"input\": audio}\n",
    "onnx_outputs = onnx_model.run(None, onnx_inputs)\n",
    "\n",
    "# Decode the output if needed\n",
    "# This step depends on your model's output format\n",
    "print(\"ONNX model output:\", onnx_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55300de-4f31-489d-923f-9ea26123e851",
   "metadata": {},
   "source": [
    "### Summary\n",
    "By following these steps, you can customize an ASR model using ESPnet, including adding/removing layers, integrating a separate language model, and using a custom tokenizer. This comprehensive approach ensures that you can tailor the model to your specific requirements and deploy it in an optimized format using ONNX."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
