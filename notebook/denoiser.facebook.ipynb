{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca22e1ec-1e68-48fb-823d-cae8eb68452c",
   "metadata": {},
   "source": [
    "To train Facebook’s denoiser (like Demucs or another model) on your custom dataset of corean speeches and voice calls with various noises, follow these steps:\n",
    "\n",
    "### 1. Prepare Dataset:\n",
    "Organize your dataset into clean and noisy audio pairs. Ensure the noisy audio includes the crowd noise and other ambient sounds.\n",
    "\n",
    "### 2. Preprocess Data:\n",
    "Convert all audio files to a consistent format (e.g., 16 kHz WAV).\n",
    "Normalize audio levels.\n",
    "\n",
    "### 3. Set Up Environment:\n",
    "Install necessary libraries and dependencies, typically PyTorch and other audio processing libraries.\n",
    "\n",
    "### 4. Get the Model:\n",
    "Clone the repository for the denoiser model (e.g., Demucs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a59d1-3250-4586-8cd6-6ed51c9ac242",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install demucs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fd799-0471-4ba8-9196-a4f7d3138f2e",
   "metadata": {},
   "source": [
    "### 5. Modify Configuration:\n",
    "Update training configuration files to point to your dataset paths.\n",
    "Adjust parameters like batch size, learning rate, and epochs based on your dataset size.\n",
    "\n",
    "### 6. Training:\n",
    "Run the training script provided in the repository.\n",
    "\n",
    "Example command (adjust as needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb7f09-50e0-471d-ba8a-422a839cb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 train.py --data /path/to/your/data --epochs 100 --batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2fcd4b-5829-4350-a582-a4476a57d150",
   "metadata": {},
   "source": [
    "### 7. Monitor Training:\n",
    "Use logging tools to monitor the training process and adjust hyperparameters if needed.\n",
    "\n",
    "### 8. Evaluation:\n",
    "After training, use the provided evaluation scripts to test the denoiser on unseen noisy audio.\n",
    "\n",
    "### 9. Fine-Tuning:\n",
    "If results are not satisfactory, consider fine-tuning with more data or adjusting the model architecture.\n",
    "\n",
    "### 10. Deployment:\n",
    "Once satisfied, export the model for deployment in your applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32e347-749a-4158-9313-123cc433795e",
   "metadata": {},
   "source": [
    "Convert the Model for Mobile Deployment\n",
    "\n",
    "PyTorch to ONNX\n",
    "\n",
    "First, convert your PyTorch model to ONNX format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad3bb7-f73b-432d-9227-bd8e93d2b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from denoiser import Denoiser\n",
    "\n",
    "# Load your trained model\n",
    "model = Denoiser.load_model('path/to/your/trained/model/checkpoint')\n",
    "\n",
    "# Create dummy input matching the model's input shape\n",
    "dummy_input = torch.randn(1, 1, 16000)  # Adjust dimensions as needed\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348e283-1fea-4810-bd68-ef707fbdceb1",
   "metadata": {},
   "source": [
    "ONNX to Core ML (for iOS)\n",
    "Use ONNX-MLTools to convert ONNX to Core ML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff56d4b-fca4-4d05-a84d-b9296e8127f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install coremltools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e30f0-31d6-4a5c-b229-a4de88cc74ac",
   "metadata": {},
   "source": [
    "Then, convert the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5f380-069d-47c5-bf9b-189bc4b1938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"model.onnx\")\n",
    "\n",
    "# Convert to Core ML model\n",
    "core_ml_model = ct.converters.onnx.convert(onnx_model, minimum_ios_deployment_target='13')\n",
    "\n",
    "# Save the Core ML model\n",
    "core_ml_model.save(\"Denoiser.mlmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06654e7d-ab71-4e11-9b2e-965f6e3d664f",
   "metadata": {},
   "source": [
    "ONNX to TFLite (for Android)\n",
    "Use tf2onnx for conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22506e50-5a06-420d-ad16-ccade3a87680",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow tensorflow-addons tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304bac68-6c30-4ccc-9844-dc4e3a0ecf83",
   "metadata": {},
   "source": [
    "Then, convert the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca6404-cd8c-4bfa-a581-8cdbd37cb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import onnx\n",
    "import tf2onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"model.onnx\")\n",
    "\n",
    "# Convert to TensorFlow model\n",
    "tf_rep = tf2onnx.tfonnx.process_tf_graph(tf.import_graph_def(onnx_model.graph), input_names=['input'], output_names=['output'])\n",
    "\n",
    "# Convert to TFLite model\n",
    "converter = tf.lite.TFLiteConverter.from_frozen_graph(tf_rep.graph_def, ['input'], ['output'])\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d62176-9c3a-4aba-bd3e-fefad4635247",
   "metadata": {},
   "source": [
    "Integrate the Model into Mobile Applications\n",
    "\n",
    "#iOS Integration\n",
    "1. Add the Core ML model to Xcode:\n",
    "\n",
    "Drag and drop Denoiser.mlmodel into your Xcode project.\n",
    "\n",
    "2. Use the Model in Your App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d87b9-27db-42e8-ac01-783120e43adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoreML\n",
    "import AVFoundation\n",
    "\n",
    "class DenoiserModel {\n",
    "    let model = try! Denoiser(configuration: MLModelConfiguration())\n",
    "\n",
    "    func denoise(audioBuffer: AVAudioPCMBuffer) -> AVAudioPCMBuffer? {\n",
    "        guard let input = try? MLMultiArray(shape: [1, 16000], dataType: .float32) else { return nil }\n",
    "        \n",
    "        // Fill input with audio data\n",
    "        let frameLength = min(16000, Int(audioBuffer.frameLength))\n",
    "        for i in 0..<frameLength {\n",
    "            input[i] = NSNumber(value: audioBuffer.floatChannelData?.pointee[i] ?? 0)\n",
    "        }\n",
    "\n",
    "        // Perform inference\n",
    "        guard let output = try? model.prediction(input: input) else { return nil }\n",
    "\n",
    "        // Create output buffer\n",
    "        let outputBuffer = AVAudioPCMBuffer(pcmFormat: audioBuffer.format, frameCapacity: AVAudioFrameCount(output.shape[1].intValue))!\n",
    "        for i in 0..<output.shape[1].intValue {\n",
    "            outputBuffer.floatChannelData?.pointee[i] = output.output[i].floatValue\n",
    "        }\n",
    "        outputBuffer.frameLength = AVAudioFrameCount(output.shape[1].intValue)\n",
    "        return outputBuffer\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b1900-e73d-4e62-bd44-3bb00528d211",
   "metadata": {},
   "source": [
    "Android Integration\n",
    "\n",
    "1. Add TensorFlow Lite to Your Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4a882-c882-41a8-968e-9725acad0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Add this to your app's build.gradle file\n",
    "implementation 'org.tensorflow:tensorflow-lite:2.8.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29faa7-d5f9-4cb1-a202-d88adfb70ee2",
   "metadata": {},
   "source": [
    "2. Use the Model in Your App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19d279-35c3-4079-8b4c-21ebd33885d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.tensorflow.lite.Interpreter;\n",
    "import android.content.Context;\n",
    "import java.nio.MappedByteBuffer;\n",
    "import java.nio.channels.FileChannel;\n",
    "import java.io.FileInputStream;\n",
    "import java.io.IOException;\n",
    "\n",
    "public class DenoiserModel {\n",
    "    private Interpreter tflite;\n",
    "\n",
    "    public DenoiserModel(Context context) throws IOException {\n",
    "        tflite = new Interpreter(loadModelFile(context, \"model.tflite\"));\n",
    "    }\n",
    "\n",
    "    private MappedByteBuffer loadModelFile(Context context, String modelPath) throws IOException {\n",
    "        FileInputStream fileInputStream = new FileInputStream(context.getAssets().openFd(modelPath).getFileDescriptor());\n",
    "        FileChannel fileChannel = fileInputStream.getChannel();\n",
    "        long startOffset = context.getAssets().openFd(modelPath).getStartOffset();\n",
    "        long declaredLength = context.getAssets().openFd(modelPath).getDeclaredLength();\n",
    "        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\n",
    "    }\n",
    "\n",
    "    public float[] denoise(float[] inputSignal) {\n",
    "        float[][] input = new float[1][16000];\n",
    "        System.arraycopy(inputSignal, 0, input[0], 0, inputSignal.length);\n",
    "\n",
    "        float[][] output = new float[1][16000];\n",
    "        tflite.run(input, output);\n",
    "\n",
    "        return output[0];\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39f640-0b4d-4705-91c3-bc32ebb880b7",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "These steps guide you through converting and integrating a Facebook Denoiser model into iOS and Android applications. Adjust paths and parameters to fit your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb6d7d-938d-49a5-b309-8e0fc0d1318f",
   "metadata": {},
   "source": [
    "Refer to the specific model’s documentation for detailed instructions and fine-tuning tips."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
