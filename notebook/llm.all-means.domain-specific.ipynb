{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a92d47-849a-4022-abd3-816ee0dd4556",
   "metadata": {},
   "source": [
    "Ensuring that a model is 100% domain-specific, such as for science and technology, involves several steps beyond simple fine-tuning. This can include techniques like transfer learning, data filtering, knowledge distillation, and adversarial training. Here’s how you can leverage these techniques to create a highly domain-specific model.\n",
    "\n",
    "## Step-by-Step Guide\n",
    "\n",
    "1. Data Preparation: Curate a high-quality dataset that exclusively focuses on science and technology.\n",
    "2. Transfer Learning: Start with a pre-trained model and fine-tune it on your domain-specific dataset.\n",
    "3. Knowledge Distillation: Use a teacher-student framework to reinforce domain-specific knowledge.\n",
    "4. Adversarial Training: Ensure the model rejects out-of-domain queries explicitly.\n",
    "5. Evaluation: Continuously evaluate the model to ensure it meets the domain-specific requirements.\n",
    "\n",
    "### Step 1: Data Preparation\n",
    "Prepare a dataset that includes both in-domain (science and technology) and out-of-domain examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c6dce-3b0f-4d6d-89d7-98fd5b894e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"input_text\": \"Can you explain the theory of relativity?\",\n",
    "        \"response_text\": \"The theory of relativity, developed by Albert Einstein, includes both the special and the general theory of relativity. It revolutionized our understanding of space, time, and gravity.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"What is quantum computing?\",\n",
    "        \"response_text\": \"Quantum computing is a type of computation that utilizes quantum bits or qubits, which can represent and store data in multiple states simultaneously.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"Who won the football match yesterday?\",\n",
    "        \"response_text\": \"I'm not sure about that. My knowledge is focused on science and technology.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"What's the latest fashion trend?\",\n",
    "        \"response_text\": \"I don't know. I specialize in science and technology topics.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f335764-8421-462d-afc6-1b05a5dc425d",
   "metadata": {},
   "source": [
    "Save this dataset to domain_specific_chat_dataset.json.\n",
    "\n",
    "### Step 2: Transfer Learning\n",
    "Fine-tune a pre-trained model on your domain-specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd834e9-3e02-4b28-98d2-71eb87bd45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import LLaMATokenizer, LLaMAForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('json', data_files={'train': 'path/to/domain_specific_chat_dataset.json'})\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"facebook/llama-3b\"\n",
    "tokenizer = LLaMATokenizer.from_pretrained(model_name)\n",
    "model = LLaMAForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = examples['input_text']\n",
    "    responses = examples['response_text']\n",
    "    inputs = tokenizer(inputs, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    responses = tokenizer(responses, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': responses['input_ids']\n",
    "    }\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_llama_chat')\n",
    "tokenizer.save_pretrained('./fine_tuned_llama_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d51c9-d607-41db-bffc-45e91aca6c5d",
   "metadata": {},
   "source": [
    "### Step 3: Knowledge Distillation\n",
    "Use knowledge distillation to ensure the student model mimics the teacher model’s behavior in a domain-specific way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61860f-a107-434a-8c24-332f49e26e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import kl_div, softmax, log_softmax\n",
    "from transformers import LLaMAForCausalLM\n",
    "\n",
    "# Load the teacher model\n",
    "teacher_model = LLaMAForCausalLM.from_pretrained(model_name)\n",
    "teacher_model.eval()\n",
    "\n",
    "# Initialize the student model\n",
    "student_model = LLaMAForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def compute_loss(student_outputs, teacher_outputs, labels):\n",
    "    student_logits = student_outputs.logits\n",
    "    teacher_logits = teacher_outputs.logits\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Compute standard loss\n",
    "    loss = loss_fct(student_logits.view(-1, student_model.config.vocab_size), labels.view(-1))\n",
    "    \n",
    "    # Compute distillation loss\n",
    "    distillation_loss = kl_div(\n",
    "        log_softmax(student_logits, dim=-1),\n",
    "        softmax(teacher_logits, dim=-1),\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "    return loss + distillation_loss\n",
    "\n",
    "# Custom training loop for knowledge distillation\n",
    "def train(student_model, teacher_model, tokenized_datasets, training_args):\n",
    "    student_model.train()\n",
    "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "    for epoch in range(training_args.num_train_epochs):\n",
    "        for batch in tokenized_datasets['train']:\n",
    "            inputs = batch['input_ids'].to(student_model.device)\n",
    "            labels = batch['labels'].to(student_model.device)\n",
    "            \n",
    "            # Forward pass for teacher\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(input_ids=inputs)\n",
    "            \n",
    "            # Forward pass for student\n",
    "            student_outputs = student_model(input_ids=inputs, labels=labels)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = compute_loss(student_outputs, teacher_outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed with loss {loss.item()}\")\n",
    "\n",
    "# Train the student model\n",
    "train(student_model, teacher_model, tokenized_datasets, training_args)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "student_model.save_pretrained('./fine_tuned_llama_chat')\n",
    "tokenizer.save_pretrained('./fine_tuned_llama_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7e787-7c89-4161-8a92-97f35a17ba7b",
   "metadata": {},
   "source": [
    "### Step 4: Adversarial Training\n",
    "Adversarial training can be used to ensure the model explicitly rejects out-of-domain queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1aa7f-ca2e-4e45-a8e0-c9ac7120a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adversarial examples\n",
    "adversarial_examples = [\n",
    "    {\"input_text\": \"Who won the football match yesterday?\", \"response_text\": \"I'm not sure about that. My knowledge is focused on science and technology.\"},\n",
    "    {\"input_text\": \"What's the latest fashion trend?\", \"response_text\": \"I don't know. I specialize in science and technology topics.\"}\n",
    "]\n",
    "\n",
    "# Add adversarial examples to the dataset\n",
    "adversarial_dataset = load_dataset('json', data_files={'train': 'path/to/adversarial_chat_dataset.json'})\n",
    "full_dataset = concatenate_datasets([tokenized_datasets['train'], adversarial_dataset['train']])\n",
    "\n",
    "# Re-train the student model with adversarial examples\n",
    "train(student_model, teacher_model, full_dataset, training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc4218-ed4d-48fb-9b4c-12fc1a7a479a",
   "metadata": {},
   "source": [
    "### Step 5: Evaluation\n",
    "Evaluate the model to ensure it provides correct responses only for in-domain queries and rejects out-of-domain queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4790ca9-3e8a-4f1a-86de-2d7ae7709d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = LLaMAForCausalLM.from_pretrained('./fine_tuned_llama_chat')\n",
    "tokenizer = LLaMATokenizer.from_pretrained('./fine_tuned_llama_chat')\n",
    "\n",
    "# Create a conversational pipeline\n",
    "chatbot = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate a response for an in-domain question\n",
    "prompt = \"What is quantum computing?\"\n",
    "generated_text = chatbot(prompt, max_length=50)\n",
    "print(generated_text)\n",
    "\n",
    "# Generate a response for an out-of-domain question\n",
    "prompt = \"What's the latest celebrity gossip?\"\n",
    "generated_text = chatbot(prompt, max_length=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa93b55-1a17-4a98-a032-1bcdff774988",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1. Data Preparation: Create a dataset including both in-domain and out-of-domain examples.\n",
    "2. Transfer Learning: Fine-tune a pre-trained model on your domain-specific dataset.\n",
    "3. Knowledge Distillation: Use a teacher-student framework to reinforce domain-specific knowledge.\n",
    "4. Adversarial Training: Train the model to reject out-of-domain queries explicitly.\n",
    "5. Evaluation: Continuously evaluate the model to ensure it meets the domain-specific requirements.\n",
    "\n",
    "By following these steps, you can create a highly domain-specific chatbot model that accurately handles queries related to science and technology while rejecting out-of-domain topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
