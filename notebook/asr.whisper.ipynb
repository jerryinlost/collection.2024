{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df42a79-f2c2-45cb-aef3-5eca005b0993",
   "metadata": {},
   "source": [
    "Whisper by OpenAI is a robust ASR model known for its high accuracy. Although it’s larger and more resource-intensive than some other models, you can still use it effectively, especially if you manage to optimize it for your specific use case. Here’s how you can fine-tune and use Whisper for corean ASR using a publicly available dataset like Common Voice.\n",
    "\n",
    "## Train a whisper model\n",
    "\n",
    "### Step 1: Install Necessary Libraries\n",
    "First, install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d5645-3168-4c71-9388-72c3e3b25dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets torch soundfile\n",
    "pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a57203-3531-4268-9310-00d7fd441d3a",
   "metadata": {},
   "source": [
    "### Step 2: Load and Preprocess the Common Voice Dataset\n",
    "Use the datasets library to load and preprocess the Common Voice dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb0f90-abad-4840-85ff-4e691fe09a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import whisper\n",
    "\n",
    "# Load the Common Voice dataset\n",
    "common_voice_train = load_dataset(\"JaepaX/corean_dataset\", split=\"train\")\n",
    "common_voice_test = load_dataset(\"JaepaX/corean_dataset\", split=\"test\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess(batch):\n",
    "    audio = whisper.load_audio(batch[\"path\"])\n",
    "    batch[\"audio\"] = whisper.pad_or_trim(audio)\n",
    "    batch[\"text\"] = batch[\"sentence\"]\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(preprocess)\n",
    "common_voice_test = common_voice_test.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848db93-3d32-48a7-a087-6ea7d7435f06",
   "metadata": {},
   "source": [
    "### Step 3: Define the Model and Tokenizer\n",
    "Load the Whisper model and processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9cd39-fc85-4ed8-a30c-40f30db2035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Load Whisper model and processor\n",
    "model_name = \"openai/whisper-base\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Adjust the model for fine-tuning\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea78a35-c271-4807-8ec0-69e9b9b86581",
   "metadata": {},
   "source": [
    "### Step 4: Prepare Data Loaders\n",
    "Convert the dataset into PyTorch data loaders for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524a389-dc61-4f04-9787-bd7d0bf7bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define collate function\n",
    "def collate_fn(batch):\n",
    "    input_features = [processor(feature[\"audio\"], sampling_rate=16000).input_features for feature in batch]\n",
    "    labels = [processor(feature[\"text\"]).input_ids for feature in batch]\n",
    "    input_features = torch.tensor(input_features)\n",
    "    labels = torch.tensor(labels)\n",
    "    return input_features, labels\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(common_voice_train, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(common_voice_test, batch_size=8, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eead9d8-69b3-4e8e-a250-bd2c57504d36",
   "metadata": {},
   "source": [
    "### Step 5: Fine-Tune the Model\n",
    "Set up the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e35e9c1-66b1-4b63-aa66-75312ea06aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        input_features, labels = batch\n",
    "        input_features = input_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=f\"Evaluating Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_features, labels = batch\n",
    "            input_features = input_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_features, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(test_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Evaluation Loss: {avg_eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa5c4d-2a07-41e3-a928-8853cd0c1ee9",
   "metadata": {},
   "source": [
    "### Step 6: Save the Fine-Tuned Model\n",
    "Save the model and processor for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b7d37-36a7-4783-9c86-2061097c0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"whisper-corean-asr\")\n",
    "processor.save_pretrained(\"whisper-corean-asr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5bcea-da3e-40d1-ab72-7ba151808393",
   "metadata": {},
   "source": [
    "### Step 7: Inference with the Fine-Tuned Model\n",
    "Use the fine-tuned model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a0009-b04b-4c92-8c99-38d7708ff3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and processor\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"whisper-corean-asr\")\n",
    "processor = WhisperProcessor.from_pretrained(\"whisper-corean-asr\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load an example audio file\n",
    "audio_path = \"path_to_audio_file.wav\"\n",
    "audio = whisper.load_audio(audio_path)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# Preprocess the audio\n",
    "input_features = processor(audio, sampling_rate=16000).input_features\n",
    "input_features = torch.tensor(input_features).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Transcription:\", transcription[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff336c22-0c89-46c6-b6e4-55ec4e53b7d0",
   "metadata": {},
   "source": [
    "This guide outlines the process of fine-tuning Whisper for corean ASR using the Common Voice dataset. Adjust the parameters and paths as needed for your specific use case and dataset. Whisper, though resource-intensive, can deliver high accuracy for ASR tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78c97e-87cd-438e-9056-05572efb40e8",
   "metadata": {},
   "source": [
    "## Convert a pretrained whisper model into ONNX format\n",
    "### Install Required Libraries\n",
    "Ensure you have the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3eca2-72f5-4fe8-909d-4b55095fb391",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64343ee6-c4df-4f3f-bf46-16d55518b12e",
   "metadata": {},
   "source": [
    "### Prepare the Model for Export\n",
    "You need to define a helper function to handle the model’s forward pass for the ONNX conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616431b5-e90b-4454-bf47-cc5636ee00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Function to handle the model's forward pass\n",
    "def forward_pass(input_features):\n",
    "    # Move the input to the appropriate device\n",
    "    input_features = input_features.to(device)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    outputs = model(input_features)\n",
    "    \n",
    "    return outputs.logits\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0eca05-8074-48e6-961f-74164a4d73dd",
   "metadata": {},
   "source": [
    "### Define a Dummy Input\n",
    "Create a dummy input that matches the input signature of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b34246-716b-46db-aa6c-d4ec596170f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dummy input in the same shape as your model expects\n",
    "dummy_input = torch.randn(1, 80, 3000).to(device)  # Example input shape (batch_size, feature_dim, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf4713-877c-4f7f-90bc-26fc33973345",
   "metadata": {},
   "source": [
    "### Export the Model to ONNX\n",
    "Use torch.onnx.export to convert the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8184fef3-4e8d-42b3-a943-dc6eb0af4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Export the model to ONNX\n",
    "onnx_model_path = \"whisper_corean_asr.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_model_path,\n",
    "    input_names=[\"input_features\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_features\": {0: \"batch_size\", 2: \"sequence_length\"},  # Variable length axes\n",
    "        \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "    },\n",
    "    opset_version=11,\n",
    ")\n",
    "\n",
    "print(f\"Model successfully exported to {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111fba6-010c-426e-91c9-1fd421fa59f7",
   "metadata": {},
   "source": [
    "### Verify the ONNX Model\n",
    "Load the ONNX model and verify it using onnxruntime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac5b17-85e9-49e6-b803-3f47aebe021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Verify the model by running an inference\n",
    "onnx_inputs = {\"input_features\": dummy_input.cpu().numpy()}\n",
    "onnx_outputs = onnx_model.run(None, onnx_inputs)\n",
    "\n",
    "print(\"ONNX model output shape:\", onnx_outputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362105b1-7d63-4907-9b71-7883732773b0",
   "metadata": {},
   "source": [
    "This code converts your fine-tuned Whisper model into ONNX format and verifies the conversion by running an inference. This ONNX model can now be optimized further and used for deployment on various platforms, including mobile devices.\n",
    "\n",
    "#Notes:\n",
    "Dynamic Axes: The dynamic_axes parameter allows the ONNX model to accept variable-length inputs, which is crucial for inference on variable-length audio sequences.\n",
    "Optimization: After exporting to ONNX, consider using tools like ONNX Runtime or TensorRT to optimize the model for better performance on your target deployment platform.\n",
    "By following these steps, you can convert your fine-tuned Whisper ASR model into an ONNX model, making it suitable for deployment on various platforms, including mobile devices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
