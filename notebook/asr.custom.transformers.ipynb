{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ff5d26-aebb-494c-b34b-fd0c9d463dea",
   "metadata": {},
   "source": [
    "Using the Transformers library from Hugging Face, you can customize an ASR model, including adding/removing layers and using a custom tokenizer. Below, Iâ€™ll guide you through the process of customizing a Wav2Vec2.0 model, fine-tuning it on the Common Voice dataset, and converting it to ONNX for deployment.\n",
    "\n",
    "#Step 1: Install Necessary Libraries\n",
    "First, make sure you have the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a93046-a432-4500-a910-65ae5f164a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets torch soundfile onnx onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe35af-59d3-4633-9343-a4d2caf65a27",
   "metadata": {},
   "source": [
    "Step 2: Load and Preprocess the Common Voice Dataset\n",
    "Use the datasets library to load and preprocess the Common Voice dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674752b0-ff1f-4c1a-8af6-1700821a24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "# Load the Common Voice dataset\n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"ko\", split=\"train\")\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"ko\", split=\"test\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess(batch):\n",
    "    audio, _ = sf.read(batch[\"path\"])\n",
    "    batch[\"audio\"] = audio\n",
    "    batch[\"text\"] = batch[\"sentence\"]\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(preprocess)\n",
    "common_voice_test = common_voice_test.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af75e5-020c-421c-b025-3444e0c9b6ef",
   "metadata": {},
   "source": [
    "Step 3: Define a Custom Tokenizer\n",
    "Create a custom tokenizer script, e.g., custom_tokenizer.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47608394-f91b-46ad-a4b2-679384ede662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_tokenizer.py\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, model_name=\"bert-base-multilingual-cased\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0ee54-6cd1-4f41-8903-5df2e3c3b84b",
   "metadata": {},
   "source": [
    "Step 4: Prepare Data Loaders with Custom Tokenizer\n",
    "Modify the data preparation script to use your custom tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4fdfe0-e9ea-4d6c-9c06-963d458e568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# Initialize custom tokenizer\n",
    "from custom_tokenizer import CustomTokenizer\n",
    "tokenizer = CustomTokenizer()\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess(batch):\n",
    "    audio, _ = sf.read(batch[\"path\"])\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
    "    batch[\"labels\"] = tokenizer.encode(batch[\"sentence\"])\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(preprocess)\n",
    "common_voice_test = common_voice_test.map(preprocess)\n",
    "\n",
    "# Define a collate function\n",
    "def collate_fn(batch):\n",
    "    input_features = [item[\"input_values\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    input_features = processor.pad(input_features, return_tensors='pt').input_values\n",
    "    labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(label) for label in labels], batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
    "    return {\"input_features\": input_features, \"labels\": labels}\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(common_voice_train, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(common_voice_test, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2104cff-70bf-4d72-916b-afc7335fd845",
   "metadata": {},
   "source": [
    "Step 5: Define the Custom ASR Model\n",
    "Extend the Wav2Vec2 model to include additional layers or modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14855e7c-2645-45ca-ac59-671f5ea50469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomWav2Vec2ForCTC(Wav2Vec2ForCTC):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.additional_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)\n",
    "        self.output_layer = nn.Linear(config.hidden_size // 2, config.vocab_size)\n",
    "\n",
    "    def forward(self, input_features, labels=None):\n",
    "        hidden_states = self.wav2vec2(input_features).last_hidden_state\n",
    "        hidden_states = self.additional_layer(hidden_states)\n",
    "        logits = self.output_layer(hidden_states)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.compute_loss(logits, labels)\n",
    "        return {'logits': logits, 'loss': loss}\n",
    "\n",
    "    def compute_loss(self, logits, labels):\n",
    "        # Define your loss computation here\n",
    "        pass\n",
    "\n",
    "model = CustomWav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db236b66-f0d1-4ddf-a4bf-76be5fa0fc1e",
   "metadata": {},
   "source": [
    "Step 6: Fine-Tune the Model\n",
    "Set up the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38eedd-2d97-4e7e-987e-7212fe7e9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-custom-corean\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    save_steps=400,\n",
    "    eval_steps=400,\n",
    "    logging_steps=400,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=lambda data: {'input_features': torch.stack([f['input_features'] for f in data]),\n",
    "                                'labels': torch.stack([f['labels'] for f in data])},\n",
    "    args=training_args,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3147bad-6f2f-4586-8cf9-618be539afd5",
   "metadata": {},
   "source": [
    "#Step 7: Convert the Model to ONNX\n",
    "Export the fine-tuned model to ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fddf91-9613-444f-90d2-783311172e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define dummy input for ONNX export\n",
    "dummy_input = torch.randn(1, 16000, device=model.device)\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input,\n",
    "    \"wav2vec2_custom_corean.onnx\",\n",
    "    input_names=[\"input_features\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"input_features\": {0: \"batch_size\", 1: \"sequence_length\"}, \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}},\n",
    "    opset_version=11\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f0bd4-086a-4c51-88d3-bf7bf4129771",
   "metadata": {},
   "source": [
    "Step 8: Verify the ONNX Model\n",
    "Load the ONNX model and run inference to ensure it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707a6b4-2509-49bc-93d8-708b90292e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = ort.InferenceSession(\"wav2vec2_custom_corean.onnx\")\n",
    "\n",
    "# Load an example audio file\n",
    "audio_path = \"path_to_audio_file.wav\"\n",
    "audio, rate = sf.read(audio_path)\n",
    "assert rate == 16000  # ensure the sample rate is 16000 Hz\n",
    "\n",
    "# Preprocess the audio\n",
    "input_values = processor(audio, sampling_rate=16000).input_values[0]\n",
    "input_values = np.expand_dims(input_values, axis=0)  # add batch dimension\n",
    "\n",
    "# Run inference\n",
    "onnx_inputs = {\"input_features\": input_values}\n",
    "onnx_outputs = onnx_model.run(None, onnx_inputs)\n",
    "\n",
    "# Decode the output if needed\n",
    "# This step depends on your model's output format\n",
    "print(\"ONNX model output:\", onnx_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf718eda-145f-4f50-8e00-e1e8a43af3f9",
   "metadata": {},
   "source": [
    "This guide provides the steps to customize a Wav2Vec2 ASR model using the Hugging Face Transformers library, including adding/removing layers, using a custom tokenizer, and converting the model to ONNX for deployment. Adjust paths, parameters, and configurations as needed for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
