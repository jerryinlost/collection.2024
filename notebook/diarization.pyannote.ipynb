{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affdd7b3-acc8-440c-baad-bef910fb2f94",
   "metadata": {},
   "source": [
    "For a language-free speaker diarization model that can run on mobile devices, I recommend using pyannote-audio, a toolkit that provides pre-trained models for speaker diarization. The models from pyannote-audio are known for their effectiveness and efficiency, making them suitable for deployment on mobile devices.\n",
    "\n",
    "Below are the steps to train your own speaker diarization model using pyannote-audio with Python:\n",
    "\n",
    "### Step 1: Install pyannote-audio and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0665ba-4732-4100-9da2-e8b565a23d66",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb627ec8-ba1f-4b3c-a809-c4df65002f5a",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Your Dataset\n",
    "Ensure your dataset is in the AMI format where each audio file has a corresponding .rttm file for annotations.\n",
    "\n",
    "#Step 3: Configure the Training Environment\n",
    "Create a configuration file config.yml for your model. Here is an example configuration for a small model suitable for mobile devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677dde34-b3c2-42c2-b67f-107530f6f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.yml\n",
    "protocol: your.dataset.Protocol\n",
    "duration: 3.2\n",
    "step: 0.8\n",
    "batch_size: 32\n",
    "architecture:\n",
    "    name: PyanNet\n",
    "    params:\n",
    "        n_features: 80\n",
    "        rnn: LSTM\n",
    "        rnn_params:\n",
    "            hidden_size: 64\n",
    "            num_layers: 2\n",
    "        linear:\n",
    "            hidden_size: 64\n",
    "        pooling: statistics\n",
    "scheduler:\n",
    "    name: ReduceLROnPlateau\n",
    "    params:\n",
    "        patience: 1\n",
    "        factor: 0.5\n",
    "min_duration: 0.0\n",
    "max_duration: 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da987e22-ba78-4423-9c67-0b5a8cdf7689",
   "metadata": {},
   "source": [
    "### Step 4: Training the Model\n",
    "Write and run the following Python script to start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31182aac-20af-40f4-8e81-8e9318fa0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.tasks import Segmentation\n",
    "from pyannote.audio.train import Trainer\n",
    "from pyannote.database import get_protocol\n",
    "\n",
    "# Load the protocol\n",
    "protocol = get_protocol('your.dataset.Protocol')\n",
    "\n",
    "# Initialize the segmentation task\n",
    "segmentation = Segmentation(\n",
    "    protocol=protocol,\n",
    "    duration=3.2,\n",
    "    batch_size=32,\n",
    "    step=0.8,\n",
    "    augmentation=None,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(model=segmentation.model, task=segmentation)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f65af-20d8-4547-85d2-d56a8695c5dc",
   "metadata": {},
   "source": [
    "### Step 5: Export the Model for Mobile Deployment\n",
    "After training, you can export the model using ONNX, TensorFlow Lite, or another format suitable for mobile deployment. Hereâ€™s an example of saving the model using ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a0742-9dd9-4331-b93c-20fc3e1f8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "\n",
    "# Load your trained model\n",
    "model = segmentation.model\n",
    "model.eval()\n",
    "\n",
    "# Dummy input for export\n",
    "dummy_input = torch.randn(1, 1, 80, 800)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model, dummy_input, \"speaker_diarization_model.onnx\", \n",
    "                  opset_version=11, input_names=['input'], output_names=['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca97fb-3947-40d0-96a2-b13293b242cd",
   "metadata": {},
   "source": [
    "### Step 6: Deploy on Mobile\n",
    "Use a framework like TensorFlow Lite or ONNX Runtime Mobile to load and run the exported model on your mobile device.\n",
    "\n",
    "#Note:\n",
    "Replace your.dataset.Protocol with the actual protocol name of your dataset.\n",
    "Make sure your dataset is correctly formatted and accessible.\n",
    "Depending on your dataset size and hardware, you might need to tune hyperparameters for optimal performance.\n",
    "This approach offers a balance between efficiency and performance, making it suitable for mobile applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d366a-ee22-4a5a-8199-890b4890d9fd",
   "metadata": {},
   "source": [
    "#### Bonus: Download pretrained and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011b91b-c619-416b-89e2-06af9b99ddf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import notebook\n",
    "\n",
    "# Step 1: Install pyannote-audio\n",
    "# pip install pyannote.audio\n",
    "\n",
    "# Step 2: Load a pre-trained model\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
    "\n",
    "# # Step 3: Download a sample audio file\n",
    "# url = \"https://www.example.com/sample.wav\"  # Replace with an actual URL\n",
    "# response = requests.get(url)\n",
    "# with open(\"sample.wav\", \"wb\") as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# # Step 4: Evaluate the model on the sample audio\n",
    "# diarization = pipeline({\"uri\": \"sample\", \"audio\": \"sample.wav\"})\n",
    "\n",
    "# # Print the diarization result\n",
    "# print(diarization)\n",
    "\n",
    "# # Optionally, visualize the result\n",
    "# notebook.crop = diarization.get_timeline().extent\n",
    "# notebook.plot_annotation(diarization, legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
