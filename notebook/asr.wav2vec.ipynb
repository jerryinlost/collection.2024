{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e7c4b4-cf14-40c2-9894-93217c17284d",
   "metadata": {},
   "source": [
    "For deploying an ASR model on mobile devices, youâ€™ll need a smaller, more efficient model. One suitable option is the Wav2Vec2.0-base model or its distilled versions. Another lightweight alternative is the DeepSpeech model.\n",
    "\n",
    "## Step-by-Step Guide for Training a Small ASR Model\n",
    "## Step 1: Install Necessary Libraries\n",
    "\n",
    "Make sure you have the required libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1800fa7-4075-4515-a56b-0205c6ce5924",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets torch soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693ba38-dd59-40c6-a4ba-9b756b11fa78",
   "metadata": {},
   "source": [
    "## Step 2: Load a Smaller Pre-trained Model and Tokenizer\n",
    "Use a smaller variant of the Wav2Vec2.0 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ba2b3-8b1f-4c91-a6a2-534413571ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# Load a smaller pre-trained model\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a040fc2-8c89-467f-b78c-3a7730abf142",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Your Dataset\n",
    "Ensure your dataset has audio files and corresponding transcripts. Load and preprocess the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c872bc-bca3-43e7-9a7d-e82ec46300ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"path_to_your_dataset\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, _ = sf.read(batch[\"file\"])\n",
    "    batch[\"speech\"] = speech_array\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=16_000).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"transcript\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(speech_file_to_array_fn)\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faeb01-1d6a-4a8d-b4ba-a3fccc789285",
   "metadata": {},
   "source": [
    "If you don't have your own dataset, try the common voice dataset. Please check out the [huggingface open ASR datasets](https://huggingface.co/datasets?task_categories=task_categories:automatic-speech-recognition&sort=trending&search=ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013093f2-e3d1-4f3e-901e-4691cc2f3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "# Load the Common Voice dataset\n",
    "common_voice = load_dataset(\"JaepaX/korean_dataset\", split=\"train+validation\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, _ = sf.read(batch[\"path\"])\n",
    "    batch[\"speech\"] = speech_array\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=16_000).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "common_voice = common_voice.map(speech_file_to_array_fn)\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4920f-c171-4e0b-a65b-daa351d708b7",
   "metadata": {},
   "source": [
    "## Step 4: Fine-Tune the Model\n",
    "Set up the training arguments and fine-tune the smaller model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3821464-5ac9-4076-862a-f9b9df3b42f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-small-korean\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    save_steps=400,\n",
    "    eval_steps=400,\n",
    "    logging_steps=400,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Define the data collator\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        batch[\"labels\"] = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5477ec-6a5d-402c-a2bc-2419c6bcd206",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the Model\n",
    "Evaluate your fine-tuned model on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc21b5-42ce-4d29-a860-adcc86769d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b43bb1-edb6-4e9d-90d2-68bd892c561f",
   "metadata": {},
   "source": [
    "# Model Optimization for Mobile\n",
    "To further optimize for mobile deployment, consider converting the model to ONNX format or using TensorFlow Lite:\n",
    "\n",
    "## Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3cb70-4b01-4df5-b44f-54d1002d7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-small-korean\")\n",
    "\n",
    "# Export the model to ONNX\n",
    "dummy_input = torch.zeros(1, 16000)  # Example input tensor\n",
    "torch.onnx.export(model, dummy_input, \"wav2vec2-small-korean.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979660f2-90bb-4f8d-9f44-0c2f4278c70c",
   "metadata": {},
   "source": [
    "This approach will help you create a smaller and efficient ASR model suitable for mobile devices. Adjust the parameters and dataset paths as needed for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
