{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2668b3-327e-42e6-8eb2-0478721185d8",
   "metadata": {},
   "source": [
    "Here’s an English version of the sample chat dataset, along with the steps to load and preprocess it for fine-tuning a conversational LLM like LLaMA.\n",
    "\n",
    "### Sample Chat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ebccb-5a2a-42e5-9bba-0b6e400634b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"input_text\": \"Hello, how are you feeling today?\",\n",
    "        \"response_text\": \"Hello! I'm feeling great today. How about you?\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"What's the weather like today?\",\n",
    "        \"response_text\": \"It's sunny and bright today. Perfect for outdoor activities.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"What are your favorite hobbies?\",\n",
    "        \"response_text\": \"I love reading books. What about you?\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"Can you recommend a movie you've seen recently?\",\n",
    "        \"response_text\": \"Sure, I recently watched 'Parasite' and it was really interesting. I recommend it!\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"What should I have for lunch?\",\n",
    "        \"response_text\": \"How about some kimchi stew or bibimbap? Both are delicious!\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a6adb-ae8e-4e7f-9b9b-90174b2eadf2",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- input_text: The text that the user inputs or the prompt given to the chatbot.\n",
    "- response_text: The text that the chatbot should generate in response to the input.\n",
    "\n",
    "### Creating a Dataset File\n",
    "Save the above JSON data into a file named chat_dataset.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101378f-31fa-489b-af4d-b907a6d1d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"input_text\": \"Hello, how are you feeling today?\",\n",
    "        \"response_text\": \"Hello! I'm feeling great today. How about you?\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"What's the weather like today?\",\n",
    "        \"response_text\": \"It's sunny and bright today. Perfect for outdoor activities.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"What are your favorite hobbies?\",\n",
    "        \"response_text\": \"I love reading books. What about you?\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"Can you recommend a movie you've seen recently?\",\n",
    "        \"response_text\": \"Sure, I recently watched 'Parasite' and it was really interesting. I recommend it!\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"What should I have for lunch?\",\n",
    "        \"response_text\": \"How about some kimchi stew or bibimbap? Both are delicious!\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc7a0f-1d4e-49b7-a5a4-2d6117f3d013",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing the Dataset\n",
    "Here’s how you can load and preprocess this dataset for fine-tuning a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c226d8-ff36-4659-bb82-d7e58cb6699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import LLaMATokenizer, LLaMAForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('json', data_files={'train': 'path/to/chat_dataset.json'})\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"facebook/llama-3b\"\n",
    "tokenizer = LLaMATokenizer.from_pretrained(model_name)\n",
    "model = LLaMAForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = examples['input_text']\n",
    "    responses = examples['response_text']\n",
    "    inputs = tokenizer(inputs, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    responses = tokenizer(responses, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': responses['input_ids']\n",
    "    }\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_llama_chat')\n",
    "tokenizer.save_pretrained('./fine_tuned_llama_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8925d-dad6-413b-9df4-8e2a80939f52",
   "metadata": {},
   "source": [
    "### Inference with the Fine-tuned Chat Model\n",
    "Load your fine-tuned model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed986f-1e44-4a7b-a6f4-594ca761e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = LLaMAForCausalLM.from_pretrained('./fine_tuned_llama_chat')\n",
    "tokenizer = LLaMATokenizer.from_pretrained('./fine_tuned_llama_chat')\n",
    "\n",
    "# Create a conversational pipeline\n",
    "chatbot = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate a response\n",
    "prompt = \"Hello, how are you?\"\n",
    "generated_text = chatbot(prompt, max_length=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee6700-6a19-414b-8b1b-cb6c2c0a4a24",
   "metadata": {},
   "source": [
    "### Notes\n",
    "1. Dataset Structure: Ensure your dataset follows the structure shown above, with clear pairs of input and response texts.\n",
    "2. Tokenization: The tokenization process may need to be adjusted based on your specific tokenizer and model requirements.\n",
    "3. Special Tokens: You might want to add special tokens to signify the beginning and end of conversations, especially for more complex datasets.\n",
    "    \n",
    "This should give you a good starting point for creating and fine-tuning a chat dataset for a conversational AI model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
