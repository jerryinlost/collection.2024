{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5g9NTF_Ixad"
      },
      "source": [
        "##Install Tranformers and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4VPaSlnHUvT"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClniiYCWHK4b"
      },
      "outputs": [],
      "source": [
        "! pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pljpioLsJOtb"
      },
      "source": [
        "##Load pre trained TF Whisper Tiny model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJNOxn5vHaGi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFWhisperModel, WhisperFeatureExtractor\n",
        "from datasets import load_dataset\n",
        "\n",
        "model = TFWhisperModel.from_pretrained(\"./models/openai/whisper-tiny\")\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"./models/openai/whisper-tiny\")\n",
        "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "inputs = feature_extractor(\n",
        "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"tf\"\n",
        ")\n",
        "input_features = inputs.input_features\n",
        "print(input_features)\n",
        "decoder_input_ids = tf.convert_to_tensor([[1, 1]]) * model.config.decoder_start_token_id\n",
        "last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n",
        "list(last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9XP25uhJl44"
      },
      "source": [
        "##Generate Saved momdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpYwMmgyHf0B"
      },
      "outputs": [],
      "source": [
        "model.save('/content/tf_whisper_saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_79jFEJYyJ"
      },
      "source": [
        "##Convert saved model to TFLite model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owez2zvzHl-p"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "saved_model_dir = '/content/tf_whisper_saved'\n",
        "tflite_model_path = 'whisper.tflite'\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFkzUrjIbNcH"
      },
      "outputs": [],
      "source": [
        "%ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpEnWZt7iQJK"
      },
      "source": [
        "##Evaluate TF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RuFFohHg2ho"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import WhisperProcessor, TFWhisperForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
        "model = TFWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
        "\n",
        "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "\n",
        "inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"tf\")\n",
        "input_features = inputs.input_features\n",
        "\n",
        "generated_ids = model.generate(input_features)\n",
        "\n",
        "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "transcription"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-eKuy_cG4u0"
      },
      "source": [
        "## Evaluate TF Lite model (naive)\n",
        "\n",
        "We can load the model as defined above... but the model is useless on its own. Generation is much more complex that a model forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnfHirgyG0W4"
      },
      "outputs": [],
      "source": [
        "tflite_model_path = 'whisper.tflite'\n",
        "interpreter = tf.lite.Interpreter(tflite_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8VJQuHJKzl4"
      },
      "source": [
        "## Create generation-enabled TF Lite model\n",
        "\n",
        "The solution consists in defining a model whose serving function is the generation call. Here's an example of how to do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSMrJo7hJ95c"
      },
      "outputs": [],
      "source": [
        "class GenerateModel(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    super(GenerateModel, self).__init__()\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(\n",
        "    # shouldn't need static batch size, but throws exception without it (needs to be fixed)\n",
        "    input_signature=[\n",
        "      tf.TensorSpec((1, 80, 3000), tf.float32, name=\"input_features\"),\n",
        "    ],\n",
        "  )\n",
        "  def serving(self, input_features):\n",
        "    outputs = self.model.generate(\n",
        "      input_features,\n",
        "      max_new_tokens=223, #change as needed\n",
        "      return_dict_in_generate=True,\n",
        "    )\n",
        "    return {\"sequences\": outputs[\"sequences\"]}\n",
        "\n",
        "saved_model_dir = '/content/tf_whisper_saved'\n",
        "tflite_model_path = 'whisper.tflite'\n",
        "\n",
        "generate_model = GenerateModel(model=model)\n",
        "tf.saved_model.save(generate_model, saved_model_dir, signatures={\"serving_default\": generate_model.serving})\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLkMa_36PgW-"
      },
      "outputs": [],
      "source": [
        "# loaded model... now with generate!\n",
        "tflite_model_path = 'whisper.tflite'\n",
        "interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "\n",
        "tflite_generate = interpreter.get_signature_runner()\n",
        "generated_ids = tflite_generate(input_features=input_features)[\"sequences\"]\n",
        "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "transcription"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
