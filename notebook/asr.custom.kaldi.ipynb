{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b718b426-8032-446c-93be-46f4de1afe4c",
   "metadata": {},
   "source": [
    "Kaldi is a highly flexible and powerful toolkit for speech recognition, but it is more complex and less user-friendly compared to other modern frameworks like SpeechBrain or Hugging Face Transformers. However, it provides a great deal of flexibility for customizing models, including adding/removing layers and using custom tokenizers. Below is a guide on how to customize a Kaldi ASR model, fine-tune it, and export it to an ONNX format.\n",
    "\n",
    "### Step 1: Install Kaldi\n",
    "First, you need to install Kaldi. Follow the official Kaldi installation instructions.\n",
    "\n",
    "### Step 2: Prepare the Common Voice Dataset\n",
    "Download and prepare the Common Voice dataset. Kaldi uses a specific directory structure and file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a926f-5fc5-43e6-b456-2e9e659b1e3a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Navigate to the Kaldi directory\n",
    "cd ~/kaldi/egs\n",
    "\n",
    "# Create a new directory for your project\n",
    "mkdir commonvoice\n",
    "cd commonvoice\n",
    "\n",
    "# Create necessary directories\n",
    "mkdir data exp mfcc\n",
    "\n",
    "# Download and preprocess the dataset\n",
    "# (Assuming you have downloaded the Mozilla Common Voice dataset)\n",
    "# Adjust paths as necessary\n",
    "COMMONVOICE_DIR=/path/to/common_voice\n",
    "\n",
    "# Prepare data directories\n",
    "for part in train test; do\n",
    "  mkdir -p data/$part\n",
    "  python3 local/prepare_data.py $COMMONVOICE_DIR $part data/$part\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f0b2a2-4610-4de7-bcf3-c0d843437f59",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation Scripts\n",
    "Create a prepare_data.py script to convert Common Voice dataset to Kaldi format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41a113-153b-4079-bc90-f211b8ab0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local/prepare_data.py\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "def prepare_data(commonvoice_dir, part, output_dir):\n",
    "    wav_scp = open(os.path.join(output_dir, 'wav.scp'), 'w')\n",
    "    text = open(os.path.join(output_dir, 'text'), 'w')\n",
    "    utt2spk = open(os.path.join(output_dir, 'utt2spk'), 'w')\n",
    "    \n",
    "    with open(os.path.join(commonvoice_dir, part, 'validated.tsv'), encoding='utf-8') as tsv_file:\n",
    "        reader = csv.DictReader(tsv_file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            utt_id = row['client_id'] + '-' + row['path'].replace('.mp3', '')\n",
    "            wav_path = os.path.join(commonvoice_dir, part, 'clips', row['path'])\n",
    "            transcription = row['sentence']\n",
    "            \n",
    "            wav_scp.write(f\"{utt_id} sox {wav_path} -t wav -r 16000 - |\\n\")\n",
    "            text.write(f\"{utt_id} {transcription}\\n\")\n",
    "            utt2spk.write(f\"{utt_id} {row['client_id']}\\n\")\n",
    "    \n",
    "    wav_scp.close()\n",
    "    text.close()\n",
    "    utt2spk.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    commonvoice_dir = sys.argv[1]\n",
    "    part = sys.argv[2]\n",
    "    output_dir = sys.argv[3]\n",
    "    prepare_data(commonvoice_dir, part, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe7441-c0cd-47d1-9263-df3fb4a61762",
   "metadata": {},
   "source": [
    "### Step 4: Feature Extraction\n",
    "Extract features from the audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ecd2b-104c-42f7-a569-09d42adb5f47",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "for part in train test; do\n",
    "  steps/make_mfcc.sh --nj 10 --mfcc-config conf/mfcc.conf data/$part exp/make_mfcc/$part mfcc\n",
    "  steps/compute_cmvn_stats.sh data/$part exp/make_mfcc/$part mfcc\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727107ec-eb3a-4a5b-adc8-96074c6bdcae",
   "metadata": {},
   "source": [
    "### Step 5: Train a Custom ASR Model\n",
    "Modify the existing Kaldi scripts to include your custom architecture. For example, you can customize the TDNN-F model:\n",
    "\n",
    "Define the neural network architecture in a configuration file (e.g., conf/custom_tdnnf.conf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9500b-d81b-4233-9635-a917e3801951",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# conf/custom_tdnnf.conf\n",
    "component name=idct type=FixedAffineComponent input-dim=40 output-dim=40 matrix=IdctMatrix num-cols=13\n",
    "component name=tdnn1.affine type=NaturalGradientAffineComponent input-dim=40 output-dim=1024\n",
    "component name=tdnn1.relu type=RectifiedLinearComponent dim=1024\n",
    "component name=tdnn1.batchnorm type=BatchNormComponent dim=1024\n",
    "\n",
    "# Add more layers as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6f28f-2c41-416a-8b18-39890b1be89d",
   "metadata": {},
   "source": [
    "Modify the training script to use your custom configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a41ed-934f-41ff-a7b9-b1861698b664",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "steps/nnet3/train_tdnnf.sh --cmd \"$train_cmd\" --feat.online-ivector-dir exp/nnet3/ivectors_train --feat.cmvn-opts \"--norm-means=false --norm-vars=false\" --chain.xent-regularize 0.1 --chain.leaky-hmm-coefficient 0.1 --chain.l2-regularize 0.00005 --chain.apply-deriv-weights false --chain.lm-opts=\"--num-extra-lm-states=2000\" --egs.dir \"\" --egs.stage -10 --egs.opts \"--frames-overlap-per-eg 0\" --egs.chunk-width 140,100,160 --trainer.num-chunk-per-minibatch 128,64 --trainer.frames-per-iter 1500000 --trainer.num-shrinkage-iters 20 --trainer.optimization.num-jobs-initial 1 --trainer.optimization.num-jobs-final 2 --trainer.optimization.initial-effective-lrate 0.00025 --trainer.optimization.final-effective-lrate 0.000025 --trainer.optimization.shrink-value 1.0 --trainer.max-param-change 2.0 --trainer.num-epochs 2 --cleanup.remove-egs true --feat-dir data/train --tree-dir exp/chain/tree --lat-dir exp/tri4_lats --dir exp/chain/custom_tdnnf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff56e8-b6cf-499e-8402-f01ca85220da",
   "metadata": {},
   "source": [
    "### Step 6: Decode the Model\n",
    "Decode using the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa895e-f000-44c4-9868-e47cd384dae4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "steps/nnet3/decode.sh --nj 10 --cmd \"$decode_cmd\" exp/chain/tree/graph data/test exp/chain/custom_tdnnf/decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21c190-e1e8-43fb-a68c-7f52220f02bf",
   "metadata": {},
   "source": [
    "### Step 7: Export the Model to ONNX\n",
    "Kaldi does not natively support exporting to ONNX, so you will need to convert the Kaldi model to a PyTorch model first, and then export it to ONNX. This process can be complex, but hereâ€™s a general approach:\n",
    "\n",
    "1. Convert Kaldi model to PyTorch: You may need to write a custom script to load Kaldi model parameters into a PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00579d5f-62d5-475c-a684-cfe2d8aba9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kaldi_io\n",
    "\n",
    "# Define PyTorch model equivalent to your Kaldi model\n",
    "class CustomTDNNF(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomTDNNF, self).__init__()\n",
    "        self.tdnn1 = torch.nn.Linear(40, 1024)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(1024)\n",
    "        # Add more layers as needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tdnn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm(x)\n",
    "        # Add more layers as needed\n",
    "        return x\n",
    "\n",
    "# Load Kaldi parameters into PyTorch model (example, adjust as needed)\n",
    "model = CustomTDNNF()\n",
    "with kaldi_io.open_or_fd('exp/chain/custom_tdnnf/final.mdl') as f:\n",
    "    kaldi_params = torch.load(f)\n",
    "    model.load_state_dict(kaldi_params)\n",
    "\n",
    "# Export to ONNX\n",
    "dummy_input = torch.randn(1, 40, 160)  # Example input\n",
    "torch.onnx.export(model, dummy_input, \"custom_tdnnf.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5e5f8-e839-4ec9-b02d-db72885796b5",
   "metadata": {},
   "source": [
    "2. Verify the ONNX model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aade91-f03d-464d-b0d9-1b3061608f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = ort.InferenceSession(\"custom_tdnnf.onnx\")\n",
    "\n",
    "# Create a dummy input\n",
    "dummy_input = np.random.randn(1, 40, 160).astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "onnx_inputs = {\"input\": dummy_input}\n",
    "onnx_outputs = onnx_model.run(None, onnx_inputs)\n",
    "\n",
    "print(\"ONNX model output:\", onnx_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ddfcb5-e103-4a39-87a2-915df52de401",
   "metadata": {},
   "source": [
    "Summary\n",
    "This guide provides a high-level overview of how to customize an ASR model with Kaldi, including adding/removing layers and preparing data. The process of converting a Kaldi model to ONNX involves an intermediate step where you convert the Kaldi model to a PyTorch model, and then export it to ONNX. This process can be complex and may require writing custom scripts to load Kaldi parameters into PyTorch. Adjust paths, parameters, and configurations as needed for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
